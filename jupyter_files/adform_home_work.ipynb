{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ETL application for counting impressions and clicks by date/hour for a specific user agent.\n",
    "Uses PySpark, environment variables (DB credentials, etc.), and writes CSV outputs.\n",
    "Follows best practices: DRY, PEP-8, logging, error handling, missing data handling.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Environment variables (example usage, no hardcoded credentials)\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"user\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"password\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "def parse_filename_datetime(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    From a filename like 'impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet',\n",
    "    extract the date/time in the format '2022-05-26 11:32'.\n",
    "    \"\"\"\n",
    "    match = re.search(r'_dk_(\\d{8})(\\d{4})', filename)\n",
    "    if not match:\n",
    "        return None\n",
    "    date_str, time_str = match.group(1), match.group(2)\n",
    "    parsed_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\")\n",
    "    return parsed_dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "def create_date_hour_df(spark, date_str):\n",
    "    \"\"\"\n",
    "    Create a Spark DataFrame with all hours of the given date (00-23) to ensure no missing hours.\n",
    "    \"\"\"\n",
    "    base_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    rows = [(base_date.strftime(\"%Y-%m-%d\"), h) for h in range(24)]\n",
    "    return spark.createDataFrame(rows, [\"date\", \"hour\"])\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"ImpressionsClicksETL\").getOrCreate()\n",
    "\n",
    "    # Example: input directory, output directory\n",
    "    input_dir = \"./input_parquet\"\n",
    "    output_dir = \"./output_csv\"\n",
    "    target_user_agent = \"some user agent\"\n",
    "\n",
    "    # Force creation of the directory if it doesn't exist\n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "    # Process all parquet files in the input directory\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith(\".parquet\"):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "\n",
    "            # Parse date/time from filename\n",
    "            date_time_str = parse_filename_datetime(file_name)\n",
    "            if not date_time_str:\n",
    "                logging.warning(\"Skipping file %s, unable to parse date/time.\", file_name)\n",
    "                continue\n",
    "\n",
    "            # Read parquet\n",
    "            df = spark.read.parquet(file_path)\n",
    "\n",
    "            # Extract date/hour from the filenameâ€™s date/time\n",
    "            extracted_date = date_time_str.split(\" \")[0]\n",
    "\n",
    "            # Filter records by our target user agent\n",
    "            filtered_df = df.filter(F.col(\"device_settings.user_agent\") == target_user_agent)\n",
    "\n",
    "            # Suppose impressions/clicks are columns: \"impressions\", \"clicks\"\n",
    "            # Derive hour from the processed date_time_str (for merging with the hour dimension)\n",
    "            hour_val = int(date_time_str.split(\" \")[1].split(\":\")[0])\n",
    "            aggregated_df = filtered_df.agg(\n",
    "                F.sum(\"impressions\").alias(\"impressions_sum\"),\n",
    "                F.sum(\"clicks\").alias(\"clicks_sum\")\n",
    "            ).withColumn(\"date\", F.lit(extracted_date)) \\\n",
    "             .withColumn(\"hour\", F.lit(hour_val))\n",
    "\n",
    "            # Create a full date-hour DataFrame (00-23) for the same date\n",
    "            date_hour_df = create_date_hour_df(spark, extracted_date)\n",
    "\n",
    "            # Join aggregated_df to date_hour_df to fill missing hours with zeros\n",
    "            joined_df = date_hour_df.join(\n",
    "                aggregated_df,\n",
    "                on=[\"date\", \"hour\"],\n",
    "                how=\"left\"\n",
    "            ).na.fill({\"impressions_sum\": 0, \"clicks_sum\": 0})\n",
    "\n",
    "            # Rename columns to match the required output\n",
    "            result_df = joined_df.select(\n",
    "                \"date\",\n",
    "                \"hour\",\n",
    "                F.col(\"impressions_sum\").alias(\"impression_count\"),\n",
    "                F.col(\"clicks_sum\").alias(\"click_count\")\n",
    "            )\n",
    "\n",
    "            # Write to CSV\n",
    "            output_file = os.path.join(output_dir, f\"impressions_clicks_{extracted_date}.csv\")\n",
    "            result_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_file)\n",
    "            logging.info(\"Processed file %s -> %s\", file_path, output_file)\n",
    "\n",
    "            # Remove input file\n",
    "            os.remove(file_path)\n",
    "            logging.info(\"Removed input file: %s\", file_path)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe051447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected OK!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from psycopg2 import OperationalError\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"adform_db\",  # default database name\n",
    "        user=\"adform_user\",    # using the correct username\n",
    "        password=\"adform_pass\", # replace with your actual password\n",
    "        host=\"localhost\",\n",
    "        port=\"5433\"\n",
    "    )\n",
    "    print(\"Connected OK!\")\n",
    "    conn.close()\n",
    "except OperationalError as e:\n",
    "    print(f\"Error connecting to database: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
