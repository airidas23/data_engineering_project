2025-01-20 10:28:11,699 - [INFO] - Spark Session created.
2025-01-20 10:31:14,326 - [ERROR] - Error occurred during Task 1 processing.
Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 35, in main
    process_files_for_user_agent(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 23, in process_files_for_user_agent
    df = spark.read.parquet(input_path + "/*.parquet")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\readwriter.py", line 544, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 538, in send_command
    logger.info("Error while receiving.", exc_info=True)
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\logging\__init__.py", line 1479, in info
    def info(self, msg, *args, **kwargs):
    
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 449, in connect_to_java_server
    self.close()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 542, in close
    def close(self, reset=False):
    
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 449, in connect_to_java_server
    self.close()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 542, in close
    def close(self, reset=False):
    
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 10:31:47,076 - [INFO] - Spark Session created.
2025-01-20 10:33:57,935 - [ERROR] - Error occurred during Task 1 processing.
Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 35, in main
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 23, in process_files_for_user_agent
    df = spark.read.parquet(input_path + "/*.parquet")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\readwriter.py", line 544, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 538, in send_command
    logger.info("Error while receiving.", exc_info=True)
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\logging\__init__.py", line 1479, in info
    def info(self, msg, *args, **kwargs):
    
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 10:34:28,823 - [INFO] - Spark Session created.
2025-01-20 10:34:30,362 - [ERROR] - Error occurred during Task 1 processing.
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 39, in main
    process_files_for_user_agent(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 23, in process_files_for_user_agent
    df = spark.read.parquet(input_path + "/*.parquet")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\readwriter.py", line 544, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/data/input/*.parquet.
2025-01-20 10:37:06,554 - [INFO] - Spark Session created.
2025-01-20 10:37:08,099 - [ERROR] - Error occurred during Task 1 processing.
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 35, in main
    process_files_for_user_agent(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 23, in process_files_for_user_agent
    df = spark.read.parquet(input_path + "/*.parquet")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\readwriter.py", line 544, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/data/input/*.parquet.
2025-01-20 10:50:21,876 - [INFO] - Spark Session created.
2025-01-20 10:50:21,876 - [ERROR] - Input path does not exist: ./data/input
2025-01-20 10:50:21,876 - [INFO] - Task 1 processing complete.
2025-01-20 10:53:01,796 - [INFO] - Spark Session created.
2025-01-20 10:59:53,714 - [ERROR] - Error occurred during Task 1 processing.
Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 35, in main
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 38, in process_files_for_user_agent
    # We assume structure:
         ^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\readwriter.py", line 544, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 538, in send_command
    logger.info("Error while receiving.", exc_info=True)
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\logging\__init__.py", line 1479, in info
    def info(self, msg, *args, **kwargs):
    
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 11:00:29,140 - [INFO] - Spark Session created.
2025-01-20 11:01:12,245 - [ERROR] - Error occurred during Task 1 processing.
Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 39, in main
    process_files_for_user_agent(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 36, in process_files_for_user_agent
    raise ValueError("Input data failed schema validation")
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\readwriter.py", line 544, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 538, in send_command
    logger.info("Error while receiving.", exc_info=True)
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\logging\__init__.py", line 1479, in info
    def info(self, msg, *args, **kwargs):
    
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 11:05:52,522 - [INFO] - Spark Session created.
2025-01-20 11:09:02,811 - [ERROR] - Error during processing: [WinError 10061] No connection could be made because the target machine actively refused it
Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 52, in process_files_for_user_agent
    df, input_files = self.read_input_files(input_path)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 32, in read_input_files
    df = self.spark.read.parquet(os.path.join(input_path, "*.parquet"))
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\readwriter.py", line 544, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 538, in send_command
    logger.info("Error while receiving.", exc_info=True)
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\logging\__init__.py", line 1479, in info
    def info(self, msg, *args, **kwargs):
    
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 11:09:02,816 - [INFO] - Task 1 processing complete.
2025-01-20 11:09:04,844 - [ERROR] - Error occurred during Task 1 processing.
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 49, in main
    parsed_args = parser.parse_args(args)
        ^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\session.py", line 1796, in stop
    self._sc.stop()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 654, in stop
    self._jsc.stop()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 11:09:29,074 - [INFO] - Starting application...
2025-01-20 11:09:55,887 - [INFO] - Spark Session created successfully.
2025-01-20 11:09:55,887 - [INFO] - Using input path: ./raw_data
2025-01-20 11:09:55,887 - [INFO] - Using output path: ./output
2025-01-20 11:12:53,700 - [ERROR] - Error during processing: [WinError 10061] No connection could be made because the target machine actively refused it
Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 52, in process_files_for_user_agent
    df, input_files = self.read_input_files(input_path)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 32, in read_input_files
    df = self.spark.read.parquet(os.path.join(input_path, "*.parquet"))
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\readwriter.py", line 544, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 538, in send_command
    logger.info("Error while receiving.", exc_info=True)
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\logging\__init__.py", line 1479, in info
    def info(self, msg, *args, **kwargs):
    
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 11:12:53,712 - [ERROR] - Task 1 processing failed.
2025-01-20 11:12:55,739 - [ERROR] - Error stopping Spark session: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 11:13:02,580 - [INFO] - Starting application...
2025-01-20 11:13:29,199 - [INFO] - Spark Session created successfully.
2025-01-20 11:13:29,199 - [INFO] - Using input path: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\raw_data
2025-01-20 11:13:29,199 - [INFO] - Using output path: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output
2025-01-20 11:14:06,462 - [ERROR] - Error during processing: [WinError 10061] No connection could be made because the target machine actively refused it
Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 52, in process_files_for_user_agent
    df, input_files = self.read_input_files(input_path)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 32, in read_input_files
    df = self.spark.read.parquet(os.path.join(input_path, "*.parquet"))
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\readwriter.py", line 544, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 538, in send_command
    logger.info("Error while receiving.", exc_info=True)
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\logging\__init__.py", line 1479, in info
    def info(self, msg, *args, **kwargs):
    
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 11:14:06,462 - [ERROR] - Task 1 processing failed.
2025-01-20 11:14:08,486 - [ERROR] - Error stopping Spark session: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 11:14:16,352 - [INFO] - Starting application...
2025-01-20 11:14:43,051 - [INFO] - Spark Session created successfully.
2025-01-20 11:14:43,051 - [INFO] - Using input path: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\raw_data
2025-01-20 11:14:43,051 - [INFO] - Using output path: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output
2025-01-20 11:17:00,076 - [ERROR] - Error during processing: [WinError 10061] No connection could be made because the target machine actively refused it
Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 52, in process_files_for_user_agent
    df, input_files = self.read_input_files(input_path)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 32, in read_input_files
    df = self.spark.read.parquet(os.path.join(input_path, "*.parquet"))
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\readwriter.py", line 544, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 538, in send_command
    logger.info("Error while receiving.", exc_info=True)
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\logging\__init__.py", line 1479, in info
    def info(self, msg, *args, **kwargs):
    
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 449, in connect_to_java_server
    self.close()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 542, in close
    def close(self, reset=False):
    
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 11:17:00,081 - [ERROR] - Task 1 processing failed.
2025-01-20 11:17:02,112 - [ERROR] - Error stopping Spark session: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 11:17:15,707 - [INFO] - Starting application...
2025-01-20 11:17:42,350 - [INFO] - Spark Session created successfully.
2025-01-20 11:17:42,350 - [INFO] - Using input path: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\raw_data
2025-01-20 11:17:42,350 - [INFO] - Using output path: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output
2025-01-20 11:25:40,900 - [ERROR] - Error during processing: [WinError 10061] No connection could be made because the target machine actively refused it
Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 52, in process_files_for_user_agent
    df, input_files = self.read_input_files(input_path)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 32, in read_input_files
    df = self.spark.read.parquet(os.path.join(input_path, "*.parquet"))
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\readwriter.py", line 544, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 538, in send_command
    logger.info("Error while receiving.", exc_info=True)
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\logging\__init__.py", line 1479, in info
    def info(self, msg, *args, **kwargs):
    
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 11:25:40,905 - [ERROR] - Task 1 processing failed.
2025-01-20 11:25:42,945 - [ERROR] - Error stopping Spark session: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 11:26:52,344 - [INFO] - Starting application...
2025-01-20 11:26:52,344 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 11:26:52,344 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 11:27:18,003 - [INFO] - Spark Session created successfully.
2025-01-20 11:30:20,925 - [ERROR] - Error during processing: [WinError 10061] No connection could be made because the target machine actively refused it
Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 52, in process_files_for_user_agent
    df, input_files = self.read_input_files(input_path)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 32, in read_input_files
    df = self.spark.read.parquet(os.path.join(input_path, "*.parquet"))
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\readwriter.py", line 544, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 538, in send_command
    logger.info("Error while receiving.", exc_info=True)
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\logging\__init__.py", line 1479, in info
    def info(self, msg, *args, **kwargs):
    
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 449, in connect_to_java_server
    self.close()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 542, in close
    def close(self, reset=False):
    
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 11:30:20,930 - [ERROR] - Task 1 processing failed.
2025-01-20 11:30:22,973 - [ERROR] - Error stopping Spark session: [WinError 10061] No connection could be made because the target machine actively refused it
2025-01-20 11:30:43,439 - [INFO] - Starting application...
2025-01-20 11:30:43,439 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 11:30:43,439 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 11:31:10,016 - [INFO] - Spark Session created successfully.
2025-01-20 11:31:14,127 - [ERROR] - Missing required field: event_type
2025-01-20 11:31:14,127 - [ERROR] - Error during processing: Input data failed schema validation
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 52, in process_files_for_user_agent
    df, input_files = self.read_input_files(input_path)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 36, in read_input_files
    raise ValueError("Input data failed schema validation")
ValueError: Input data failed schema validation
2025-01-20 11:31:14,127 - [ERROR] - Task 1 processing failed.
2025-01-20 11:31:14,422 - [INFO] - Spark Session stopped successfully.
2025-01-20 11:46:27,793 - [INFO] - Starting application...
2025-01-20 11:46:27,793 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 11:46:27,793 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 11:46:54,910 - [INFO] - Spark Session created successfully.
2025-01-20 11:46:59,688 - [ERROR] - Missing required field: event_type
2025-01-20 11:46:59,688 - [ERROR] - Error during processing: Input data failed schema validation
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 52, in process_files_for_user_agent
    df, input_files = self.read_input_files(input_path)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 36, in read_input_files
    raise ValueError("Input data failed schema validation")
ValueError: Input data failed schema validation
2025-01-20 11:46:59,688 - [ERROR] - Task 1 processing failed.
2025-01-20 11:47:00,239 - [INFO] - Spark Session stopped successfully.
2025-01-20 11:49:17,135 - [INFO] - Starting application...
2025-01-20 11:49:17,135 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 11:49:17,135 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 11:49:43,842 - [INFO] - Spark Session created successfully.
2025-01-20 11:49:47,997 - [ERROR] - Missing required fields: event_type
2025-01-20 11:49:47,997 - [ERROR] - Task 1 processing failed.
2025-01-20 11:49:48,227 - [INFO] - Spark Session stopped successfully.
2025-01-20 11:52:48,901 - [INFO] - Starting application...
2025-01-20 11:52:48,901 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 11:52:48,901 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 11:53:15,673 - [INFO] - Spark Session created successfully.
2025-01-20 11:53:19,724 - [ERROR] - Missing required fields: event_type
2025-01-20 11:53:19,724 - [ERROR] - Task 1 processing failed.
2025-01-20 11:53:20,065 - [INFO] - Spark Session stopped successfully.
2025-01-20 12:03:46,085 - [INFO] - Starting application...
2025-01-20 12:03:46,085 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 12:03:46,085 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 12:04:13,149 - [INFO] - Spark Session created successfully.
2025-01-20 12:04:13,149 - [INFO] - Found 5 impression files and 6 click files
2025-01-20 12:04:17,946 - [ERROR] - Error during processing: Cannot resolve column name "shown_in_non_friendly_iframe" among (transaction_header, user_identity, fraud_detection, geo_location, device_settings, connection, banner, rotator, interaction_id, page_url, landing_url, publisher_domain, banner_click_url_id, keywords, rtb_vars, publisher_data, dco_vars, banner_vars, adx_vars, ext_vars, test_id, raw_request_url, server_impression_time_ms, is_p2c_enabled, membership_id, unload, cross_device, client_id, client_provider, creation_time_local, inventory_source_creation_time_local, cookie_policy, deprecated_processing_id, payload, payload_cross_device, publisher_domain_normalized, payload_external_identity, invalidation_reason, exclude_from_processing_output, event_type).
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 96, in process_files_for_user_agent
    df = impressions_df.unionByName(clicks_df)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\dataframe.py", line 4043, in unionByName
    return DataFrame(self._jdf.unionByName(other._jdf, allowMissingColumns), self.sparkSession)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Cannot resolve column name "shown_in_non_friendly_iframe" among (transaction_header, user_identity, fraud_detection, geo_location, device_settings, connection, banner, rotator, interaction_id, page_url, landing_url, publisher_domain, banner_click_url_id, keywords, rtb_vars, publisher_data, dco_vars, banner_vars, adx_vars, ext_vars, test_id, raw_request_url, server_impression_time_ms, is_p2c_enabled, membership_id, unload, cross_device, client_id, client_provider, creation_time_local, inventory_source_creation_time_local, cookie_policy, deprecated_processing_id, payload, payload_cross_device, publisher_domain_normalized, payload_external_identity, invalidation_reason, exclude_from_processing_output, event_type).
2025-01-20 12:04:17,953 - [ERROR] - Task 1 processing failed.
2025-01-20 12:04:18,064 - [INFO] - Spark Session stopped successfully.
2025-01-20 12:06:18,640 - [INFO] - Starting application...
2025-01-20 12:06:18,640 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 12:06:18,641 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 12:06:45,244 - [INFO] - Spark Session created successfully.
2025-01-20 12:06:45,244 - [INFO] - Found 5 impression files and 6 click files
2025-01-20 12:06:53,046 - [INFO] - Row count metrics at initial: {'total_rows': 54, 'impression_count': 24, 'click_count': 30}
2025-01-20 12:06:53,046 - [INFO] - Initial data metrics: {'total_rows': 54, 'impression_count': 24, 'click_count': 30}
2025-01-20 12:06:53,918 - [INFO] - Filtered to 54 rows for user agent: some user agent
2025-01-20 12:07:07,040 - [ERROR] - Error during processing: An error occurred while calling o131.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 25) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 38 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)

	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)

	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)

	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)

	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)

	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:296)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:179)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:173)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:149)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:145)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:73)

	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)

	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120)

	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:194)

	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:194)

	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:196)

	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:302)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:300)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)

	at scala.collection.IterableLike.foreach(IterableLike.scala:74)

	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)

	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:300)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 38 more

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 132, in process_files_for_user_agent
    self._write_output(complete_df, output_path, date_part)
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 175, in _write_output
    .csv(output_file)
     ^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\readwriter.py", line 1864, in csv
    self._jwrite.csv(path)
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o131.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 25) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 38 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)

	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)

	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)

	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)

	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)

	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:296)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:179)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:173)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:149)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:145)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:73)

	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)

	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120)

	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:194)

	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:194)

	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:196)

	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:302)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:300)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)

	at scala.collection.IterableLike.foreach(IterableLike.scala:74)

	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)

	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:300)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 38 more


2025-01-20 12:07:07,044 - [ERROR] - Task 1 processing failed.
2025-01-20 12:07:07,451 - [INFO] - Spark Session stopped successfully.
2025-01-20 12:15:19,099 - [INFO] - Starting application...
2025-01-20 12:15:19,099 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 12:15:19,099 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 12:15:24,810 - [INFO] - Spark Session created successfully.
2025-01-20 12:15:24,810 - [INFO] - Found 5 impression files and 6 click files
2025-01-20 12:15:30,008 - [ERROR] - Error during processing: Cannot resolve column name "shown_in_non_friendly_iframe" among (transaction_header, user_identity, fraud_detection, geo_location, device_settings, connection, banner, rotator, interaction_id, page_url, landing_url, publisher_domain, banner_click_url_id, keywords, rtb_vars, publisher_data, dco_vars, banner_vars, adx_vars, ext_vars, test_id, raw_request_url, server_impression_time_ms, is_p2c_enabled, membership_id, unload, cross_device, client_id, client_provider, creation_time_local, inventory_source_creation_time_local, cookie_policy, deprecated_processing_id, payload, payload_cross_device, publisher_domain_normalized, payload_external_identity, invalidation_reason, exclude_from_processing_output, event_type).
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 133, in process_files_for_user_agent
    df = impressions_df.unionByName(clicks_df)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\dataframe.py", line 4037, in unionByName
    return DataFrame(self._jdf.unionByName(other._jdf, allowMissingColumns), self.sparkSession)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Cannot resolve column name "shown_in_non_friendly_iframe" among (transaction_header, user_identity, fraud_detection, geo_location, device_settings, connection, banner, rotator, interaction_id, page_url, landing_url, publisher_domain, banner_click_url_id, keywords, rtb_vars, publisher_data, dco_vars, banner_vars, adx_vars, ext_vars, test_id, raw_request_url, server_impression_time_ms, is_p2c_enabled, membership_id, unload, cross_device, client_id, client_provider, creation_time_local, inventory_source_creation_time_local, cookie_policy, deprecated_processing_id, payload, payload_cross_device, publisher_domain_normalized, payload_external_identity, invalidation_reason, exclude_from_processing_output, event_type).
2025-01-20 12:15:30,017 - [ERROR] - Task 1 processing failed.
2025-01-20 12:15:30,218 - [INFO] - Spark Session stopped successfully.
2025-01-20 12:17:57,441 - [INFO] - Starting application...
2025-01-20 12:17:57,442 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 12:17:57,442 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 12:18:24,811 - [INFO] - Spark Session created successfully.
2025-01-20 12:18:24,811 - [INFO] - Found 5 impression files and 6 click files
2025-01-20 12:18:33,697 - [INFO] - Filtered to 54 rows for user agent: some user agent
2025-01-20 12:18:46,786 - [ERROR] - Error writing output: An error occurred while calling o126.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 11.0 failed 1 times, most recent failure: Lost task 1.0 in stage 11.0 (TID 14) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.sql.execution.joins.UnsafeCartesianRDD.compute(CartesianProductExec.scala:46)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 47 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)

	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)

	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)

	at org.apache.spark.rdd.RDD.collect(RDD.scala:1045)

	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)

	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:296)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:179)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:173)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:149)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:145)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:73)

	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)

	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120)

	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:187)

	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:187)

	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:189)

	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:286)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:284)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)

	at scala.collection.IterableLike.foreach(IterableLike.scala:74)

	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)

	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:284)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.sql.execution.joins.UnsafeCartesianRDD.compute(CartesianProductExec.scala:46)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 47 more


2025-01-20 12:18:46,787 - [ERROR] - Error during processing: An error occurred while calling o126.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 11.0 failed 1 times, most recent failure: Lost task 1.0 in stage 11.0 (TID 14) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.sql.execution.joins.UnsafeCartesianRDD.compute(CartesianProductExec.scala:46)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 47 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)

	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)

	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)

	at org.apache.spark.rdd.RDD.collect(RDD.scala:1045)

	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)

	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:296)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:179)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:173)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:149)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:145)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:73)

	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)

	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120)

	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:187)

	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:187)

	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:189)

	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:286)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:284)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)

	at scala.collection.IterableLike.foreach(IterableLike.scala:74)

	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)

	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:284)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.sql.execution.joins.UnsafeCartesianRDD.compute(CartesianProductExec.scala:46)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 47 more

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 180, in process_files_for_user_agent
    self._write_output(complete_df, output_path, date_part)
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 200, in _write_output
    .csv(output_file)
     ^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\readwriter.py", line 1864, in csv
    self._jwrite.csv(path)
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o126.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 11.0 failed 1 times, most recent failure: Lost task 1.0 in stage 11.0 (TID 14) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.sql.execution.joins.UnsafeCartesianRDD.compute(CartesianProductExec.scala:46)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 47 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)

	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)

	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)

	at org.apache.spark.rdd.RDD.collect(RDD.scala:1045)

	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)

	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:296)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:179)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:173)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:149)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:145)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:73)

	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)

	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72)

	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120)

	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:187)

	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:187)

	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:189)

	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:286)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:284)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)

	at scala.collection.IterableLike.foreach(IterableLike.scala:74)

	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)

	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:284)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.sql.execution.joins.UnsafeCartesianRDD.compute(CartesianProductExec.scala:46)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 47 more


2025-01-20 12:18:46,792 - [ERROR] - Task 1 processing failed.
2025-01-20 12:18:46,983 - [INFO] - Spark Session stopped successfully.
2025-01-20 12:23:36,711 - [INFO] - Starting application...
2025-01-20 12:23:36,711 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 12:23:36,711 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 12:24:03,717 - [INFO] - Spark Session created successfully.
2025-01-20 12:24:03,717 - [ERROR] - Error during processing: 'DataProcessor' object has no attribute 'process_files_for_user_agent'
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 150, in main
    success = processor.process_files_for_user_agent(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DataProcessor' object has no attribute 'process_files_for_user_agent'
2025-01-20 12:24:04,070 - [INFO] - Spark Session stopped successfully.
2025-01-20 12:27:01,335 - [INFO] - Starting application...
2025-01-20 12:27:01,336 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 12:27:01,336 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 12:27:34,447 - [INFO] - Spark Session created successfully.
2025-01-20 12:27:34,448 - [ERROR] - Error during processing: 'DataProcessor' object has no attribute 'process_files_for_user_agent'
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 150, in main
    success = processor.process_files_for_user_agent(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DataProcessor' object has no attribute 'process_files_for_user_agent'
2025-01-20 12:27:34,784 - [INFO] - Spark Session stopped successfully.
2025-01-20 12:37:19,029 - [INFO] - Starting application...
2025-01-20 12:37:19,029 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 12:37:19,029 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 12:37:45,762 - [INFO] - Spark Session created successfully
2025-01-20 12:37:50,356 - [ERROR] - Error processing files for date 2022-05-27: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_type` cannot be resolved. Did you mean one of the following? [`test_id`, `client_id`, `ext_vars`, `rotator`, `banner`].;
'Pivot ArrayBuffer(hour#80), 'event_type, [impression, click], [count(1)]
+- Project [transaction_header#0, user_identity#1, fraud_detection#2, geo_location#3, device_settings#4, connection#5, banner#6, rotator#7, interaction_id#8L, page_url#9, landing_url#10, publisher_domain#11, banner_click_url_id#12, keywords#13, rtb_vars#14, publisher_data#15, dco_vars#16, banner_vars#17, adx_vars#18, ext_vars#19, test_id#20, raw_request_url#21, server_impression_time_ms#22L, is_p2c_enabled#23, ... 16 more fields]
   +- Filter (device_settings#4.user_agent = some user agent)
      +- Relation [transaction_header#0,user_identity#1,fraud_detection#2,geo_location#3,device_settings#4,connection#5,banner#6,rotator#7,interaction_id#8L,page_url#9,landing_url#10,publisher_domain#11,banner_click_url_id#12,keywords#13,rtb_vars#14,publisher_data#15,dco_vars#16,banner_vars#17,adx_vars#18,ext_vars#19,test_id#20,raw_request_url#21,server_impression_time_ms#22L,is_p2c_enabled#23,... 15 more fields] parquet

2025-01-20 12:37:50,360 - [ERROR] - Error during processing: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_type` cannot be resolved. Did you mean one of the following? [`test_id`, `client_id`, `ext_vars`, `rotator`, `banner`].;
'Pivot ArrayBuffer(hour#80), 'event_type, [impression, click], [count(1)]
+- Project [transaction_header#0, user_identity#1, fraud_detection#2, geo_location#3, device_settings#4, connection#5, banner#6, rotator#7, interaction_id#8L, page_url#9, landing_url#10, publisher_domain#11, banner_click_url_id#12, keywords#13, rtb_vars#14, publisher_data#15, dco_vars#16, banner_vars#17, adx_vars#18, ext_vars#19, test_id#20, raw_request_url#21, server_impression_time_ms#22L, is_p2c_enabled#23, ... 16 more fields]
   +- Filter (device_settings#4.user_agent = some user agent)
      +- Relation [transaction_header#0,user_identity#1,fraud_detection#2,geo_location#3,device_settings#4,connection#5,banner#6,rotator#7,interaction_id#8L,page_url#9,landing_url#10,publisher_domain#11,banner_click_url_id#12,keywords#13,rtb_vars#14,publisher_data#15,dco_vars#16,banner_vars#17,adx_vars#18,ext_vars#19,test_id#20,raw_request_url#21,server_impression_time_ms#22L,is_p2c_enabled#23,... 15 more fields] parquet

2025-01-20 12:37:50,360 - [ERROR] - Processing failed
2025-01-20 12:37:50,653 - [INFO] - Spark Session stopped
2025-01-20 12:38:47,330 - [INFO] - Starting application...
2025-01-20 12:38:47,330 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 12:38:47,330 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 12:39:14,847 - [INFO] - Spark Session created successfully
2025-01-20 12:39:20,138 - [ERROR] - Error processing files for date 2022-05-27: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_type` cannot be resolved. Did you mean one of the following? [`test_id`, `client_id`, `ext_vars`, `rotator`, `banner`].;
'Pivot ArrayBuffer(hour#80), 'event_type, [impression, click], [count(1)]
+- Project [transaction_header#0, user_identity#1, fraud_detection#2, geo_location#3, device_settings#4, connection#5, banner#6, rotator#7, interaction_id#8L, page_url#9, landing_url#10, publisher_domain#11, banner_click_url_id#12, keywords#13, rtb_vars#14, publisher_data#15, dco_vars#16, banner_vars#17, adx_vars#18, ext_vars#19, test_id#20, raw_request_url#21, server_impression_time_ms#22L, is_p2c_enabled#23, ... 16 more fields]
   +- Filter (device_settings#4.user_agent = some user agent)
      +- Relation [transaction_header#0,user_identity#1,fraud_detection#2,geo_location#3,device_settings#4,connection#5,banner#6,rotator#7,interaction_id#8L,page_url#9,landing_url#10,publisher_domain#11,banner_click_url_id#12,keywords#13,rtb_vars#14,publisher_data#15,dco_vars#16,banner_vars#17,adx_vars#18,ext_vars#19,test_id#20,raw_request_url#21,server_impression_time_ms#22L,is_p2c_enabled#23,... 15 more fields] parquet

2025-01-20 12:39:20,142 - [ERROR] - Error during processing: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_type` cannot be resolved. Did you mean one of the following? [`test_id`, `client_id`, `ext_vars`, `rotator`, `banner`].;
'Pivot ArrayBuffer(hour#80), 'event_type, [impression, click], [count(1)]
+- Project [transaction_header#0, user_identity#1, fraud_detection#2, geo_location#3, device_settings#4, connection#5, banner#6, rotator#7, interaction_id#8L, page_url#9, landing_url#10, publisher_domain#11, banner_click_url_id#12, keywords#13, rtb_vars#14, publisher_data#15, dco_vars#16, banner_vars#17, adx_vars#18, ext_vars#19, test_id#20, raw_request_url#21, server_impression_time_ms#22L, is_p2c_enabled#23, ... 16 more fields]
   +- Filter (device_settings#4.user_agent = some user agent)
      +- Relation [transaction_header#0,user_identity#1,fraud_detection#2,geo_location#3,device_settings#4,connection#5,banner#6,rotator#7,interaction_id#8L,page_url#9,landing_url#10,publisher_domain#11,banner_click_url_id#12,keywords#13,rtb_vars#14,publisher_data#15,dco_vars#16,banner_vars#17,adx_vars#18,ext_vars#19,test_id#20,raw_request_url#21,server_impression_time_ms#22L,is_p2c_enabled#23,... 15 more fields] parquet

2025-01-20 12:39:20,142 - [ERROR] - Processing failed
2025-01-20 12:39:20,277 - [INFO] - Spark Session stopped
2025-01-20 12:42:25,011 - [INFO] - Starting application...
2025-01-20 12:42:25,011 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 12:42:25,011 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 12:42:52,128 - [INFO] - Spark Session created successfully
2025-01-20 12:43:08,499 - [ERROR] - Error processing files for date 2022-05-26: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 12:43:08,500 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 12:43:08,500 - [ERROR] - Processing failed
2025-01-20 12:43:08,668 - [INFO] - Spark Session stopped
2025-01-20 12:46:28,001 - [INFO] - Starting application...
2025-01-20 12:46:28,001 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 12:46:28,001 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 12:46:54,856 - [INFO] - Spark Session created successfully
2025-01-20 12:47:10,992 - [ERROR] - Error processing files for date 2022-05-26: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 12:47:10,992 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 12:47:10,992 - [ERROR] - Processing failed
2025-01-20 12:47:11,364 - [INFO] - Spark Session stopped
2025-01-20 12:53:01,877 - [INFO] - Starting application...
2025-01-20 12:53:01,877 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 12:53:01,877 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 12:53:28,807 - [INFO] - Spark Session created successfully
2025-01-20 12:53:45,112 - [ERROR] - Error processing files for date 2022-05-27: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 12:53:45,113 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 12:53:45,113 - [ERROR] - Processing failed
2025-01-20 12:53:45,295 - [INFO] - Spark Session stopped
2025-01-20 12:55:53,779 - [INFO] - Starting application...
2025-01-20 12:55:53,779 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 12:55:53,779 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 12:56:20,722 - [INFO] - Spark Session created successfully
2025-01-20 12:56:36,634 - [ERROR] - Error processing files for date 2022-05-26: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 12:56:36,635 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 12:56:36,635 - [ERROR] - Processing failed
2025-01-20 12:56:36,734 - [INFO] - Spark Session stopped
2025-01-20 13:00:29,528 - [INFO] - Starting application...
2025-01-20 13:00:29,528 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 13:00:29,528 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 13:00:56,295 - [INFO] - Spark Session created successfully
2025-01-20 13:01:12,461 - [ERROR] - Error processing files for date 2022-05-26: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 13:01:12,462 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 13:01:12,462 - [ERROR] - Processing failed
2025-01-20 13:01:12,895 - [INFO] - Spark Session stopped
2025-01-20 13:03:22,047 - [INFO] - Starting application...
2025-01-20 13:03:22,047 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 13:03:22,047 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 13:03:27,642 - [INFO] - Spark Session created successfully
2025-01-20 13:03:43,611 - [ERROR] - Error processing files for date 2022-05-27: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 13:03:43,611 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 13:03:43,611 - [ERROR] - Processing failed
2025-01-20 13:03:44,118 - [INFO] - Spark Session stopped
2025-01-20 13:30:43,607 - [INFO] - Starting application...
2025-01-20 13:30:43,607 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 13:30:43,607 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 13:31:10,308 - [INFO] - Spark Session created successfully
2025-01-20 13:31:26,788 - [ERROR] - Error processing files for date 2022-05-26: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 13:31:26,789 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 13:31:26,789 - [ERROR] - Processing failed
2025-01-20 13:31:27,168 - [INFO] - Spark Session stopped
2025-01-20 13:51:45,538 - [INFO] - Starting application...
2025-01-20 13:51:45,538 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 13:51:45,538 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 13:52:12,908 - [INFO] - Spark Session created successfully
2025-01-20 13:52:29,781 - [ERROR] - Error processing files for date 2022-05-27: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 13:52:29,783 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 13:52:29,783 - [ERROR] - Processing failed
2025-01-20 13:52:29,986 - [INFO] - Spark Session stopped
2025-01-20 14:13:16,986 - [INFO] - Starting application...
2025-01-20 14:13:16,986 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 14:13:16,986 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 14:13:35,411 - [INFO] - Spark Session created successfully
2025-01-20 14:13:51,768 - [ERROR] - Error processing files for date 2022-05-27: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 14:13:51,769 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 14:13:51,769 - [ERROR] - Processing failed
2025-01-20 14:13:51,923 - [INFO] - Spark Session stopped
2025-01-20 14:19:17,578 - [INFO] - Starting application...
2025-01-20 14:19:17,588 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 14:19:17,588 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 14:19:44,325 - [INFO] - Spark Session created successfully
2025-01-20 14:20:00,547 - [ERROR] - Error processing files for date 2022-05-26: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 14:20:00,548 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 14:20:00,548 - [ERROR] - Processing failed
2025-01-20 14:20:00,885 - [INFO] - Spark Session stopped
2025-01-20 14:22:40,497 - [INFO] - Starting application...
2025-01-20 14:22:40,497 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 14:22:40,497 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 14:23:07,224 - [INFO] - Spark Session created successfully
2025-01-20 14:23:23,382 - [ERROR] - Error processing files for date 2022-05-26: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 14:23:23,383 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 14:23:23,383 - [ERROR] - Processing failed
2025-01-20 14:23:23,710 - [INFO] - Spark Session stopped
2025-01-20 14:27:03,413 - [INFO] - Starting application...
2025-01-20 14:27:03,413 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 14:27:03,413 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 14:27:29,995 - [ERROR] - Error during processing: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.io.FileNotFoundException: File file:/C:/tmp/spark-events does not exist

	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)

	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)

	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)

	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)

	at org.apache.spark.deploy.history.EventLogFileWriter.requireLogBaseDirAsDirectory(EventLogFileWriters.scala:77)

	at org.apache.spark.deploy.history.SingleEventLogFileWriter.start(EventLogFileWriters.scala:221)

	at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:81)

	at org.apache.spark.SparkContext.<init>(SparkContext.scala:637)

	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)

	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)

	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)

	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:238)

	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)

	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 162, in main
    spark = create_spark_session()
            ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 102, in create_spark_session
    .getOrCreate()
     ^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\session.py", line 497, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 515, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 203, in __init__
    self._do_init(
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 296, in _do_init
    self._jsc = jsc or self._initialize_context(self._conf._jconf)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 421, in _initialize_context
    return self._jvm.JavaSparkContext(jconf)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\java_gateway.py", line 1587, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.io.FileNotFoundException: File file:/C:/tmp/spark-events does not exist

	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)

	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)

	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)

	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)

	at org.apache.spark.deploy.history.EventLogFileWriter.requireLogBaseDirAsDirectory(EventLogFileWriters.scala:77)

	at org.apache.spark.deploy.history.SingleEventLogFileWriter.start(EventLogFileWriters.scala:221)

	at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:81)

	at org.apache.spark.SparkContext.<init>(SparkContext.scala:637)

	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)

	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)

	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)

	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:238)

	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)

	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)


2025-01-20 14:30:57,955 - [INFO] - Starting application...
2025-01-20 14:30:57,955 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 14:30:57,955 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 14:31:24,726 - [INFO] - Spark Session created successfully
2025-01-20 14:31:41,222 - [ERROR] - Error processing files for date 2022-05-26: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 14:31:41,222 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 14:31:41,222 - [ERROR] - Processing failed
2025-01-20 14:31:41,716 - [INFO] - Spark Session stopped
2025-01-20 14:34:31,050 - [INFO] - Starting application...
2025-01-20 14:34:31,050 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 14:34:31,050 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 14:35:30,761 - [INFO] - Spark Session created successfully
2025-01-20 14:35:53,890 - [ERROR] - Error processing files for date 2022-05-26: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 14:35:53,891 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 14:35:53,891 - [ERROR] - Processing failed
2025-01-20 14:35:54,309 - [INFO] - Spark Session stopped
2025-01-20 14:58:32,615 - [INFO] - Starting application...
2025-01-20 14:58:32,615 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 14:58:32,615 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 14:59:00,570 - [INFO] - Spark Session created successfully
2025-01-20 14:59:17,909 - [ERROR] - Error processing files for date 2022-05-26: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 14:59:17,910 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 14:59:17,910 - [ERROR] - Processing failed
2025-01-20 14:59:18,392 - [INFO] - Spark Session stopped
2025-01-20 15:17:33,710 - [INFO] - Starting application...
2025-01-20 15:17:33,710 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 15:17:33,710 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 15:18:02,602 - [INFO] - Spark Session created successfully
2025-01-20 15:18:20,775 - [ERROR] - Error processing files for date 2022-05-27: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 15:18:20,795 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 15:18:20,795 - [ERROR] - Processing failed
2025-01-20 15:18:20,946 - [INFO] - Spark Session stopped
2025-01-20 15:55:07,485 - [INFO] - Starting application...
2025-01-20 15:55:07,485 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 15:55:07,485 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 15:55:13,218 - [INFO] - Spark Session created successfully
2025-01-20 15:55:29,291 - [ERROR] - Error processing files for date 2022-05-26: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 15:55:29,292 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: org.apache.spark.SparkException: Python worker failed to connect back.

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more

Caused by: java.net.SocketTimeoutException: Accept timed out

	at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)

	at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)

	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)

	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)

	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)

	... 17 more


2025-01-20 15:55:29,292 - [ERROR] - Processing failed
2025-01-20 15:55:29,810 - [INFO] - Spark Session stopped
2025-01-20 16:00:45,655 - [INFO] - Starting application...
2025-01-20 16:00:45,655 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 16:00:45,655 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 16:01:12,279 - [INFO] - Spark Session created successfully
2025-01-20 16:01:25,999 - [INFO] - Successfully wrote output for date 2022-05-26
2025-01-20 16:01:25,999 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-20 16:01:26,000 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-20 16:01:26,000 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-20 16:01:37,765 - [INFO] - Successfully wrote output for date 2022-05-27
2025-01-20 16:01:37,766 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-20 16:01:37,766 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-20 16:01:37,767 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-20 16:01:37,767 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-20 16:01:37,767 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-20 16:01:37,768 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-20 16:01:37,768 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-20 16:01:37,768 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-20 16:01:37,768 - [INFO] - Processing completed successfully
2025-01-20 16:01:38,633 - [INFO] - Spark Session stopped
2025-01-20 16:23:49,399 - [INFO] - Starting application...
2025-01-20 16:23:49,399 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 16:23:49,399 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 16:24:16,189 - [INFO] - Spark Session created successfully
2025-01-20 16:24:16,189 - [WARNING] - No parquet files found in input directory
2025-01-20 16:24:16,189 - [ERROR] - Processing failed
2025-01-20 16:24:16,522 - [INFO] - Spark Session stopped
2025-01-20 16:29:21,015 - [INFO] - Starting application...
2025-01-20 16:29:21,015 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 16:29:21,015 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 16:29:47,779 - [INFO] - Spark Session created successfully
2025-01-20 16:30:02,133 - [ERROR] - Error writing output for date 2022-05-26: An error occurred while calling o143.csv.
: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)

	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)

	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)

	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)

	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)

	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)


2025-01-20 16:30:02,135 - [ERROR] - Error processing files for date 2022-05-26: An error occurred while calling o143.csv.
: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)

	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)

	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)

	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)

	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)

	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)


2025-01-20 16:30:02,135 - [ERROR] - Error during processing: An error occurred while calling o143.csv.
: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)

	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)

	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)

	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)

	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)

	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)


2025-01-20 16:30:02,135 - [ERROR] - Processing failed
2025-01-20 16:30:02,375 - [INFO] - Spark Session stopped
2025-01-20 16:30:26,855 - [INFO] - Starting application...
2025-01-20 16:30:26,855 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 16:30:26,855 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 16:30:53,431 - [INFO] - Spark Session created successfully
2025-01-20 16:31:13,551 - [ERROR] - Error writing output for date 2022-05-27: An error occurred while calling o213.csv.
: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)

	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)

	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)

	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)

	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)

	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)


2025-01-20 16:31:13,552 - [ERROR] - Error processing files for date 2022-05-27: An error occurred while calling o213.csv.
: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)

	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)

	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)

	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)

	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)

	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)


2025-01-20 16:31:13,553 - [ERROR] - Error during processing: An error occurred while calling o213.csv.
: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)

	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)

	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)

	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)

	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)

	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)


2025-01-20 16:31:13,553 - [ERROR] - Processing failed
2025-01-20 16:31:13,713 - [INFO] - Spark Session stopped
2025-01-20 16:32:15,927 - [INFO] - Starting application...
2025-01-20 16:32:15,927 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 16:32:15,927 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 16:32:42,901 - [INFO] - Spark Session created successfully
2025-01-20 16:33:03,554 - [ERROR] - Error writing output for date 2022-05-27: An error occurred while calling o213.csv.
: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)

	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)

	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)

	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)

	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)

	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)


2025-01-20 16:33:03,555 - [ERROR] - Error processing files for date 2022-05-27: An error occurred while calling o213.csv.
: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)

	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)

	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)

	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)

	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)

	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)


2025-01-20 16:33:03,556 - [ERROR] - Error during processing: An error occurred while calling o213.csv.
: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)

	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)

	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)

	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)

	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)

	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)


2025-01-20 16:33:03,556 - [ERROR] - Processing failed
2025-01-20 16:33:04,661 - [INFO] - Spark Session stopped
2025-01-20 16:36:01,656 - [INFO] - Starting application...
2025-01-20 16:36:01,656 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 16:36:01,656 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 16:36:28,565 - [INFO] - Spark Session created successfully
2025-01-20 16:36:42,838 - [ERROR] - Error writing output for date 2022-05-26: An error occurred while calling o141.csv.
: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)

	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)

	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)

	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)

	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)

	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)


2025-01-20 16:36:42,839 - [ERROR] - Error processing files for date 2022-05-26: An error occurred while calling o141.csv.
: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)

	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)

	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)

	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)

	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)

	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)


2025-01-20 16:36:42,839 - [ERROR] - Error during processing: An error occurred while calling o141.csv.
: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)

	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)

	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)

	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)

	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)

	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)

	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)

	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)


2025-01-20 16:36:42,840 - [ERROR] - Processing failed
2025-01-20 16:36:43,861 - [INFO] - Spark Session stopped
2025-01-20 16:38:42,429 - [INFO] - Starting application...
2025-01-20 16:38:42,429 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 16:38:42,429 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 16:39:09,121 - [INFO] - Spark Session created successfully
2025-01-20 16:39:28,717 - [INFO] - Successfully wrote output for date 2022-05-27
2025-01-20 16:39:28,717 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-20 16:39:28,718 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-20 16:39:28,718 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-20 16:39:28,719 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-20 16:39:28,719 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-20 16:39:28,720 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-20 16:39:28,720 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-20 16:39:28,720 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-20 16:39:34,456 - [INFO] - Successfully wrote output for date 2022-05-26
2025-01-20 16:39:34,456 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-20 16:39:34,457 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-20 16:39:34,457 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-20 16:39:34,457 - [INFO] - Processing completed successfully
2025-01-20 16:39:35,327 - [INFO] - Spark Session stopped
2025-01-20 16:42:04,978 - [INFO] - Starting application...
2025-01-20 16:42:04,979 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 16:42:04,979 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 16:42:32,038 - [INFO] - Spark Session created successfully
2025-01-20 16:42:32,038 - [WARNING] - No parquet files found in input directory
2025-01-20 16:42:32,038 - [ERROR] - Processing failed
2025-01-20 16:42:32,362 - [INFO] - Spark Session stopped
2025-01-20 16:43:25,356 - [INFO] - Starting application...
2025-01-20 16:43:25,356 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 16:43:25,356 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 16:43:31,230 - [INFO] - Spark Session created successfully
2025-01-20 16:43:45,085 - [INFO] - Successfully wrote output for date 2022-05-26
2025-01-20 16:43:45,085 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-20 16:43:45,086 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-20 16:43:45,086 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-20 16:43:56,354 - [INFO] - Successfully wrote output for date 2022-05-27
2025-01-20 16:43:56,355 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-20 16:43:56,355 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-20 16:43:56,355 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-20 16:43:56,356 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-20 16:43:56,356 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-20 16:43:56,356 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-20 16:43:56,356 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-20 16:43:56,356 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-20 16:43:56,356 - [INFO] - Processing completed successfully
2025-01-20 16:43:57,220 - [INFO] - Spark Session stopped
2025-01-20 16:50:30,810 - [INFO] - Starting application...
2025-01-20 16:50:30,810 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 16:50:30,810 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 16:50:57,604 - [INFO] - Spark Session created successfully
2025-01-20 16:51:11,334 - [INFO] - Successfully wrote output for date 2022-05-26
2025-01-20 16:51:11,334 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-20 16:51:11,335 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-20 16:51:11,335 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-20 16:51:22,677 - [INFO] - Successfully wrote output for date 2022-05-27
2025-01-20 16:51:22,677 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-20 16:51:22,678 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-20 16:51:22,678 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-20 16:51:22,679 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-20 16:51:22,679 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-20 16:51:22,679 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-20 16:51:22,680 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-20 16:51:22,680 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-20 16:51:22,681 - [INFO] - Processing completed successfully
2025-01-20 16:51:23,547 - [INFO] - Spark Session stopped
2025-01-20 17:04:08,766 - [INFO] - Starting application...
2025-01-20 17:04:08,766 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 17:04:08,766 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 17:04:16,023 - [INFO] - Spark Session created successfully
2025-01-20 17:04:31,940 - [ERROR] - Error processing files for date ('2022-05-27', 12): [UNSUPPORTED_FEATURE.LITERAL_TYPE] The feature is not supported: Literal for '[2022-05-27, 12]' of class java.util.ArrayList.
2025-01-20 17:04:31,943 - [ERROR] - Error during processing: [UNSUPPORTED_FEATURE.LITERAL_TYPE] The feature is not supported: Literal for '[2022-05-27, 12]' of class java.util.ArrayList.
2025-01-20 17:04:31,943 - [ERROR] - Processing failed
2025-01-20 17:04:32,655 - [INFO] - Spark Session stopped
2025-01-20 17:07:35,733 - [INFO] - Starting application...
2025-01-20 17:07:35,734 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 17:07:35,734 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 17:08:02,463 - [INFO] - Spark Session created successfully
2025-01-20 17:08:22,748 - [INFO] - Successfully wrote output for date 2022-05-27 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-20 17:08:22,748 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-20 17:08:22,749 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-20 17:08:22,749 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-20 17:08:22,750 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-20 17:08:22,750 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-20 17:08:22,752 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-20 17:08:22,752 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-20 17:08:22,753 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-20 17:08:28,611 - [INFO] - Successfully wrote output for date 2022-05-26 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-20 17:08:28,611 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-20 17:08:28,612 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-20 17:08:28,612 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-20 17:08:28,612 - [INFO] - Processing completed successfully
2025-01-20 17:08:29,456 - [INFO] - Spark Session stopped
2025-01-20 17:26:59,734 - [INFO] - Starting application...
2025-01-20 17:26:59,734 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 17:26:59,735 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 17:27:07,649 - [INFO] - Spark Session created successfully
2025-01-20 17:27:15,738 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-20 17:27:16,301 - [INFO] - Found 0 impressions for user agent: your user agent string
2025-01-20 17:27:25,155 - [INFO] - Total impressions: 0, Total clicks: 0 for date 2022-05-26
2025-01-20 17:27:25,670 - [INFO] - Successfully wrote output for date 2022-05-26 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-20 17:27:25,671 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-20 17:27:25,671 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-20 17:27:25,672 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-20 17:27:26,082 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-20 17:27:26,294 - [INFO] - Found 0 impressions for user agent: your user agent string
2025-01-20 17:27:28,945 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-20 17:27:29,179 - [INFO] - Found 0 clicks for user agent: your user agent string
2025-01-20 17:27:39,120 - [INFO] - Total impressions: 0, Total clicks: 0 for date 2022-05-27
2025-01-20 17:27:39,254 - [INFO] - Successfully wrote output for date 2022-05-27 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-20 17:27:39,254 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-20 17:27:39,255 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-20 17:27:39,256 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-20 17:27:39,256 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-20 17:27:39,257 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-20 17:27:39,257 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-20 17:27:39,258 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-20 17:27:39,258 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-20 17:27:39,258 - [INFO] - Processing completed successfully
2025-01-20 17:27:39,885 - [INFO] - Spark Session stopped
2025-01-20 17:32:38,419 - [INFO] - Starting application...
2025-01-20 17:32:38,419 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 17:32:38,420 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 17:33:06,033 - [INFO] - Spark Session created successfully
2025-01-20 17:33:13,743 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-20 17:33:14,332 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-20 17:33:16,784 - [ERROR] - Error processing files for date '2022-05-27': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-20 17:33:16,786 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:829)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-20 17:33:16,786 - [ERROR] - Processing failed
2025-01-20 17:33:17,334 - [INFO] - Spark Session stopped
2025-01-20 17:34:41,295 - [INFO] - Starting application...
2025-01-20 17:34:41,295 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-20 17:34:41,295 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-20 17:35:08,904 - [INFO] - Spark Session created successfully
2025-01-20 17:35:08,904 - [INFO] - Filtering for user agent: some user agent
2025-01-20 17:35:16,647 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-20 17:35:17,296 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-20 17:35:19,430 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-20 17:35:20,195 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-20 17:35:20,554 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-20 17:35:25,837 - [INFO] - Successfully wrote output for date 2022-05-26 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-20 17:35:26,234 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-20 17:35:26,480 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-20 17:35:28,123 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-20 17:35:28,548 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-20 17:35:28,836 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-20 17:35:29,187 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-20 17:35:29,567 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-20 17:35:31,308 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-20 17:35:31,756 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-20 17:35:32,185 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-20 17:35:37,673 - [INFO] - Successfully wrote output for date 2022-05-27 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-20 17:35:37,673 - [INFO] - Processing completed successfully
2025-01-20 17:35:38,836 - [INFO] - Spark Session stopped
2025-01-21 09:03:39,252 - [INFO] - Starting application...
2025-01-21 09:03:39,252 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 09:03:39,252 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 09:04:06,930 - [INFO] - Spark Session created successfully
2025-01-21 09:04:06,930 - [INFO] - Filtering for user agent: some user agent
2025-01-21 09:04:13,340 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 09:04:13,880 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-21 09:04:15,953 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 09:04:16,694 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 09:04:17,002 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 09:04:21,197 - [INFO] - Successfully wrote output for date 2022-05-26 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-21 09:04:21,505 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 09:04:21,681 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 09:04:22,954 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 09:04:23,287 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-21 09:04:23,472 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-21 09:04:23,722 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 09:04:23,915 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-21 09:04:25,300 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 09:04:25,657 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 09:04:25,884 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 09:04:29,608 - [INFO] - Successfully wrote output for date 2022-05-27 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-21 09:04:29,608 - [INFO] - Processing completed successfully
2025-01-21 09:04:30,041 - [INFO] - Spark Session stopped
2025-01-21 09:08:46,273 - [INFO] - Starting application...
2025-01-21 09:08:46,273 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 09:08:46,273 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 09:09:12,407 - [INFO] - Spark Session created successfully
2025-01-21 09:09:12,407 - [INFO] - Filtering for user agent: some user agent
2025-01-21 09:09:18,005 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 09:09:18,504 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-21 09:09:20,283 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 09:09:20,840 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 09:09:21,143 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 09:09:24,931 - [INFO] - Successfully wrote output for date 2022-05-26 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-21 09:09:25,212 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 09:09:25,374 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 09:09:26,691 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 09:09:27,074 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-21 09:09:27,251 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-21 09:09:27,512 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 09:09:27,706 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-21 09:09:29,052 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 09:09:29,352 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 09:09:29,669 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 09:09:32,982 - [INFO] - Successfully wrote output for date 2022-05-27 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-21 09:09:32,982 - [INFO] - Processing completed successfully
2025-01-21 09:09:33,391 - [INFO] - Spark Session stopped
2025-01-21 09:11:14,780 - [INFO] - Starting application...
2025-01-21 09:11:14,780 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 09:11:14,780 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 09:11:41,089 - [INFO] - Spark Session created successfully
2025-01-21 09:11:41,090 - [INFO] - Filtering for user agent: some user agent
2025-01-21 09:11:47,048 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 09:11:47,557 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 09:11:49,427 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 09:11:49,956 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-21 09:11:50,207 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-21 09:11:50,526 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 09:11:50,803 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-21 09:11:52,346 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 09:11:52,665 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 09:11:52,908 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 09:11:57,591 - [INFO] - Successfully wrote output for date 2022-05-27 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-21 09:11:57,873 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 09:11:58,084 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-21 09:11:59,224 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 09:11:59,491 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 09:11:59,647 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 09:12:01,881 - [INFO] - Successfully wrote output for date 2022-05-26 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-21 09:12:01,881 - [INFO] - Processing completed successfully
2025-01-21 09:12:02,503 - [INFO] - Spark Session stopped
2025-01-21 09:17:56,458 - [INFO] - Starting application...
2025-01-21 09:17:56,460 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 09:17:56,460 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 09:18:22,630 - [INFO] - Spark Session created successfully
2025-01-21 09:18:22,630 - [INFO] - Filtering for user agent: some user agent
2025-01-21 09:18:28,172 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 09:18:28,666 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-21 09:18:30,475 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 09:18:31,091 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 09:18:31,365 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 09:18:31,365 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-21 09:18:31,365 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 09:18:31,367 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-21 09:18:34,920 - [ERROR] - Error writing output for date 2022-05-26: An error occurred while calling o178.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 17) (host.docker.internal executor driver): org.apache.spark.SparkFileNotFoundException: File file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.

	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)

	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

Caused by: org.apache.spark.SparkFileNotFoundException: File file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.

	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)

	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


2025-01-21 09:18:34,920 - [ERROR] - Error processing files for date '2022-05-26': An error occurred while calling o178.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 17) (host.docker.internal executor driver): org.apache.spark.SparkFileNotFoundException: File file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.

	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)

	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

Caused by: org.apache.spark.SparkFileNotFoundException: File file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.

	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)

	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


2025-01-21 09:18:34,923 - [ERROR] - Error during processing: An error occurred while calling o178.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 17) (host.docker.internal executor driver): org.apache.spark.SparkFileNotFoundException: File file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.

	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)

	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

Caused by: org.apache.spark.SparkFileNotFoundException: File file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.

	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)

	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


2025-01-21 09:18:34,923 - [ERROR] - Processing failed
2025-01-21 09:18:35,250 - [INFO] - Spark Session stopped
2025-01-21 09:23:56,730 - [INFO] - Starting application...
2025-01-21 09:23:56,730 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 09:23:56,730 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 09:24:14,188 - [INFO] - Spark Session created successfully
2025-01-21 09:24:14,188 - [INFO] - Filtering for user agent: some user agent
2025-01-21 09:24:20,538 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 09:24:21,025 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-21 09:24:22,967 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 09:24:23,644 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 09:24:24,475 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 09:24:24,476 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-21 09:24:24,476 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 09:24:24,477 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-21 09:24:28,777 - [ERROR] - Error writing output for date 2022-05-26: An error occurred while calling o178.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 17) (host.docker.internal executor driver): org.apache.spark.SparkFileNotFoundException: File file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.

	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)

	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

Caused by: org.apache.spark.SparkFileNotFoundException: File file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.

	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)

	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


2025-01-21 09:24:28,777 - [ERROR] - Error processing files for date '2022-05-26': An error occurred while calling o178.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 17) (host.docker.internal executor driver): org.apache.spark.SparkFileNotFoundException: File file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.

	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)

	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

Caused by: org.apache.spark.SparkFileNotFoundException: File file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.

	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)

	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


2025-01-21 09:24:28,778 - [ERROR] - Error during processing: An error occurred while calling o178.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 17) (host.docker.internal executor driver): org.apache.spark.SparkFileNotFoundException: File file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.

	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)

	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

Caused by: org.apache.spark.SparkFileNotFoundException: File file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.

	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)

	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)

	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


2025-01-21 09:24:28,778 - [ERROR] - Processing failed
2025-01-21 09:24:29,052 - [INFO] - Spark Session stopped
2025-01-21 09:28:10,741 - [INFO] - Starting application...
2025-01-21 09:28:10,741 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 09:28:10,741 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 09:28:16,338 - [INFO] - Spark Session created successfully
2025-01-21 09:28:16,338 - [INFO] - Filtering for user agent: some user agent
2025-01-21 09:28:23,268 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 09:28:24,595 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 09:28:26,803 - [ERROR] - Error processing files for date '2022-05-27': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 09:28:26,804 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 09:28:26,804 - [ERROR] - Processing failed
2025-01-21 09:28:27,170 - [INFO] - Spark Session stopped
2025-01-21 09:28:54,501 - [INFO] - Starting application...
2025-01-21 09:28:54,501 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 09:28:54,501 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 09:29:20,840 - [INFO] - Spark Session created successfully
2025-01-21 09:29:20,840 - [INFO] - Filtering for user agent: some user agent
2025-01-21 09:29:26,431 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 09:29:26,990 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-21 09:29:29,635 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 09:29:30,188 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 09:29:30,467 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 09:29:34,475 - [INFO] - Successfully wrote output for date 2022-05-26 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-21 09:29:34,477 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-21 09:29:34,478 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 09:29:34,478 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-21 09:29:34,785 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 09:29:34,975 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 09:29:36,205 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 09:29:36,535 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-21 09:29:36,713 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-21 09:29:37,001 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 09:29:37,153 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-21 09:29:38,658 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 09:29:38,937 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 09:29:39,214 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 09:29:42,602 - [INFO] - Successfully wrote output for date 2022-05-27 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-21 09:29:42,602 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-21 09:29:42,603 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 09:29:42,603 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-21 09:29:42,604 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-21 09:29:42,604 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 09:29:42,605 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-21 09:29:42,606 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-21 09:29:42,606 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-21 09:29:42,606 - [INFO] - Processing completed successfully
2025-01-21 09:29:42,970 - [INFO] - Spark Session stopped
2025-01-21 10:19:27,559 - [INFO] - Starting application...
2025-01-21 10:19:27,560 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 10:19:27,560 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 10:19:40,986 - [INFO] - Spark Session created successfully
2025-01-21 10:19:40,986 - [INFO] - Spark version: 3.5.4
2025-01-21 10:19:40,987 - [INFO] - Spark master: local[*]
2025-01-21 10:19:40,987 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 10:19:40,987 - [INFO] - Filtering for user agent: some user agent
2025-01-21 10:19:46,305 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 10:19:46,819 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-21 10:19:48,722 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 10:19:49,348 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 10:19:49,630 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 10:19:53,555 - [INFO] - Successfully wrote output for date 2022-05-26 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-21 10:19:53,555 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-21 10:19:53,556 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 10:19:53,557 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-21 10:19:53,928 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 10:19:54,126 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 10:19:55,322 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 10:19:55,669 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-21 10:19:55,897 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-21 10:19:56,181 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 10:19:56,425 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-21 10:19:57,828 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 10:19:58,151 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 10:19:58,436 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 10:20:02,365 - [INFO] - Successfully wrote output for date 2022-05-27 to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-21 10:20:02,365 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-21 10:20:02,367 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 10:20:02,367 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-21 10:20:02,368 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-21 10:20:02,369 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 10:20:02,369 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-21 10:20:02,370 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-21 10:20:02,371 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-21 10:20:02,371 - [INFO] - Processing completed successfully
2025-01-21 10:20:02,824 - [INFO] - Spark Session stopped
2025-01-21 12:44:25,066 - [INFO] - Starting application...
2025-01-21 12:44:25,066 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 12:44:25,066 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 12:44:30,061 - [INFO] - Spark Session created successfully
2025-01-21 12:44:30,061 - [INFO] - Spark version: 3.5.4
2025-01-21 12:44:30,061 - [INFO] - Spark master: local[*]
2025-01-21 12:44:30,061 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 12:44:30,061 - [INFO] - Filtering for user agent: some user agent
2025-01-21 12:44:30,062 - [INFO] - Spark master: local[*]
2025-01-21 12:44:30,062 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 12:44:30,062 - [INFO] - Filtering for user agent: some user agent
2025-01-21 12:44:36,380 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 12:44:36,861 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 12:44:38,743 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 12:44:39,342 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10)]
2025-01-21 12:44:39,597 - [INFO] - Final hour distribution: [Row(hour=19, count=10)]
2025-01-21 12:44:43,592 - [ERROR] - Error writing output for date 2022-05-26 19:32: An error occurred while calling o178.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 20) (host.docker.internal executor driver): java.io.IOException: Mkdirs failed to create file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output/task1_output_2022-05-26 19:32.csv/_temporary/0/_temporary/attempt_202501211244435836006096783739024_0022_m_000000_20 (exists=false, cwd=file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)

	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)

	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)

	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.io.IOException: Mkdirs failed to create file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output/task1_output_2022-05-26 19:32.csv/_temporary/0/_temporary/attempt_202501211244435836006096783739024_0022_m_000000_20 (exists=false, cwd=file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)

	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)

	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)

	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more


2025-01-21 12:44:43,592 - [ERROR] - Error processing files for date '2022-05-26 19:32': An error occurred while calling o178.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 20) (host.docker.internal executor driver): java.io.IOException: Mkdirs failed to create file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output/task1_output_2022-05-26 19:32.csv/_temporary/0/_temporary/attempt_202501211244435836006096783739024_0022_m_000000_20 (exists=false, cwd=file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)

	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)

	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)

	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.io.IOException: Mkdirs failed to create file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output/task1_output_2022-05-26 19:32.csv/_temporary/0/_temporary/attempt_202501211244435836006096783739024_0022_m_000000_20 (exists=false, cwd=file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)

	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)

	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)

	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more


2025-01-21 12:44:43,593 - [ERROR] - Error during processing: An error occurred while calling o178.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 20) (host.docker.internal executor driver): java.io.IOException: Mkdirs failed to create file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output/task1_output_2022-05-26 19:32.csv/_temporary/0/_temporary/attempt_202501211244435836006096783739024_0022_m_000000_20 (exists=false, cwd=file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)

	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)

	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)

	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.io.IOException: Mkdirs failed to create file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output/task1_output_2022-05-26 19:32.csv/_temporary/0/_temporary/attempt_202501211244435836006096783739024_0022_m_000000_20 (exists=false, cwd=file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)

	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)

	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)

	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more


2025-01-21 12:44:43,593 - [ERROR] - Processing failed
2025-01-21 12:44:44,495 - [INFO] - Spark Session stopped
2025-01-21 12:45:32,460 - [INFO] - Starting application...
2025-01-21 12:45:32,460 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 12:45:32,460 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 12:45:58,274 - [INFO] - Spark Session created successfully
2025-01-21 12:45:58,275 - [INFO] - Spark version: 3.5.4
2025-01-21 12:45:58,275 - [INFO] - Spark master: local[*]
2025-01-21 12:45:58,275 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 12:45:58,275 - [INFO] - Filtering for user agent: some user agent
2025-01-21 12:45:58,275 - [INFO] - Spark master: local[*]
2025-01-21 12:45:58,275 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 12:45:58,275 - [INFO] - Filtering for user agent: some user agent
2025-01-21 12:46:03,297 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 12:46:03,736 - [INFO] - Found 10 clicks for user agent: some user agent
2025-01-21 12:46:05,329 - [ERROR] - Error processing files for date '2022-05-27 11:31': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 12:46:05,330 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 12:46:05,330 - [ERROR] - Processing failed
2025-01-21 12:46:05,724 - [INFO] - Spark Session stopped
2025-01-21 12:48:57,361 - [INFO] - Starting application...
2025-01-21 12:48:57,361 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 12:48:57,361 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 12:49:23,576 - [INFO] - Spark Session created successfully
2025-01-21 12:49:23,576 - [INFO] - Spark version: 3.5.4
2025-01-21 12:49:23,576 - [INFO] - Spark master: local[*]
2025-01-21 12:49:23,576 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 12:49:23,576 - [INFO] - Filtering for user agent: some user agent
2025-01-21 12:49:23,576 - [INFO] - Spark master: local[*]
2025-01-21 12:49:23,576 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 12:49:23,576 - [INFO] - Filtering for user agent: some user agent
2025-01-21 12:49:29,612 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 12:49:30,030 - [INFO] - Found 10 clicks for user agent: some user agent
2025-01-21 12:49:31,637 - [ERROR] - Error processing files for date '2022-05-27 11:31': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 12:49:31,638 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 12:49:31,638 - [ERROR] - Processing failed
2025-01-21 12:49:32,039 - [INFO] - Spark Session stopped
2025-01-21 12:52:57,363 - [INFO] - Starting application...
2025-01-21 12:52:57,363 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 12:52:57,363 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 12:53:23,921 - [INFO] - Spark Session created successfully
2025-01-21 12:53:23,922 - [INFO] - Spark version: 3.5.4
2025-01-21 12:53:23,922 - [INFO] - Spark master: local[*]
2025-01-21 12:53:23,922 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 12:53:23,922 - [INFO] - Filtering for user agent: some user agent
2025-01-21 12:53:23,922 - [INFO] - Spark master: local[*]
2025-01-21 12:53:23,922 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 12:53:23,922 - [INFO] - Filtering for user agent: some user agent
2025-01-21 12:53:28,942 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 12:53:29,381 - [INFO] - Found 10 clicks for user agent: some user agent
2025-01-21 12:53:31,039 - [ERROR] - Error processing files for date '2022-05-27 11:31': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 12:53:31,040 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 12:53:31,040 - [ERROR] - Processing failed
2025-01-21 12:53:31,331 - [INFO] - Spark Session stopped
2025-01-21 12:55:27,477 - [INFO] - Starting application...
2025-01-21 12:55:27,477 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 12:55:27,477 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 12:55:53,570 - [INFO] - Spark Session created successfully
2025-01-21 12:55:53,571 - [INFO] - Spark version: 3.5.4
2025-01-21 12:55:53,571 - [INFO] - Spark master: local[*]
2025-01-21 12:55:53,571 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 12:55:53,571 - [INFO] - Filtering for user agent: some user agent
2025-01-21 12:55:53,571 - [INFO] - Spark master: local[*]
2025-01-21 12:55:53,571 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 12:55:53,571 - [INFO] - Filtering for user agent: some user agent
2025-01-21 12:55:58,598 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 12:55:59,035 - [INFO] - Found 10 clicks for user agent: some user agent
2025-01-21 12:56:00,654 - [ERROR] - Error processing files for date '2022-05-27 12:01': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 12:56:00,656 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 12:56:00,656 - [ERROR] - Processing failed
2025-01-21 12:56:01,022 - [INFO] - Spark Session stopped
2025-01-21 13:06:50,257 - [INFO] - Starting application...
2025-01-21 13:06:50,258 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 13:06:50,258 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 13:07:16,221 - [INFO] - Spark Session created successfully
2025-01-21 13:07:16,221 - [INFO] - Spark version: 3.5.4
2025-01-21 13:07:16,221 - [INFO] - Spark master: local[*]
2025-01-21 13:07:16,221 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:07:16,221 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:07:16,221 - [INFO] - Spark master: local[*]
2025-01-21 13:07:16,222 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:07:16,222 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:07:21,319 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 13:07:21,761 - [INFO] - Found 4 impressions for user agent: some user agent
2025-01-21 13:07:23,509 - [ERROR] - Error processing files for date '2022-05-26 11:32': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 5) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 13:07:23,510 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 5) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 13:07:23,510 - [ERROR] - Processing failed
2025-01-21 13:07:23,668 - [INFO] - Spark Session stopped
2025-01-21 13:08:09,064 - [INFO] - Starting application...
2025-01-21 13:08:09,064 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 13:08:09,064 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 13:08:34,950 - [INFO] - Spark Session created successfully
2025-01-21 13:08:34,951 - [INFO] - Spark version: 3.5.4
2025-01-21 13:08:34,951 - [INFO] - Spark master: local[*]
2025-01-21 13:08:34,951 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:08:34,951 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:08:34,951 - [INFO] - Spark master: local[*]
2025-01-21 13:08:34,951 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:08:34,951 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:08:40,109 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 13:08:40,568 - [INFO] - Found 10 clicks for user agent: some user agent
2025-01-21 13:08:42,292 - [ERROR] - Error processing files for date '2022-05-27 12:01': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)

	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)

	at java.base/java.io.DataOutputStream.flush(DataOutputStream.java:123)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:454)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)

	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)

	at java.base/java.io.DataOutputStream.flush(DataOutputStream.java:123)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:454)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 13:08:42,293 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)

	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)

	at java.base/java.io.DataOutputStream.flush(DataOutputStream.java:123)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:454)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)

	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)

	at java.base/java.io.DataOutputStream.flush(DataOutputStream.java:123)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:454)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 13:08:42,293 - [ERROR] - Processing failed
2025-01-21 13:08:42,398 - [INFO] - Spark Session stopped
2025-01-21 13:09:24,950 - [INFO] - Starting application...
2025-01-21 13:09:24,951 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 13:09:24,951 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 13:09:47,887 - [INFO] - Spark Session created successfully
2025-01-21 13:09:47,888 - [INFO] - Spark version: 3.5.4
2025-01-21 13:09:47,888 - [INFO] - Spark master: local[*]
2025-01-21 13:09:47,888 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:09:47,888 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:09:47,888 - [INFO] - Spark master: local[*]
2025-01-21 13:09:47,888 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:09:47,889 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:10:01,123 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 13:10:02,592 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 13:10:07,500 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 13:10:09,378 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-21 13:10:10,304 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-21 13:10:10,990 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 13:10:11,339 - [INFO] - Found 10 clicks for user agent: some user agent
2025-01-21 13:10:14,922 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-21 13:10:15,799 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-21 13:10:16,437 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-21 13:10:27,080 - [ERROR] - Error writing output for date 2022-05-27 12:31: [WinError 267] The directory name is invalid: 'C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\\temp_2022-05-27 12:31_131027'
2025-01-21 13:10:27,081 - [ERROR] - Stack trace: Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\data_processing.py", line 272, in _write_output
    os.makedirs(temp_dir, exist_ok=True)
  File "<frozen os>", line 225, in makedirs
NotADirectoryError: [WinError 267] The directory name is invalid: 'C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\\temp_2022-05-27 12:31_131027'

2025-01-21 13:10:27,081 - [ERROR] - Error processing files for date '2022-05-27 12:31': [WinError 267] The directory name is invalid: 'C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\\temp_2022-05-27 12:31_131027'
2025-01-21 13:10:27,081 - [ERROR] - Error during processing: [WinError 267] The directory name is invalid: 'C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\\temp_2022-05-27 12:31_131027'
2025-01-21 13:10:27,082 - [ERROR] - Processing failed
2025-01-21 13:10:27,942 - [INFO] - Spark Session stopped
2025-01-21 13:13:30,063 - [INFO] - Starting application...
2025-01-21 13:13:30,064 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 13:13:30,064 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 13:13:55,948 - [INFO] - Spark Session created successfully
2025-01-21 13:13:55,948 - [INFO] - Spark version: 3.5.4
2025-01-21 13:13:55,948 - [INFO] - Spark master: local[*]
2025-01-21 13:13:55,949 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:13:55,949 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:13:55,949 - [INFO] - Spark master: local[*]
2025-01-21 13:13:55,949 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:13:55,949 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:14:00,923 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 13:14:01,437 - [INFO] - Found 10 clicks for user agent: some user agent
2025-01-21 13:14:03,043 - [ERROR] - Error processing files for date '2022-05-27 11:31': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 13:14:03,044 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 13:14:03,044 - [ERROR] - Processing failed
2025-01-21 13:14:03,412 - [INFO] - Spark Session stopped
2025-01-21 13:19:25,249 - [INFO] - Starting application...
2025-01-21 13:19:25,249 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 13:19:25,249 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 13:19:51,503 - [INFO] - Spark Session created successfully
2025-01-21 13:19:51,504 - [INFO] - Spark version: 3.5.4
2025-01-21 13:19:51,504 - [INFO] - Spark master: local[*]
2025-01-21 13:19:51,504 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:19:51,504 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:19:51,504 - [INFO] - Spark master: local[*]
2025-01-21 13:19:51,504 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:19:51,504 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:19:56,477 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 13:19:56,903 - [INFO] - Found 4 impressions for user agent: some user agent
2025-01-21 13:19:58,581 - [ERROR] - Error processing files for date '2022-05-26 11:32': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 5) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 13:19:58,582 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 5) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 13:19:58,582 - [ERROR] - Processing failed
2025-01-21 13:19:58,995 - [INFO] - Spark Session stopped
2025-01-21 13:22:37,323 - [INFO] - Starting application...
2025-01-21 13:22:37,324 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 13:22:37,324 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 13:23:03,724 - [INFO] - Spark Session created successfully
2025-01-21 13:23:03,725 - [INFO] - Spark version: 3.5.4
2025-01-21 13:23:03,725 - [INFO] - Spark master: local[*]
2025-01-21 13:23:03,725 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:23:03,725 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:23:03,725 - [INFO] - Spark master: local[*]
2025-01-21 13:23:03,725 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:23:03,725 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:23:09,008 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 13:23:09,442 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 13:23:11,150 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 13:23:11,699 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10)]
2025-01-21 13:23:11,922 - [INFO] - Final hour distribution: [Row(hour=19, count=10)]
2025-01-21 13:23:15,648 - [ERROR] - Error writing output for date 2022-05-26 19:32: An error occurred while calling o178.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 24.0 failed 1 times, most recent failure: Lost task 3.0 in stage 24.0 (TID 25) (host.docker.internal executor driver): java.io.IOException: Mkdirs failed to create file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output/task1_output_2022-05-26 19:32.csv/_temporary/0/_temporary/attempt_202501211323152800286551285545705_0024_m_000003_25 (exists=false, cwd=file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)

	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)

	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)

	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.io.IOException: Mkdirs failed to create file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output/task1_output_2022-05-26 19:32.csv/_temporary/0/_temporary/attempt_202501211323152800286551285545705_0024_m_000003_25 (exists=false, cwd=file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)

	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)

	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)

	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more


2025-01-21 13:23:15,649 - [ERROR] - Error processing files for date '2022-05-26 19:32': An error occurred while calling o178.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 24.0 failed 1 times, most recent failure: Lost task 3.0 in stage 24.0 (TID 25) (host.docker.internal executor driver): java.io.IOException: Mkdirs failed to create file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output/task1_output_2022-05-26 19:32.csv/_temporary/0/_temporary/attempt_202501211323152800286551285545705_0024_m_000003_25 (exists=false, cwd=file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)

	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)

	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)

	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.io.IOException: Mkdirs failed to create file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output/task1_output_2022-05-26 19:32.csv/_temporary/0/_temporary/attempt_202501211323152800286551285545705_0024_m_000003_25 (exists=false, cwd=file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)

	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)

	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)

	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more


2025-01-21 13:23:15,650 - [ERROR] - Error during processing: An error occurred while calling o178.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 24.0 failed 1 times, most recent failure: Lost task 3.0 in stage 24.0 (TID 25) (host.docker.internal executor driver): java.io.IOException: Mkdirs failed to create file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output/task1_output_2022-05-26 19:32.csv/_temporary/0/_temporary/attempt_202501211323152800286551285545705_0024_m_000003_25 (exists=false, cwd=file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)

	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)

	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)

	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)

	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)

	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)

	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)

	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)

	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)

	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)

	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)

	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)

	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)

	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)

	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.io.IOException: Mkdirs failed to create file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output/task1_output_2022-05-26 19:32.csv/_temporary/0/_temporary/attempt_202501211323152800286551285545705_0024_m_000003_25 (exists=false, cwd=file:/C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)

	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)

	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)

	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)

	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)

	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)

	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)

	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)

	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)

	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	... 1 more


2025-01-21 13:23:15,650 - [ERROR] - Processing failed
2025-01-21 13:23:16,588 - [INFO] - Spark Session stopped
2025-01-21 13:24:45,018 - [INFO] - Starting application...
2025-01-21 13:24:45,018 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 13:24:45,018 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 13:25:11,186 - [INFO] - Spark Session created successfully
2025-01-21 13:25:11,186 - [INFO] - Spark version: 3.5.4
2025-01-21 13:25:11,186 - [INFO] - Spark master: local[*]
2025-01-21 13:25:11,186 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:25:11,186 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:25:11,186 - [INFO] - Spark master: local[*]
2025-01-21 13:25:11,187 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:25:11,187 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:25:11,189 - [ERROR] - Error extracting date from filename clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet: unconverted data remains: 1131
2025-01-21 13:25:11,189 - [ERROR] - Error during processing: unconverted data remains: 1131
2025-01-21 13:25:11,189 - [ERROR] - Processing failed
2025-01-21 13:25:11,505 - [INFO] - Spark Session stopped
2025-01-21 13:28:44,155 - [INFO] - Starting application...
2025-01-21 13:28:44,155 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 13:28:44,155 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 13:29:10,337 - [INFO] - Spark Session created successfully
2025-01-21 13:29:10,337 - [INFO] - Spark version: 3.5.4
2025-01-21 13:29:10,337 - [INFO] - Spark master: local[*]
2025-01-21 13:29:10,337 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:29:10,337 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:29:10,338 - [INFO] - Spark master: local[*]
2025-01-21 13:29:10,338 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:29:10,338 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:29:15,306 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 13:29:15,709 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 13:29:17,359 - [ERROR] - Error processing files for date '2022-05-27 12:31': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 13:29:17,359 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 13:29:17,360 - [ERROR] - Processing failed
2025-01-21 13:29:17,759 - [INFO] - Spark Session stopped
2025-01-21 13:31:53,060 - [INFO] - Starting application...
2025-01-21 13:31:53,060 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 13:31:53,060 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 13:32:18,788 - [INFO] - Spark Session created successfully
2025-01-21 13:32:18,788 - [INFO] - Spark version: 3.5.4
2025-01-21 13:32:18,788 - [INFO] - Spark master: local[*]
2025-01-21 13:32:18,788 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:32:18,788 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:32:18,788 - [INFO] - Spark master: local[*]
2025-01-21 13:32:18,788 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 13:32:18,788 - [INFO] - Filtering for user agent: some user agent
2025-01-21 13:32:23,790 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 13:32:24,247 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-21 13:32:25,928 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 13:32:26,479 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 13:32:26,753 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 13:32:30,380 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26
2025-01-21 13:32:30,381 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-21 13:32:30,381 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 13:32:30,382 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-21 13:32:30,668 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 13:32:30,871 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 13:32:32,065 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 13:32:32,371 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-21 13:32:32,556 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-21 13:32:32,822 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 13:32:32,968 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-21 13:32:34,415 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 13:32:34,743 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 13:32:34,979 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 13:32:38,244 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27
2025-01-21 13:32:38,245 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-21 13:32:38,245 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 13:32:38,246 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-21 13:32:38,246 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-21 13:32:38,247 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 13:32:38,247 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-21 13:32:38,248 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-21 13:32:38,248 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-21 13:32:38,249 - [INFO] - Processing completed successfully
2025-01-21 13:32:38,692 - [INFO] - Spark Session stopped
2025-01-21 14:05:01,825 - [INFO] - Starting application...
2025-01-21 14:05:01,825 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 14:05:01,826 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 14:05:27,623 - [INFO] - Spark Session created successfully
2025-01-21 14:05:27,624 - [INFO] - Spark version: 3.5.4
2025-01-21 14:05:27,624 - [INFO] - Spark master: local[*]
2025-01-21 14:05:27,624 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 14:05:27,624 - [INFO] - Filtering for user agent: some user agent
2025-01-21 14:05:27,624 - [INFO] - Spark master: local[*]
2025-01-21 14:05:27,624 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 14:05:27,624 - [INFO] - Filtering for user agent: some user agent
2025-01-21 14:05:33,059 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 14:05:33,602 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 14:05:35,663 - [ERROR] - Error processing files for date '2022-05-27': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 14:05:35,663 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-21 14:05:35,663 - [ERROR] - Processing failed
2025-01-21 14:05:36,162 - [INFO] - Spark Session stopped
2025-01-21 14:07:02,182 - [INFO] - Starting application...
2025-01-21 14:07:02,182 - [INFO] - Using input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-21 14:07:02,182 - [INFO] - Using output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-21 14:07:28,482 - [INFO] - Spark Session created successfully
2025-01-21 14:07:28,483 - [INFO] - Spark version: 3.5.4
2025-01-21 14:07:28,483 - [INFO] - Spark master: local[*]
2025-01-21 14:07:28,483 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 14:07:28,483 - [INFO] - Filtering for user agent: some user agent
2025-01-21 14:07:28,483 - [INFO] - Spark master: local[*]
2025-01-21 14:07:28,483 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 14:07:28,483 - [INFO] - Filtering for user agent: some user agent
2025-01-21 14:07:33,356 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 14:07:33,823 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 14:07:35,469 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 14:07:36,006 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-21 14:07:36,248 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-21 14:07:36,583 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 14:07:36,787 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-21 14:07:38,323 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 14:07:38,648 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 14:07:38,879 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 14:07:43,730 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27
2025-01-21 14:07:43,731 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-21 14:07:43,732 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 14:07:43,732 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-21 14:07:43,733 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-21 14:07:43,733 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 14:07:43,734 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-21 14:07:43,734 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-21 14:07:43,735 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-21 14:07:44,010 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 14:07:44,208 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-21 14:07:45,416 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 14:07:45,739 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 14:07:45,927 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 14:07:48,083 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26
2025-01-21 14:07:48,084 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-21 14:07:48,084 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 14:07:48,085 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-21 14:07:48,085 - [INFO] - Processing completed successfully
2025-01-21 14:07:48,745 - [INFO] - Spark Session stopped
2025-01-21 21:56:08,493 - [INFO] - Starting application...
2025-01-21 21:56:43,900 - [INFO] - Spark Session created successfully
2025-01-21 21:56:43,901 - [INFO] - Spark version: 3.5.4
2025-01-21 21:56:43,901 - [INFO] - Spark master: local[*]
2025-01-21 21:56:43,901 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 21:56:43,901 - [INFO] - Filtering for user agent: some user agent
2025-01-21 21:56:43,901 - [INFO] - Spark master: local[*]
2025-01-21 21:56:43,901 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 21:56:43,901 - [INFO] - Filtering for user agent: some user agent
2025-01-21 21:56:50,736 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 21:56:51,200 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-21 21:56:52,916 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 21:56:53,535 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 21:56:53,847 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 21:56:58,392 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26
2025-01-21 21:56:58,393 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-21 21:56:58,393 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 21:56:58,394 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-21 21:56:58,681 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 21:56:58,866 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 21:57:00,076 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 21:57:00,408 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-21 21:57:00,609 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-21 21:57:00,930 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 21:57:01,079 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-21 21:57:02,419 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 21:57:02,716 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 21:57:02,931 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 21:57:06,238 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27
2025-01-21 21:57:06,239 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-21 21:57:06,239 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 21:57:06,240 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-21 21:57:06,241 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-21 21:57:06,242 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 21:57:06,242 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-21 21:57:06,243 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-21 21:57:06,243 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-21 21:57:06,243 - [INFO] - Starting warehouse loading process...
2025-01-21 21:57:06,306 - [ERROR] - Error creating schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/20/e3q8)
2025-01-21 21:57:06,306 - [ERROR] - Error in process_and_load_data: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 146, in __init__
    self._dbapi_connection = engine.raw_connection()
                             ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3298, in raw_connection
    return self.pool.connect()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 712, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 177, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 674, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 900, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 646, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 622, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 169, in process_and_load_data
    etl.create_schema()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 49, in create_schema
    with self.engine.connect() as conn:
         ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3274, in connect
    return self._connection_cls(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 148, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2439, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 146, in __init__
    self._dbapi_connection = engine.raw_connection()
                             ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3298, in raw_connection
    return self.pool.connect()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 712, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 177, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 674, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 900, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 646, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 622, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/20/e3q8)
2025-01-21 21:57:06,410 - [ERROR] - Processing or loading failed
2025-01-21 21:57:06,679 - [INFO] - Spark Session stopped
2025-01-21 22:42:25,303 - [INFO] - Starting application...
2025-01-21 22:42:44,394 - [INFO] - Spark Session created successfully
2025-01-21 22:42:44,395 - [INFO] - Spark version: 3.5.4
2025-01-21 22:42:44,395 - [INFO] - Spark master: local[*]
2025-01-21 22:42:44,395 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 22:42:44,395 - [INFO] - Filtering for user agent: some user agent
2025-01-21 22:42:44,395 - [INFO] - Spark master: local[*]
2025-01-21 22:42:44,395 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 22:42:44,395 - [INFO] - Filtering for user agent: some user agent
2025-01-21 22:42:50,789 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 22:42:51,366 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-21 22:42:53,372 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 22:42:54,219 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 22:42:54,565 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 22:42:59,589 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26
2025-01-21 22:42:59,589 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-21 22:42:59,590 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 22:42:59,591 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-21 22:42:59,978 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 22:43:00,200 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 22:43:01,784 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 22:43:02,197 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-21 22:43:02,440 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-21 22:43:02,777 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 22:43:03,065 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-21 22:43:04,831 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 22:43:05,289 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 22:43:05,596 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 22:43:09,330 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27
2025-01-21 22:43:09,331 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-21 22:43:09,331 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 22:43:09,332 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-21 22:43:09,333 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-21 22:43:09,334 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 22:43:09,334 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-21 22:43:09,335 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-21 22:43:09,336 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-21 22:43:09,336 - [INFO] - Starting warehouse loading process...
2025-01-21 22:43:09,397 - [ERROR] - Error creating schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-21 22:43:09,397 - [ERROR] - Error in process_and_load_data: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 173, in process_and_load_data
    etl.create_schema()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 49, in create_schema
    with self.engine.connect() as conn:
         ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-21 22:43:09,409 - [ERROR] - Processing or loading failed
2025-01-21 22:43:10,598 - [INFO] - Spark Session stopped
2025-01-21 23:04:41,584 - [INFO] - Starting application...
2025-01-21 23:05:07,991 - [INFO] - Spark Session created successfully
2025-01-21 23:05:07,992 - [INFO] - Spark version: 3.5.4
2025-01-21 23:05:07,992 - [INFO] - Spark master: local[*]
2025-01-21 23:05:07,992 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 23:05:07,992 - [INFO] - Filtering for user agent: some user agent
2025-01-21 23:05:07,992 - [INFO] - Spark master: local[*]
2025-01-21 23:05:07,992 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 23:05:07,992 - [INFO] - Filtering for user agent: some user agent
2025-01-21 23:05:07,992 - [WARNING] - No parquet files found in input directory
2025-01-21 23:05:07,992 - [ERROR] - Spark processing failed
2025-01-21 23:05:07,992 - [ERROR] - Processing or loading failed
2025-01-21 23:05:08,320 - [INFO] - Spark Session stopped
2025-01-21 23:23:18,765 - [INFO] - Starting application...
2025-01-21 23:23:45,153 - [INFO] - Spark Session created successfully
2025-01-21 23:23:45,153 - [INFO] - Spark version: 3.5.4
2025-01-21 23:23:45,154 - [INFO] - Spark master: local[*]
2025-01-21 23:23:45,154 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 23:23:45,154 - [INFO] - Filtering for user agent: some user agent
2025-01-21 23:23:45,154 - [INFO] - Spark master: local[*]
2025-01-21 23:23:45,154 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 23:23:45,154 - [INFO] - Filtering for user agent: some user agent
2025-01-21 23:23:45,154 - [WARNING] - No parquet files found in input directory
2025-01-21 23:23:45,154 - [ERROR] - Spark processing failed
2025-01-21 23:23:45,154 - [ERROR] - Processing or loading failed
2025-01-21 23:23:45,490 - [INFO] - Spark Session stopped
2025-01-21 23:24:40,854 - [INFO] - Starting application...
2025-01-21 23:24:56,912 - [INFO] - Spark Session created successfully
2025-01-21 23:24:56,913 - [INFO] - Spark version: 3.5.4
2025-01-21 23:24:56,913 - [INFO] - Spark master: local[*]
2025-01-21 23:24:56,913 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 23:24:56,913 - [INFO] - Filtering for user agent: some user agent
2025-01-21 23:24:56,913 - [INFO] - Spark master: local[*]
2025-01-21 23:24:56,913 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-21 23:24:56,913 - [INFO] - Filtering for user agent: some user agent
2025-01-21 23:25:02,649 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 23:25:03,137 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-21 23:25:05,065 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 23:25:05,657 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-21 23:25:05,933 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-21 23:25:06,298 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-21 23:25:06,543 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-21 23:25:08,108 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 23:25:08,459 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 23:25:08,719 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-21 23:25:13,850 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27
2025-01-21 23:25:13,850 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-21 23:25:13,851 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-21 23:25:13,852 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-21 23:25:13,852 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-21 23:25:13,852 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-21 23:25:13,854 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-21 23:25:13,855 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-21 23:25:13,855 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-21 23:25:14,177 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-21 23:25:14,365 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-21 23:25:15,771 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 23:25:16,064 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 23:25:16,227 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-21 23:25:18,670 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26
2025-01-21 23:25:18,670 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-21 23:25:18,671 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-21 23:25:18,671 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-21 23:25:18,671 - [INFO] - Starting warehouse loading process...
2025-01-21 23:25:18,727 - [ERROR] - Error creating schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-21 23:25:18,727 - [ERROR] - Error in process_and_load_data: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 175, in process_and_load_data
    etl.create_schema()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 51, in create_schema
    with self.engine.connect() as conn:
         ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-21 23:25:18,734 - [ERROR] - Processing or loading failed
2025-01-21 23:25:19,211 - [INFO] - Spark Session stopped
2025-01-22 01:19:54,924 - [INFO] - Starting application...
2025-01-22 01:20:21,267 - [INFO] - Spark Session created successfully
2025-01-22 01:20:21,267 - [INFO] - Reading input files from: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 01:20:22,801 - [ERROR] - Error in processing data: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for CSV. It must be specified manually.
2025-01-22 01:20:22,801 - [ERROR] - Processing or loading failed
2025-01-22 01:20:23,126 - [INFO] - Spark Session stopped
2025-01-22 01:20:50,731 - [INFO] - Starting application...
2025-01-22 01:21:16,982 - [INFO] - Spark Session created successfully
2025-01-22 01:21:16,982 - [INFO] - Reading input files from: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 01:21:18,454 - [ERROR] - Error in processing data: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for CSV. It must be specified manually.
2025-01-22 01:21:18,454 - [ERROR] - Processing or loading failed
2025-01-22 01:21:18,840 - [INFO] - Spark Session stopped
2025-01-22 01:21:38,766 - [INFO] - Starting application...
2025-01-22 01:21:44,213 - [INFO] - Spark Session created successfully
2025-01-22 01:21:44,213 - [INFO] - Reading input files from: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 01:21:45,742 - [ERROR] - Error in processing data: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for CSV. It must be specified manually.
2025-01-22 01:21:45,742 - [ERROR] - Processing or loading failed
2025-01-22 01:21:46,030 - [INFO] - Spark Session stopped
2025-01-22 01:21:47,562 - [INFO] - Starting application...
2025-01-22 01:22:14,802 - [INFO] - Spark Session created successfully
2025-01-22 01:22:14,803 - [INFO] - Reading input files from: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 01:22:16,457 - [ERROR] - Error in processing data: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for CSV. It must be specified manually.
2025-01-22 01:22:16,457 - [ERROR] - Processing or loading failed
2025-01-22 01:22:16,664 - [INFO] - Spark Session stopped
2025-01-22 01:23:23,121 - [INFO] - Starting application...
2025-01-22 01:23:50,047 - [INFO] - Spark Session created successfully
2025-01-22 01:23:50,047 - [INFO] - Reading input files from: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 01:23:51,683 - [ERROR] - Error in processing data: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for CSV. It must be specified manually.
2025-01-22 01:23:51,683 - [ERROR] - Processing or loading failed
2025-01-22 01:23:51,888 - [INFO] - Spark Session stopped
2025-01-22 01:27:38,805 - [INFO] - Starting application...
2025-01-22 01:27:54,978 - [INFO] - Spark Session created successfully
2025-01-22 01:27:54,978 - [INFO] - Reading input files from: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 01:27:56,530 - [ERROR] - Error in processing data: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for CSV. It must be specified manually.
2025-01-22 01:27:56,530 - [ERROR] - Processing or loading failed
2025-01-22 01:27:56,847 - [INFO] - Spark Session stopped
2025-01-22 01:28:16,560 - [INFO] - Starting application...
2025-01-22 01:28:21,948 - [INFO] - Spark Session created successfully
2025-01-22 01:28:21,948 - [INFO] - Reading input files from: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 01:28:23,531 - [ERROR] - Error in processing data: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for CSV. It must be specified manually.
2025-01-22 01:28:23,531 - [ERROR] - Processing or loading failed
2025-01-22 01:28:23,763 - [INFO] - Spark Session stopped
2025-01-22 01:31:19,504 - [INFO] - Starting application...
2025-01-22 01:31:45,938 - [INFO] - Spark Session created successfully
2025-01-22 01:31:45,940 - [INFO] - Spark version: 3.5.4
2025-01-22 01:31:45,940 - [INFO] - Spark master: local[*]
2025-01-22 01:31:45,940 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 01:31:45,940 - [INFO] - Filtering for user agent: some user agent
2025-01-22 01:31:45,940 - [INFO] - Spark master: local[*]
2025-01-22 01:31:45,940 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 01:31:45,940 - [INFO] - Filtering for user agent: some user agent
2025-01-22 01:31:45,940 - [WARNING] - No parquet files found in input directory
2025-01-22 01:31:45,941 - [ERROR] - Spark processing failed
2025-01-22 01:31:45,941 - [ERROR] - Processing or loading failed
2025-01-22 01:31:46,256 - [INFO] - Spark Session stopped
2025-01-22 01:35:23,045 - [INFO] - Starting application...
2025-01-22 01:35:50,064 - [INFO] - Spark Session created successfully
2025-01-22 01:35:50,065 - [INFO] - Spark version: 3.5.4
2025-01-22 01:35:50,065 - [INFO] - Spark master: local[*]
2025-01-22 01:35:50,065 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 01:35:50,065 - [INFO] - Filtering for user agent: some user agent
2025-01-22 01:35:50,065 - [INFO] - Spark master: local[*]
2025-01-22 01:35:50,065 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 01:35:50,065 - [INFO] - Filtering for user agent: some user agent
2025-01-22 01:35:50,065 - [WARNING] - No parquet files found in input directory
2025-01-22 01:35:50,065 - [ERROR] - Error in process_and_load_data: 'bool' object has no attribute 'coalesce'
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 166, in process_and_load_data
    ).coalesce(1)  # Ensure single partition
      ^^^^^^^^
AttributeError: 'bool' object has no attribute 'coalesce'
2025-01-22 01:35:50,065 - [ERROR] - Processing or loading failed
2025-01-22 01:35:50,383 - [INFO] - Spark Session stopped
2025-01-22 01:37:18,733 - [INFO] - Starting application...
2025-01-22 01:37:45,534 - [INFO] - Spark Session created successfully
2025-01-22 01:37:45,535 - [INFO] - Spark version: 3.5.4
2025-01-22 01:37:45,535 - [INFO] - Spark master: local[*]
2025-01-22 01:37:45,535 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 01:37:45,535 - [INFO] - Filtering for user agent: some user agent
2025-01-22 01:37:45,535 - [INFO] - Spark master: local[*]
2025-01-22 01:37:45,535 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 01:37:45,535 - [INFO] - Filtering for user agent: some user agent
2025-01-22 01:37:45,535 - [WARNING] - No parquet files found in input directory
2025-01-22 01:37:45,535 - [ERROR] - Spark processing failed
2025-01-22 01:37:45,535 - [ERROR] - Processing or loading failed
2025-01-22 01:37:45,855 - [INFO] - Spark Session stopped
2025-01-22 01:44:09,927 - [INFO] - Starting application...
2025-01-22 01:44:15,318 - [INFO] - Spark Session created successfully
2025-01-22 01:44:15,318 - [INFO] - Spark version: 3.5.4
2025-01-22 01:44:15,318 - [INFO] - Spark master: local[*]
2025-01-22 01:44:15,318 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 01:44:15,318 - [INFO] - Filtering for user agent: some user agent
2025-01-22 01:44:15,318 - [INFO] - Spark master: local[*]
2025-01-22 01:44:15,318 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 01:44:15,318 - [INFO] - Filtering for user agent: some user agent
2025-01-22 01:44:15,318 - [WARNING] - No parquet files found in input directory
2025-01-22 01:44:15,318 - [ERROR] - Spark processing failed
2025-01-22 01:44:15,318 - [ERROR] - Processing or loading failed
2025-01-22 01:44:15,638 - [INFO] - Spark Session stopped
2025-01-22 01:45:06,206 - [INFO] - Starting application...
2025-01-22 01:45:21,537 - [INFO] - Spark Session created successfully
2025-01-22 01:45:21,538 - [INFO] - Spark version: 3.5.4
2025-01-22 01:45:21,538 - [INFO] - Spark master: local[*]
2025-01-22 01:45:21,538 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 01:45:21,538 - [INFO] - Filtering for user agent: some user agent
2025-01-22 01:45:21,538 - [INFO] - Spark master: local[*]
2025-01-22 01:45:21,538 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 01:45:21,538 - [INFO] - Filtering for user agent: some user agent
2025-01-22 01:45:28,632 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 01:45:29,217 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 01:45:31,385 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 01:45:31,962 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 01:45:32,253 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 01:45:36,280 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26
2025-01-22 01:45:36,281 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 01:45:36,282 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 01:45:36,282 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 01:45:36,612 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 01:45:36,799 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 01:45:38,209 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 01:45:38,570 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 01:45:38,762 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 01:45:39,051 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 01:45:39,313 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 01:45:40,917 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 01:45:41,229 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 01:45:41,495 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 01:45:45,096 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27
2025-01-22 01:45:45,097 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 01:45:45,098 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 01:45:45,099 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 01:45:45,099 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 01:45:45,100 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 01:45:45,101 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 01:45:45,101 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 01:45:45,102 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 01:45:45,102 - [INFO] - Starting warehouse loading process...
2025-01-22 01:45:45,161 - [ERROR] - Error creating schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 01:45:45,161 - [ERROR] - Error in process_and_load_data: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 176, in process_and_load_data
    etl.create_schema()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 51, in create_schema
    with self.engine.connect() as conn:
         ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 01:45:45,173 - [ERROR] - Processing or loading failed
2025-01-22 01:45:45,503 - [INFO] - Spark Session stopped
2025-01-22 08:31:56,395 - [INFO] - Starting application...
2025-01-22 08:32:31,871 - [INFO] - Spark Session created successfully
2025-01-22 08:32:31,871 - [INFO] - Starting Task 1 - Spark Processing
2025-01-22 08:32:31,872 - [INFO] - Spark version: 3.5.4
2025-01-22 08:32:31,872 - [INFO] - Spark master: local[*]
2025-01-22 08:32:31,872 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 08:32:31,872 - [INFO] - Filtering for user agent: some user agent
2025-01-22 08:32:31,872 - [INFO] - Spark master: local[*]
2025-01-22 08:32:31,872 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 08:32:31,872 - [INFO] - Filtering for user agent: some user agent
2025-01-22 08:32:38,065 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 08:32:38,586 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 08:32:40,738 - [ERROR] - Error processing files for date '2022-05-27': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-22 08:32:40,738 - [ERROR] - Error during processing: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)

	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:566)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.net.SocketException: Connection reset by peer: socket write error

	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)

	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)

	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)

	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)

	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)

	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)

	at scala.collection.Iterator.foreach(Iterator.scala:943)

	at scala.collection.Iterator.foreach$(Iterator.scala:943)

	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)

	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)

	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)

	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)


2025-01-22 08:32:40,739 - [ERROR] - Task 1 - Spark processing failed
2025-01-22 08:32:40,739 - [ERROR] - Processing or loading failed
2025-01-22 08:32:40,826 - [INFO] - Spark Session stopped
2025-01-22 08:37:31,399 - [INFO] - Starting application...
2025-01-22 08:37:57,234 - [INFO] - Spark Session created successfully
2025-01-22 08:37:57,235 - [INFO] - Spark version: 3.5.4
2025-01-22 08:37:57,235 - [INFO] - Spark master: local[*]
2025-01-22 08:37:57,235 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 08:37:57,235 - [INFO] - Filtering for user agent: some user agent
2025-01-22 08:37:57,235 - [INFO] - Spark master: local[*]
2025-01-22 08:37:57,235 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 08:37:57,235 - [INFO] - Filtering for user agent: some user agent
2025-01-22 08:38:02,798 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 08:38:03,252 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 08:38:04,926 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 08:38:05,477 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 08:38:05,744 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 08:38:09,496 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26
2025-01-22 08:38:09,496 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 08:38:09,496 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 08:38:09,498 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 08:38:09,757 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 08:38:09,907 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 08:38:11,147 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 08:38:11,421 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 08:38:11,617 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 08:38:11,840 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 08:38:12,017 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 08:38:13,402 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 08:38:13,698 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 08:38:14,075 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 08:38:17,782 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27
2025-01-22 08:38:17,782 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 08:38:17,782 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 08:38:17,784 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 08:38:17,784 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 08:38:17,784 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 08:38:17,786 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 08:38:17,786 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 08:38:17,786 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 08:38:17,786 - [INFO] - Starting warehouse loading process...
2025-01-22 08:38:17,786 - [ERROR] - Error in process_and_load_data: 'ClientReportETL' object has no attribute 'create_schema'
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 177, in process_and_load_data
    etl.create_schema()
    ^^^^^^^^^^^^^^^^^
AttributeError: 'ClientReportETL' object has no attribute 'create_schema'
2025-01-22 08:38:17,788 - [ERROR] - Processing or loading failed
2025-01-22 08:38:18,947 - [INFO] - Spark Session stopped
2025-01-22 08:43:37,909 - [INFO] - Starting application...
2025-01-22 08:43:46,723 - [INFO] - Spark Session created successfully
2025-01-22 08:43:46,724 - [INFO] - Spark version: 3.5.4
2025-01-22 08:43:46,724 - [INFO] - Spark master: local[*]
2025-01-22 08:43:46,724 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 08:43:46,724 - [INFO] - Spark version: 3.5.4
2025-01-22 08:43:46,724 - [INFO] - Spark master: local[*]
2025-01-22 08:43:46,724 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 08:43:46,724 - [INFO] - Filtering for user agent: some user agent
2025-01-22 08:43:46,724 - [INFO] - Spark master: local[*]
2025-01-22 08:43:46,724 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 08:43:46,725 - [INFO] - Filtering for user agent: some user agent
2025-01-22 08:43:51,786 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 08:43:52,256 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 08:43:54,006 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 08:43:54,562 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 08:43:54,825 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 08:43:55,201 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 08:43:55,422 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 08:43:56,788 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 08:43:57,102 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 08:43:57,340 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 08:44:01,849 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27
2025-01-22 08:44:01,849 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 08:44:01,850 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 08:44:01,850 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 08:44:01,851 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 08:44:01,851 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 08:44:01,851 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 08:44:01,852 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 08:44:01,852 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 08:44:02,180 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 08:44:02,351 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 08:44:03,454 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 08:44:03,736 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 08:44:03,937 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 08:44:06,065 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26
2025-01-22 08:44:06,065 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 08:44:06,066 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 08:44:06,066 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 08:44:06,067 - [INFO] - Starting client report loading process...
2025-01-22 08:44:06,067 - [INFO] - Client report loading completed successfully
2025-01-22 08:44:06,067 - [INFO] - Processing and loading completed successfully
2025-01-22 08:44:06,720 - [INFO] - Spark Session stopped
2025-01-22 08:55:47,681 - [INFO] - Starting application...
2025-01-22 08:55:52,366 - [INFO] - Spark Session created successfully
2025-01-22 08:55:52,366 - [INFO] - Spark version: 3.5.4
2025-01-22 08:55:52,366 - [INFO] - Spark master: local[*]
2025-01-22 08:55:52,366 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 08:55:52,366 - [INFO] - Spark version: 3.5.4
2025-01-22 08:55:52,366 - [INFO] - Spark master: local[*]
2025-01-22 08:55:52,366 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 08:55:52,366 - [INFO] - Filtering for user agent: some user agent
2025-01-22 08:55:52,366 - [INFO] - Spark master: local[*]
2025-01-22 08:55:52,366 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 08:55:52,366 - [INFO] - Filtering for user agent: some user agent
2025-01-22 08:55:57,391 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 08:55:57,795 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 08:55:59,605 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 08:56:00,238 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 08:56:00,470 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 08:56:00,835 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 08:56:01,122 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 08:56:02,913 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 08:56:03,458 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 08:56:03,749 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 08:56:08,496 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27
2025-01-22 08:56:08,497 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 08:56:08,497 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 08:56:08,498 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 08:56:08,498 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 08:56:08,499 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 08:56:08,499 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 08:56:08,500 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 08:56:08,500 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 08:56:08,834 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 08:56:08,994 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 08:56:10,146 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 08:56:10,426 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 08:56:10,594 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 08:56:12,746 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26
2025-01-22 08:56:12,747 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 08:56:12,748 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 08:56:12,748 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 08:56:12,748 - [INFO] - Starting client report loading process...
2025-01-22 08:56:12,749 - [ERROR] - No files were processed
2025-01-22 08:56:12,749 - [ERROR] - Processing or loading failed
2025-01-22 08:56:13,366 - [INFO] - Spark Session stopped
2025-01-22 09:07:01,097 - [INFO] - Starting application...
2025-01-22 09:07:27,303 - [INFO] - Spark Session created successfully
2025-01-22 09:07:27,303 - [INFO] - Spark version: 3.5.4
2025-01-22 09:07:27,303 - [INFO] - Spark master: local[*]
2025-01-22 09:07:27,303 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 09:07:27,304 - [INFO] - Starting file processing...
2025-01-22 09:07:27,304 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 09:07:27,304 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 09:07:27,304 - [INFO] - User agent filter: some user agent
2025-01-22 09:07:27,305 - [INFO] - Found 11 total parquet files
2025-01-22 09:07:27,305 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 09:07:27,305 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 09:07:32,334 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 09:07:32,787 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 09:07:34,389 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 09:07:34,964 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 09:07:35,215 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 09:07:38,769 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 09:07:38,770 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 09:07:38,770 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 09:07:38,771 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 09:07:38,771 - [WARNING] - Failed to process data for date: 2022-05-26
2025-01-22 09:07:39,028 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 09:07:39,186 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 09:07:40,279 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 09:07:40,547 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 09:07:40,747 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 09:07:41,010 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 09:07:41,182 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 09:07:42,495 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 09:07:42,777 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 09:07:43,022 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 09:07:46,106 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 09:07:46,107 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 09:07:46,108 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 09:07:46,108 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 09:07:46,109 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 09:07:46,110 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 09:07:46,110 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 09:07:46,111 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 09:07:46,112 - [INFO] - Removed pr2025-01-22 09:07:46,112 - [WARNING] - Failed to process data for date: 2022-05-27
2025-01-22 09:07:46,112 - [ERROR] - No dates were successfully processed
2025-01-22 09:07:46,112 - [ERROR] - Spark processing failed
2025-01-22 09:07:46,112 - [ERROR] - Processing or loading failed
2025-01-22 09:07:46,664 - [INFO] - Spark Session stoppedocessed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet

2025-01-22 09:09:58,934 - [INFO] - Starting application...
2025-01-22 09:10:10,848 - [INFO] - Spark Session created successfully
2025-01-22 09:10:10,848 - [INFO] - Spark version: 3.5.4
2025-01-22 09:10:10,849 - [INFO] - Spark master: local[*]
2025-01-22 09:10:10,849 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 09:10:10,849 - [INFO] - Starting file processing...
2025-01-22 09:10:10,849 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 09:10:10,849 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 09:10:10,849 - [INFO] - User agent filter: some user agent
2025-01-22 09:10:10,850 - [INFO] - Found 11 total parquet files
2025-01-22 09:10:10,850 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 09:10:10,850 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 09:10:15,780 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 09:10:16,194 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 09:10:17,851 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 09:10:18,438 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 09:10:18,674 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 09:10:22,132 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 09:10:22,132 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 09:10:22,133 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 09:10:22,133 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 09:10:22,134 - [WARNING] - Failed to process data for date: 2022-05-26
2025-01-22 09:10:22,439 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 09:10:22,612 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 09:10:23,707 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 09:10:24,024 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 09:10:24,255 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 09:10:24,494 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 09:10:24,665 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 09:10:26,006 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 09:10:26,278 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 09:10:26,553 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 09:10:29,693 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 09:10:29,694 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 09:10:29,694 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 09:10:29,695 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 09:10:29,695 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 09:10:29,695 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 09:10:29,696 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 09:10:29,696 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 09:10:29,696 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 09:10:29,696 - [WARNING] - Failed to process data for date: 2022-05-27
2025-01-22 09:10:29,697 - [ERROR] - No dates were successfully processed
2025-01-22 09:10:29,697 - [ERROR] - Spark processing failed
2025-01-22 09:10:29,697 - [ERROR] - Processing or loading failed
2025-01-22 09:10:30,159 - [INFO] - Spark Session stopped
2025-01-22 09:13:56,263 - [INFO] - Starting application...
2025-01-22 09:14:22,206 - [INFO] - Spark Session created successfully
2025-01-22 09:14:22,207 - [INFO] - Spark version: 3.5.4
2025-01-22 09:14:22,207 - [INFO] - Spark master: local[*]
2025-01-22 09:14:22,207 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 09:14:22,207 - [INFO] - Starting file processing...
2025-01-22 09:14:22,207 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 09:14:22,207 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 09:14:22,207 - [INFO] - User agent filter: some user agent
2025-01-22 09:14:22,207 - [WARNING] - No parquet files found in input directory
2025-01-22 09:14:22,207 - [ERROR] - Spark processing failed
2025-01-22 09:14:22,207 - [ERROR] - Processing or loading failed
2025-01-22 09:14:22,550 - [INFO] - Spark Session stopped
2025-01-22 09:15:01,586 - [INFO] - Starting application...
2025-01-22 09:15:16,355 - [INFO] - Spark Session created successfully
2025-01-22 09:15:16,355 - [INFO] - Spark version: 3.5.4
2025-01-22 09:15:16,355 - [INFO] - Spark master: local[*]
2025-01-22 09:15:16,355 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 09:15:16,355 - [INFO] - Starting file processing...
2025-01-22 09:15:16,355 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 09:15:16,355 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 09:15:16,355 - [INFO] - User agent filter: some user agent
2025-01-22 09:15:16,356 - [INFO] - Found 11 total parquet files
2025-01-22 09:15:16,356 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 09:15:16,356 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 09:15:21,321 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 09:15:21,750 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 09:15:23,379 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 09:15:23,918 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 09:15:24,198 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 09:15:27,639 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 09:15:27,640 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 09:15:27,640 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 09:15:27,641 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 09:15:27,641 - [WARNING] - Failed to process data for date: 2022-05-26
2025-01-22 09:15:27,901 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 09:15:28,072 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 09:15:29,167 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 09:15:29,432 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 09:15:29,648 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 09:15:29,907 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 09:15:30,055 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 09:15:31,362 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 09:15:31,633 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 09:15:31,946 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 09:15:35,064 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 09:15:35,065 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 09:15:35,065 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 09:15:35,065 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 09:15:35,065 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 09:15:35,066 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 09:15:35,066 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 09:15:35,067 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 09:15:35,067 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 09:15:35,067 - [WARNING] - Failed to process data for date: 2022-05-27
2025-01-22 09:15:35,067 - [ERROR] - No dates were successfully processed
2025-01-22 09:15:35,068 - [ERROR] - Spark processing failed
2025-01-22 09:15:35,068 - [ERROR] - Processing or loading failed
2025-01-22 09:15:35,620 - [INFO] - Spark Session stopped
2025-01-22 09:43:11,185 - [INFO] - Starting application...
2025-01-22 09:43:11,264 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 09:43:11,265 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 253, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 23, in __init__
    self._ensure_schema()  # Ensure schema exists on initialization
    ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 28, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 09:43:50,253 - [INFO] - Starting application...
2025-01-22 09:43:50,321 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 09:43:50,322 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 253, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 23, in __init__
    self._ensure_schema()  # Ensure schema exists on initialization
    ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 28, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 09:46:18,465 - [INFO] - Starting application...
2025-01-22 09:46:18,537 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 09:46:18,537 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 253, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 23, in __init__
    self._ensure_schema()  # Ensure schema exists on initialization
    ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 28, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 10:49:02,580 - [INFO] - Starting application...
2025-01-22 10:49:02,651 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 10:49:02,651 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 253, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 23, in __init__
    self._ensure_schema()  # Ensure schema exists on initialization
    ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 28, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 10:50:28,020 - [INFO] - Starting application...
2025-01-22 10:50:28,845 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 10:50:28,845 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 253, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 23, in __init__
    self._ensure_schema()  # Ensure schema exists on initialization
    ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 28, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 10:50:31,402 - [INFO] - Starting application...
2025-01-22 10:50:31,541 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 10:50:31,542 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 253, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 23, in __init__
    self._ensure_schema()  # Ensure schema exists on initialization
    ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 28, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 10:51:21,485 - [INFO] - Starting application...
2025-01-22 10:51:22,109 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 10:51:22,109 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 253, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 23, in __init__
    self._ensure_schema()  # Ensure schema exists on initialization
    ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 28, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 10:56:13,433 - [INFO] - Starting application...
2025-01-22 10:56:13,504 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 10:56:13,505 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 253, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 23, in __init__
    self._ensure_schema()  # Ensure schema exists on initialization
    ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 28, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 11:38:40,223 - [INFO] - Starting application...
2025-01-22 11:38:44,387 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 11:38:44,387 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 253, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 23, in __init__
    self._ensure_schema()  # Ensure schema exists on initialization
    ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 28, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 11:39:24,897 - [INFO] - Starting application...
2025-01-22 11:39:24,958 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 11:39:51,551 - [INFO] - Spark Session created successfully
2025-01-22 11:39:51,552 - [INFO] - Spark version: 3.5.4
2025-01-22 11:39:51,552 - [INFO] - Spark master: local[*]
2025-01-22 11:39:51,552 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 11:39:51,552 - [INFO] - Starting file processing...
2025-01-22 11:39:51,552 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 11:39:51,553 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 11:39:51,553 - [INFO] - User agent filter: some user agent
2025-01-22 11:39:51,554 - [INFO] - Found 11 total parquet files
2025-01-22 11:39:51,554 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 11:39:51,554 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 11:39:56,928 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 11:39:57,366 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 11:39:59,077 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 11:39:59,634 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 11:39:59,888 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 11:40:03,544 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 11:40:03,544 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 11:40:03,545 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 11:40:03,546 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 11:40:03,546 - [WARNING] - Failed to process data for date: 2022-05-26
2025-01-22 11:40:03,805 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 11:40:03,955 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 11:40:05,053 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 11:40:05,349 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 11:40:05,532 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 11:40:05,831 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 11:40:05,985 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 11:40:07,336 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 11:40:07,584 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 11:40:07,838 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 11:40:11,007 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 11:40:11,007 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 11:40:11,008 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 11:40:11,008 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 11:40:11,008 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 11:40:11,009 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 11:40:11,009 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 11:40:11,010 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 11:40:11,010 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 11:40:11,010 - [WARNING] - Failed to process data for date: 2022-05-27
2025-01-22 11:40:11,010 - [ERROR] - No dates were successfully processed
2025-01-22 11:40:11,010 - [ERROR] - Spark processing failed
2025-01-22 11:40:11,010 - [ERROR] - Processing or loading failed
2025-01-22 11:40:11,568 - [INFO] - Spark Session stopped
2025-01-22 11:58:37,318 - [INFO] - Starting application...
2025-01-22 11:58:37,405 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 11:58:44,743 - [INFO] - Spark Session created successfully
2025-01-22 11:58:44,743 - [INFO] - Spark version: 3.5.4
2025-01-22 11:58:44,743 - [INFO] - Spark master: local[*]
2025-01-22 11:58:44,743 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 11:58:44,743 - [INFO] - Starting file processing...
2025-01-22 11:58:44,743 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 11:58:44,743 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 11:58:44,743 - [INFO] - User agent filter: some user agent
2025-01-22 11:58:44,744 - [INFO] - Found 11 total parquet files
2025-01-22 11:58:44,744 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 11:58:44,745 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 11:58:50,024 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 11:58:50,468 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 11:58:52,226 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 11:58:52,800 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 11:58:53,065 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 11:58:56,662 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 11:58:56,662 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 11:58:56,663 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 11:58:56,663 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 11:58:56,663 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 11:58:56,948 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 11:58:57,101 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 11:58:58,288 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 11:58:58,577 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 11:58:58,787 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 11:58:59,085 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 11:58:59,276 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 11:59:00,660 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 11:59:00,926 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 11:59:01,253 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 11:59:04,440 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 11:59:04,441 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 11:59:04,441 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 11:59:04,442 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 11:59:04,442 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 11:59:04,443 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 11:59:04,443 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 11:59:04,444 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 11:59:04,444 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 11:59:04,444 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 11:59:04,445 - [INFO] - Successfully processed 2 dates
2025-01-22 11:59:04,445 - [INFO] - Starting client report loading process...
2025-01-22 11:59:04,445 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 11:59:04,449 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 11:59:04,452 - [ERROR] - Error loading data: 'ClientReportETL' object has no attribute 'validate_data'
2025-01-22 11:59:04,452 - [ERROR] - Error processing C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv: 'ClientReportETL' object has no attribute 'validate_data'
2025-01-22 11:59:04,452 - [ERROR] - Processing or loading failed
2025-01-22 11:59:04,952 - [INFO] - Spark Session stopped
2025-01-22 12:03:57,065 - [INFO] - Starting application...
2025-01-22 12:03:57,125 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 12:04:03,045 - [INFO] - Spark Session created successfully
2025-01-22 12:04:03,045 - [INFO] - Spark version: 3.5.4
2025-01-22 12:04:03,045 - [INFO] - Spark master: local[*]
2025-01-22 12:04:03,045 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 12:04:03,045 - [INFO] - Starting file processing...
2025-01-22 12:04:03,045 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 12:04:03,046 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 12:04:03,046 - [INFO] - User agent filter: some user agent
2025-01-22 12:04:03,047 - [INFO] - Found 11 total parquet files
2025-01-22 12:04:03,047 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 12:04:03,047 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 12:04:08,457 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 12:04:08,962 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 12:04:10,642 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 12:04:11,206 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 12:04:11,458 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 12:04:15,018 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 12:04:15,019 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 12:04:15,019 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 12:04:15,020 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 12:04:15,020 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 12:04:15,278 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 12:04:15,437 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 12:04:16,575 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 12:04:16,875 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 12:04:17,075 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 12:04:17,366 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 12:04:17,570 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 12:04:18,916 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 12:04:19,187 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 12:04:19,383 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 12:04:22,492 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 12:04:22,493 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 12:04:22,494 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 12:04:22,494 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 12:04:22,495 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 12:04:22,495 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 12:04:22,495 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 12:04:22,496 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 12:04:22,496 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 12:04:22,497 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 12:04:22,497 - [INFO] - Successfully processed 2 dates
2025-01-22 12:04:22,497 - [INFO] - Starting client report loading process...
2025-01-22 12:04:22,497 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 12:04:22,502 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 12:04:22,506 - [ERROR] - Error loading data: 'ClientReportETL' object has no attribute 'validate_data'
2025-01-22 12:04:22,506 - [ERROR] - Error processing C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv: 'ClientReportETL' object has no attribute 'validate_data'
2025-01-22 12:04:22,506 - [ERROR] - Processing or loading failed
2025-01-22 12:04:23,005 - [INFO] - Spark Session stopped
2025-01-22 12:17:45,083 - [INFO] - Starting application...
2025-01-22 12:17:45,146 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 12:17:52,201 - [INFO] - Spark Session created successfully
2025-01-22 12:17:52,201 - [INFO] - Spark version: 3.5.4
2025-01-22 12:17:52,201 - [INFO] - Spark master: local[*]
2025-01-22 12:17:52,202 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 12:17:52,202 - [INFO] - Starting file processing...
2025-01-22 12:17:52,202 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 12:17:52,202 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 12:17:52,202 - [INFO] - User agent filter: some user agent
2025-01-22 12:17:52,203 - [INFO] - Found 11 total parquet files
2025-01-22 12:17:52,203 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 12:17:52,203 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 12:17:57,460 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 12:17:57,916 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 12:17:59,702 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 12:18:00,279 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 12:18:00,556 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 12:18:04,174 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 12:18:04,174 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 12:18:04,175 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 12:18:04,176 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 12:18:04,176 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 12:18:04,441 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 12:18:04,611 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 12:18:05,783 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 12:18:06,096 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 12:18:06,328 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 12:18:06,609 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 12:18:06,787 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 12:18:08,189 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 12:18:08,533 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 12:18:08,817 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 12:18:12,045 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 12:18:12,046 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 12:18:12,046 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 12:18:12,047 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 12:18:12,047 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 12:18:12,048 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 12:18:12,048 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 12:18:12,049 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 12:18:12,049 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 12:18:12,049 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 12:18:12,049 - [INFO] - Successfully processed 2 dates
2025-01-22 12:18:12,049 - [INFO] - Starting client report loading process...
2025-01-22 12:18:12,050 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 12:18:12,054 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 12:18:12,058 - [INFO] - Data validation passed
2025-01-22 12:18:12,060 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 12:18:12,095 - [INFO] - Successfully loaded 24 rows
2025-01-22 12:18:12,100 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-22 12:18:12,100 - [INFO] - - Archived rows: 0
2025-01-22 12:18:12,100 - [INFO] - - Loaded rows: 24
2025-01-22 12:18:12,100 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-22 12:18:12,104 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 12:18:12,108 - [ERROR] - Data validation failed:
Found records where clicks exceed impressions
2025-01-22 12:18:12,108 - [ERROR] - Error loading data: Data validation failed: Found records where clicks exceed impressions
2025-01-22 12:18:12,108 - [ERROR] - Error processing C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv: Data validation failed: Found records where clicks exceed impressions
2025-01-22 12:18:12,108 - [ERROR] - Processing or loading failed
2025-01-22 12:18:12,542 - [INFO] - Spark Session stopped
2025-01-22 12:22:52,184 - [INFO] - Starting application...
2025-01-22 12:22:52,260 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 12:23:01,386 - [INFO] - Spark Session created successfully
2025-01-22 12:23:01,386 - [INFO] - Spark version: 3.5.4
2025-01-22 12:23:01,386 - [INFO] - Spark master: local[*]
2025-01-22 12:23:01,386 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 12:23:01,387 - [INFO] - Starting file processing...
2025-01-22 12:23:01,387 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 12:23:01,387 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 12:23:01,387 - [INFO] - User agent filter: some user agent
2025-01-22 12:23:01,388 - [INFO] - Found 11 total parquet files
2025-01-22 12:23:01,388 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 12:23:01,388 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 12:23:06,939 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 12:23:07,412 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 12:23:09,290 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 12:23:09,845 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 12:23:10,142 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 12:23:13,775 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 12:23:13,776 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 12:23:13,776 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 12:23:13,777 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 12:23:13,777 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 12:23:14,035 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 12:23:14,207 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 12:23:15,412 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 12:23:15,779 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 12:23:15,967 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 12:23:16,247 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 12:23:16,452 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 12:23:17,919 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 12:23:18,323 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 12:23:18,571 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 12:23:21,824 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 12:23:21,825 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 12:23:21,825 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 12:23:21,826 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 12:23:21,826 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 12:23:21,827 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 12:23:21,827 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 12:23:21,828 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 12:23:21,828 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 12:23:21,828 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 12:23:21,828 - [INFO] - Successfully processed 2 dates
2025-01-22 12:23:21,828 - [INFO] - Starting client report loading process...
2025-01-22 12:23:21,829 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 12:23:21,833 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 12:23:21,839 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 12:23:21,878 - [INFO] - Successfully loaded 24 rows
2025-01-22 12:23:21,882 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-22 12:23:21,882 - [INFO] - - Archived rows: 24
2025-01-22 12:23:21,883 - [INFO] - - Loaded rows: 24
2025-01-22 12:23:21,883 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-22 12:23:21,887 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 12:23:21,891 - [WARNING] - Found 2 records where clicks exceed impressions
2025-01-22 12:23:21,902 - [INFO] - Stored 2 invalid records for later analysis
2025-01-22 12:23:21,902 - [ERROR] - Error loading data: Data validation failed: 
2025-01-22 12:23:21,903 - [ERROR] - Error processing C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv: Data validation failed: 
2025-01-22 12:23:21,903 - [ERROR] - Processing or loading failed
2025-01-22 12:23:22,383 - [INFO] - Spark Session stopped
2025-01-22 12:36:52,172 - [INFO] - Starting application...
2025-01-22 12:36:52,240 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 12:36:57,049 - [INFO] - Spark Session created successfully
2025-01-22 12:36:57,049 - [INFO] - Spark version: 3.5.4
2025-01-22 12:36:57,049 - [INFO] - Spark master: local[*]
2025-01-22 12:36:57,049 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 12:36:57,049 - [INFO] - Starting file processing...
2025-01-22 12:36:57,049 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 12:36:57,049 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 12:36:57,049 - [INFO] - User agent filter: some user agent
2025-01-22 12:36:57,050 - [INFO] - Found 11 total parquet files
2025-01-22 12:36:57,050 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 12:36:57,050 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 12:37:02,989 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 12:37:03,952 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 12:37:06,363 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 12:37:07,274 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 12:37:07,671 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 12:37:11,842 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 12:37:11,843 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 12:37:11,843 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 12:37:11,844 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 12:37:11,844 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 12:37:12,111 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 12:37:12,289 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 12:37:13,412 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 12:37:13,909 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 12:37:14,130 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 12:37:14,382 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 12:37:14,575 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 12:37:15,920 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 12:37:16,454 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 12:37:16,676 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 12:37:19,781 - [WARNING] - Found rows where clicks exceed impressions:
2025-01-22 12:37:20,618 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 12:37:20,619 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 12:37:20,619 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 12:37:20,620 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 12:37:20,621 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 12:37:20,621 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 12:37:20,621 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 12:37:20,622 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 12:37:20,622 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 12:37:20,622 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 12:37:20,622 - [INFO] - Successfully processed 2 dates
2025-01-22 12:37:20,622 - [INFO] - Starting client report loading process...
2025-01-22 12:37:20,623 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 12:37:20,627 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 12:37:20,632 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 12:37:20,636 - [ERROR] - Error loading data: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "client_report_archive_pkey"
DETAIL:  Key (datetime)=(2022-05-26 00:00:00) already exists.

[SQL: 
                    INSERT INTO adform_dw.client_report_archive 
                    SELECT * FROM adform_dw.client_report;
                ]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
2025-01-22 12:37:20,636 - [ERROR] - Error processing C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "client_report_archive_pkey"
DETAIL:  Key (datetime)=(2022-05-26 00:00:00) already exists.

[SQL: 
                    INSERT INTO adform_dw.client_report_archive 
                    SELECT * FROM adform_dw.client_report;
                ]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
2025-01-22 12:37:20,636 - [ERROR] - Processing or loading failed
2025-01-22 12:37:21,366 - [INFO] - Spark Session stopped
2025-01-22 12:46:09,187 - [INFO] - Starting application...
2025-01-22 12:46:09,252 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 12:46:34,933 - [INFO] - Spark Session created successfully
2025-01-22 12:46:34,934 - [INFO] - Spark version: 3.5.4
2025-01-22 12:46:34,934 - [INFO] - Spark master: local[*]
2025-01-22 12:46:34,934 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 12:46:34,934 - [INFO] - Starting file processing...
2025-01-22 12:46:34,934 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 12:46:34,934 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 12:46:34,934 - [INFO] - User agent filter: some user agent
2025-01-22 12:46:34,935 - [INFO] - Found 11 total parquet files
2025-01-22 12:46:34,935 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 12:46:34,935 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 12:46:39,962 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 12:46:40,440 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 12:46:42,244 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 12:46:43,198 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 12:46:43,619 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 12:46:48,628 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 12:46:48,629 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 12:46:48,629 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 12:46:48,630 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 12:46:48,630 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 12:46:48,876 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 12:46:49,049 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 12:46:50,193 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 12:46:50,636 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 12:46:50,820 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 12:46:51,104 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 12:46:51,270 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 12:46:52,584 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 12:46:53,019 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 12:46:53,223 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 12:46:57,187 - [WARNING] - Found records where clicks exceed impressions:
2025-01-22 12:46:57,923 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 12:46:57,923 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 12:46:57,924 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 12:46:57,924 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 12:46:57,925 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 12:46:57,925 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 12:46:57,926 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 12:46:57,927 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 12:46:57,927 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 12:46:57,927 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 12:46:57,927 - [INFO] - Successfully processed 2 dates
2025-01-22 12:46:57,927 - [INFO] - Starting client report loading process...
2025-01-22 12:46:57,928 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 12:46:57,930 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 12:46:57,930 - [ERROR] - Error loading data: 'datetime'
2025-01-22 12:46:57,930 - [ERROR] - Error processing C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv: 'datetime'
2025-01-22 12:46:57,930 - [ERROR] - Processing or loading failed
2025-01-22 12:46:58,170 - [INFO] - Spark Session stopped
2025-01-22 12:54:30,047 - [INFO] - Starting application...
2025-01-22 12:54:30,108 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 12:54:48,363 - [INFO] - Spark Session created successfully
2025-01-22 12:54:48,363 - [INFO] - Spark version: 3.5.4
2025-01-22 12:54:48,363 - [INFO] - Spark master: local[*]
2025-01-22 12:54:48,363 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 12:54:48,364 - [INFO] - Starting file processing...
2025-01-22 12:54:48,364 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 12:54:48,364 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 12:54:48,364 - [INFO] - User agent filter: some user agent
2025-01-22 12:54:48,365 - [INFO] - Found 11 total parquet files
2025-01-22 12:54:48,365 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 12:54:48,365 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 12:54:53,587 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 12:54:54,024 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 12:54:55,680 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 12:54:56,570 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 12:54:56,946 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 12:55:01,639 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 12:55:01,639 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 12:55:01,639 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 12:55:01,640 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 12:55:01,640 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 12:55:01,887 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 12:55:02,050 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 12:55:03,190 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 12:55:03,612 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 12:55:03,844 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 12:55:04,094 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 12:55:04,281 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 12:55:05,702 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 12:55:06,130 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 12:55:06,381 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 12:55:10,317 - [WARNING] - Found records where clicks exceed impressions:
2025-01-22 12:55:11,044 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 12:55:11,044 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 12:55:11,044 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 12:55:11,045 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 12:55:11,045 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 12:55:11,046 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 12:55:11,046 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 12:55:11,047 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 12:55:11,047 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 12:55:11,047 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 12:55:11,047 - [INFO] - Successfully processed 2 dates
2025-01-22 12:55:11,047 - [INFO] - Starting client report loading process...
2025-01-22 12:55:11,048 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 12:55:11,049 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 12:55:11,055 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 12:55:11,060 - [INFO] - Archived 0 unique rows
2025-01-22 12:55:11,096 - [INFO] - Successfully loaded 24 rows
2025-01-22 12:55:11,102 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-22 12:55:11,102 - [INFO] - - Archived rows: 0
2025-01-22 12:55:11,102 - [INFO] - - Loaded rows: 24
2025-01-22 12:55:11,102 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-22 12:55:11,104 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 12:55:11,107 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 12:55:11,110 - [INFO] - Archived 0 unique rows
2025-01-22 12:55:11,132 - [INFO] - Successfully loaded 24 rows
2025-01-22 12:55:11,137 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-22 12:55:11,137 - [INFO] - - Archived rows: 0
2025-01-22 12:55:11,137 - [INFO] - - Loaded rows: 24
2025-01-22 12:55:11,137 - [INFO] - 
Verifying data load...
2025-01-22 12:55:11,137 - [ERROR] - Error in process_and_load_data: 'ClientReportETL' object has no attribute 'verify_load'
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 196, in process_and_load_data
    verification = etl.verify_load()
                   ^^^^^^^^^^^^^^^
AttributeError: 'ClientReportETL' object has no attribute 'verify_load'
2025-01-22 12:55:11,137 - [ERROR] - Processing or loading failed
2025-01-22 12:55:11,306 - [INFO] - Spark Session stopped
2025-01-22 13:04:00,278 - [INFO] - Starting application...
2025-01-22 13:04:00,344 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 13:04:10,979 - [INFO] - Spark Session created successfully
2025-01-22 13:04:10,979 - [INFO] - Spark version: 3.5.4
2025-01-22 13:04:10,979 - [INFO] - Spark master: local[*]
2025-01-22 13:04:10,979 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 13:04:10,979 - [INFO] - Starting file processing...
2025-01-22 13:04:10,979 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 13:04:10,979 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 13:04:10,980 - [INFO] - User agent filter: some user agent
2025-01-22 13:04:10,981 - [INFO] - Found 11 total parquet files
2025-01-22 13:04:10,981 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 13:04:10,981 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 13:04:16,029 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 13:04:16,488 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 13:04:18,215 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 13:04:19,084 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 13:04:19,505 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 13:04:24,373 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 13:04:24,374 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 13:04:24,374 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 13:04:24,375 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 13:04:24,375 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 13:04:24,676 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 13:04:24,822 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 13:04:25,965 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 13:04:26,415 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 13:04:26,629 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 13:04:26,902 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 13:04:27,094 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 13:04:28,434 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 13:04:28,853 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 13:04:29,063 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 13:04:32,934 - [WARNING] - Found records where clicks exceed impressions:
2025-01-22 13:04:33,672 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 13:04:33,673 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 13:04:33,673 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 13:04:33,674 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 13:04:33,674 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 13:04:33,675 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 13:04:33,675 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 13:04:33,676 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 13:04:33,677 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 13:04:33,677 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 13:04:33,677 - [INFO] - Successfully processed 2 dates
2025-01-22 13:04:33,677 - [INFO] - Starting client report loading process...
2025-01-22 13:04:33,678 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 13:04:33,680 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 13:04:33,685 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 13:04:33,691 - [INFO] - Archived 24 unique rows
2025-01-22 13:04:33,723 - [INFO] - Successfully loaded 24 rows
2025-01-22 13:04:33,728 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-22 13:04:33,728 - [INFO] - - Archived rows: 24
2025-01-22 13:04:33,728 - [INFO] - - Loaded rows: 24
2025-01-22 13:04:33,729 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-22 13:04:33,732 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 13:04:33,736 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 13:04:33,740 - [INFO] - Archived 0 unique rows
2025-01-22 13:04:33,765 - [INFO] - Successfully loaded 24 rows
2025-01-22 13:04:33,770 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-22 13:04:33,770 - [INFO] - - Archived rows: 0
2025-01-22 13:04:33,770 - [INFO] - - Loaded rows: 24
2025-01-22 13:04:33,771 - [INFO] - 
Verifying data load...
2025-01-22 13:04:33,776 - [INFO] - Verification results: {'total_rows': 24, 'date_range': {'start': datetime.datetime(2022, 5, 27, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('30'), 'clicks': Decimal('30')}}
2025-01-22 13:04:33,777 - [INFO] - 
Load Summary:
2025-01-22 13:04:33,777 - [INFO] - Total files processed: 2
2025-01-22 13:04:33,777 - [INFO] - Total rows loaded: 48
2025-01-22 13:04:33,777 - [INFO] - 
Date Range:
2025-01-22 13:04:33,777 - [INFO] -   Start: 2022-05-27 00:00:00
2025-01-22 13:04:33,777 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-22 13:04:33,777 - [INFO] - 
Total Counts:
2025-01-22 13:04:33,777 - [INFO] -   Impressions: 30
2025-01-22 13:04:33,777 - [INFO] -   Clicks: 30
2025-01-22 13:04:33,777 - [INFO] - 
Hourly Breakdown:
2025-01-22 13:04:33,777 - [ERROR] - Error in process_and_load_data: 'hourly_breakdown'
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 209, in process_and_load_data
    logger.info(f"\n{verification['hourly_breakdown'].to_string()}")
                     ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
KeyError: 'hourly_breakdown'
2025-01-22 13:04:33,778 - [ERROR] - Processing or loading failed
2025-01-22 13:04:34,929 - [INFO] - Spark Session stopped
2025-01-22 13:07:00,848 - [INFO] - Starting application...
2025-01-22 13:07:00,907 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 13:07:05,863 - [INFO] - Spark Session created successfully
2025-01-22 13:07:05,863 - [INFO] - Spark version: 3.5.4
2025-01-22 13:07:05,863 - [INFO] - Spark master: local[*]
2025-01-22 13:07:05,864 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 13:07:05,864 - [INFO] - Starting file processing...
2025-01-22 13:07:05,864 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 13:07:05,864 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 13:07:05,864 - [INFO] - User agent filter: some user agent
2025-01-22 13:07:05,866 - [INFO] - Found 11 total parquet files
2025-01-22 13:07:05,866 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 13:07:05,866 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 13:07:11,985 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 13:07:12,564 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 13:07:14,679 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 13:07:15,540 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 13:07:15,933 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 13:07:20,605 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 13:07:20,605 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 13:07:20,606 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 13:07:20,606 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 13:07:20,607 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 13:07:20,867 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 13:07:21,040 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 13:07:22,126 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 13:07:22,589 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 13:07:22,787 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 13:07:23,055 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 13:07:23,249 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 13:07:24,568 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 13:07:25,006 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 13:07:25,209 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 13:07:29,151 - [WARNING] - Found records where clicks exceed impressions:
2025-01-22 13:07:29,925 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 13:07:29,925 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 13:07:29,926 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 13:07:29,926 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 13:07:29,927 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 13:07:29,928 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 13:07:29,928 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 13:07:29,929 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 13:07:29,929 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 13:07:29,930 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 13:07:29,930 - [INFO] - Successfully processed 2 dates
2025-01-22 13:07:29,930 - [INFO] - Starting client report loading process...
2025-01-22 13:07:29,930 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 13:07:29,932 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 13:07:29,939 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 13:07:29,945 - [INFO] - Archived 0 unique rows
2025-01-22 13:07:29,981 - [INFO] - Successfully loaded 24 rows
2025-01-22 13:07:29,987 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-22 13:07:29,987 - [INFO] - - Archived rows: 0
2025-01-22 13:07:29,987 - [INFO] - - Loaded rows: 24
2025-01-22 13:07:29,987 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-22 13:07:29,990 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 13:07:29,995 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 13:07:29,999 - [INFO] - Archived 0 unique rows
2025-01-22 13:07:30,024 - [INFO] - Successfully loaded 24 rows
2025-01-22 13:07:30,029 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-22 13:07:30,029 - [INFO] - - Archived rows: 0
2025-01-22 13:07:30,029 - [INFO] - - Loaded rows: 24
2025-01-22 13:07:30,029 - [INFO] - 
Verifying data load...
2025-01-22 13:07:30,034 - [INFO] - Verification results: {'total_rows': 24, 'date_range': {'start': datetime.datetime(2022, 5, 27, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('30'), 'clicks': Decimal('30')}}
2025-01-22 13:07:30,035 - [INFO] - 
Load Summary:
2025-01-22 13:07:30,035 - [INFO] - Total files processed: 2
2025-01-22 13:07:30,035 - [INFO] - Total rows loaded: 48
2025-01-22 13:07:30,035 - [INFO] - 
Date Range:
2025-01-22 13:07:30,035 - [INFO] -   Start: 2022-05-27 00:00:00
2025-01-22 13:07:30,035 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-22 13:07:30,035 - [INFO] - 
Total Counts:
2025-01-22 13:07:30,035 - [INFO] -   Impressions: 30
2025-01-22 13:07:30,035 - [INFO] -   Clicks: 30
2025-01-22 13:07:30,035 - [INFO] - 
Client report loading completed successfully
2025-01-22 13:07:30,035 - [INFO] - Processing and loading completed successfully
2025-01-22 13:07:31,147 - [INFO] - Spark Session stopped
2025-01-22 14:20:33,584 - [INFO] - Starting application...
2025-01-22 14:20:33,652 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 14:20:38,563 - [INFO] - Spark Session created successfully
2025-01-22 14:20:38,563 - [INFO] - Spark version: 3.5.4
2025-01-22 14:20:38,563 - [INFO] - Spark master: local[*]
2025-01-22 14:20:38,564 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 14:20:38,564 - [INFO] - Starting file processing...
2025-01-22 14:20:38,564 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 14:20:38,564 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 14:20:38,564 - [INFO] - User agent filter: some user agent
2025-01-22 14:20:38,565 - [INFO] - Found 11 total parquet files
2025-01-22 14:20:38,565 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 14:20:38,565 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 14:20:45,107 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 14:20:45,680 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 14:20:47,448 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 14:20:48,387 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 14:20:48,779 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 14:20:53,743 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 14:20:53,743 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 14:20:53,744 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 14:20:53,744 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 14:20:53,744 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 14:20:54,040 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 14:20:54,216 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 14:20:55,406 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 14:20:55,944 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 14:20:56,164 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 14:20:56,478 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 14:20:56,658 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 14:20:58,053 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 14:20:58,492 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 14:20:58,744 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 14:21:02,667 - [WARNING] - Found records where clicks exceed impressions:
2025-01-22 14:21:03,437 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 14:21:03,437 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 14:21:03,438 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 14:21:03,438 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 14:21:03,439 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 14:21:03,439 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 14:21:03,440 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 14:21:03,440 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 14:21:03,441 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 14:21:03,441 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 14:21:03,441 - [INFO] - Successfully processed 2 dates
2025-01-22 14:21:03,441 - [INFO] - Starting client report loading process...
2025-01-22 14:21:03,441 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 14:21:03,445 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 14:21:03,456 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 14:21:03,461 - [INFO] - Archived 0 unique rows
2025-01-22 14:21:03,492 - [INFO] - Successfully loaded 24 rows
2025-01-22 14:21:03,497 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-22 14:21:03,497 - [INFO] - - Archived rows: 0
2025-01-22 14:21:03,497 - [INFO] - - Loaded rows: 24
2025-01-22 14:21:03,497 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-22 14:21:03,506 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 14:21:03,511 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 14:21:03,514 - [INFO] - Archived 0 unique rows
2025-01-22 14:21:03,536 - [INFO] - Successfully loaded 24 rows
2025-01-22 14:21:03,541 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-22 14:21:03,541 - [INFO] - - Archived rows: 0
2025-01-22 14:21:03,541 - [INFO] - - Loaded rows: 24
2025-01-22 14:21:03,541 - [INFO] - 
Verifying data load...
2025-01-22 14:21:03,545 - [INFO] - Verification results: {'total_rows': 24, 'date_range': {'start': datetime.datetime(2022, 5, 27, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('30'), 'clicks': Decimal('30')}}
2025-01-22 14:21:03,546 - [INFO] - 
Load Summary:
2025-01-22 14:21:03,546 - [INFO] - Total files processed: 2
2025-01-22 14:21:03,546 - [INFO] - Total rows loaded: 48
2025-01-22 14:21:03,546 - [INFO] - 
Date Range:
2025-01-22 14:21:03,546 - [INFO] -   Start: 2022-05-27 00:00:00
2025-01-22 14:21:03,546 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-22 14:21:03,546 - [INFO] - 
Total Counts:
2025-01-22 14:21:03,546 - [INFO] -   Impressions: 30
2025-01-22 14:21:03,546 - [INFO] -   Clicks: 30
2025-01-22 14:21:03,546 - [INFO] - 
Client report loading completed successfully
2025-01-22 14:21:03,546 - [INFO] - Processing and loading completed successfully
2025-01-22 14:21:04,676 - [INFO] - Spark Session stopped
2025-01-22 14:55:09,001 - [INFO] - Starting application...
2025-01-22 14:55:09,069 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 14:55:13,864 - [INFO] - Spark Session created successfully
2025-01-22 14:55:13,864 - [INFO] - Spark version: 3.5.4
2025-01-22 14:55:13,865 - [INFO] - Spark master: local[*]
2025-01-22 14:55:13,865 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 14:55:13,865 - [INFO] - Starting file processing...
2025-01-22 14:55:13,865 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 14:55:13,865 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 14:55:13,865 - [INFO] - User agent filter: some user agent
2025-01-22 14:55:13,866 - [INFO] - Found 11 total parquet files
2025-01-22 14:55:13,866 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 14:55:13,866 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 14:55:21,188 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 14:55:21,822 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 14:55:23,935 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 14:55:24,777 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 14:55:25,215 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 14:55:30,157 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 14:55:30,159 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 14:55:30,159 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 14:55:30,160 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 14:55:30,160 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 14:55:30,464 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 14:55:30,640 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 14:55:31,836 - [ERROR] - Error processing files for date '2022-05-27': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 36) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)
	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)
	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.SocketException: Connection reset by peer: socket write error
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)
	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)
	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)

2025-01-22 14:55:31,836 - [WARNING] - Failed to process data for date: 2022-05-27 - An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 36) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)
	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)
	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.SocketException: Connection reset by peer: socket write error
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)
	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)
	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)

2025-01-22 14:55:31,836 - [INFO] - Successfully processed 1 dates
2025-01-22 14:55:31,836 - [INFO] - Starting client report loading process...
2025-01-22 14:55:31,837 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 14:55:31,838 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 14:55:31,844 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 14:55:31,849 - [INFO] - Archived 0 unique rows
2025-01-22 14:55:31,864 - [INFO] - Successfully loaded 24 rows
2025-01-22 14:55:31,869 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-22 14:55:31,869 - [INFO] - - Archived rows: 0
2025-01-22 14:55:31,869 - [INFO] - - Loaded rows: 24
2025-01-22 14:55:31,870 - [INFO] - 
Verifying data load...
2025-01-22 14:55:31,874 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-22 14:55:31,875 - [INFO] - 
Load Summary:
2025-01-22 14:55:31,876 - [INFO] - Total files processed: 1
2025-01-22 14:55:31,876 - [INFO] - Total rows loaded: 24
2025-01-22 14:55:31,876 - [INFO] - 
Date Range:
2025-01-22 14:55:31,876 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-22 14:55:31,876 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-22 14:55:31,876 - [INFO] - 
Total Counts:
2025-01-22 14:55:31,876 - [INFO] -   Impressions: 44
2025-01-22 14:55:31,876 - [INFO] -   Clicks: 30
2025-01-22 14:55:31,876 - [INFO] - 
Client report loading completed successfully
2025-01-22 14:55:31,876 - [INFO] - Processing and loading completed successfully
2025-01-22 14:55:32,406 - [INFO] - Spark Session stopped
2025-01-22 15:00:34,278 - [INFO] - Starting application...
2025-01-22 15:00:34,340 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 15:01:00,433 - [INFO] - Spark Session created successfully
2025-01-22 15:01:00,434 - [INFO] - Spark version: 3.5.4
2025-01-22 15:01:00,434 - [INFO] - Spark master: local[*]
2025-01-22 15:01:00,434 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 15:01:00,434 - [INFO] - Starting file processing...
2025-01-22 15:01:00,434 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 15:01:00,434 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 15:01:00,434 - [INFO] - User agent filter: some user agent
2025-01-22 15:01:00,435 - [INFO] - Found 8 total parquet files
2025-01-22 15:01:00,435 - [INFO] - Impressions files by date: 2022-05-27
2025-01-22 15:01:00,435 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 15:01:07,647 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 15:01:08,150 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 15:01:10,153 - [ERROR] - Error processing files for date '2022-05-27': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)
	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)
	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.SocketException: Connection reset by peer: socket write error
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)
	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)
	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)

2025-01-22 15:01:10,154 - [WARNING] - Failed to process data for date: 2022-05-27 - An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)
	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)
	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.SocketException: Connection reset by peer: socket write error
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)
	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)
	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)

2025-01-22 15:01:10,154 - [ERROR] - No dates were successfully processed
2025-01-22 15:01:10,154 - [ERROR] - Spark processing failed
2025-01-22 15:01:10,154 - [ERROR] - Processing or loading failed
2025-01-22 15:01:10,378 - [INFO] - Spark Session stopped
2025-01-22 15:02:54,521 - [INFO] - Starting application...
2025-01-22 15:02:54,583 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 15:03:01,311 - [INFO] - Spark Session created successfully
2025-01-22 15:03:01,312 - [INFO] - Spark version: 3.5.4
2025-01-22 15:03:01,312 - [INFO] - Spark master: local[*]
2025-01-22 15:03:01,312 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 15:03:01,312 - [INFO] - Starting file processing...
2025-01-22 15:03:01,313 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 15:03:01,313 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 15:03:01,313 - [INFO] - User agent filter: some user agent
2025-01-22 15:03:01,314 - [INFO] - Found 11 total parquet files
2025-01-22 15:03:01,314 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 15:03:01,314 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 15:03:07,537 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 15:03:08,009 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 15:03:09,949 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 15:03:10,872 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 15:03:11,278 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 15:03:16,129 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 15:03:16,129 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 15:03:16,130 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 15:03:16,130 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 15:03:16,131 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 15:03:16,406 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 15:03:16,567 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 15:03:17,681 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 15:03:18,113 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 15:03:18,346 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 15:03:18,611 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 15:03:18,802 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 15:03:20,157 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 15:03:20,687 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 15:03:20,911 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 15:03:24,943 - [WARNING] - Found records where clicks exceed impressions:
2025-01-22 15:03:25,664 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 15:03:25,664 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 15:03:25,665 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 15:03:25,665 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 15:03:25,666 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 15:03:25,666 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 15:03:25,667 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 15:03:25,667 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 15:03:25,668 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 15:03:25,668 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 15:03:25,668 - [INFO] - Successfully processed 2 dates
2025-01-22 15:03:25,668 - [INFO] - Starting client report loading process...
2025-01-22 15:03:25,668 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 15:03:25,670 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 15:03:25,676 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 15:03:25,681 - [INFO] - Archived 0 unique rows
2025-01-22 15:03:25,719 - [INFO] - Successfully loaded 24 rows
2025-01-22 15:03:25,726 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-22 15:03:25,726 - [INFO] - - Archived rows: 0
2025-01-22 15:03:25,726 - [INFO] - - Loaded rows: 24
2025-01-22 15:03:25,726 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-22 15:03:25,728 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 15:03:25,732 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 15:03:25,736 - [INFO] - Archived 0 unique rows
2025-01-22 15:03:25,761 - [INFO] - Successfully loaded 24 rows
2025-01-22 15:03:25,765 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-22 15:03:25,765 - [INFO] - - Archived rows: 0
2025-01-22 15:03:25,765 - [INFO] - - Loaded rows: 24
2025-01-22 15:03:25,765 - [INFO] - 
Verifying data load...
2025-01-22 15:03:25,770 - [INFO] - Verification results: {'total_rows': 24, 'date_range': {'start': datetime.datetime(2022, 5, 27, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('30'), 'clicks': Decimal('30')}}
2025-01-22 15:03:25,772 - [INFO] - 
Load Summary:
2025-01-22 15:03:25,772 - [INFO] - Total files processed: 2
2025-01-22 15:03:25,772 - [INFO] - Total rows loaded: 48
2025-01-22 15:03:25,772 - [INFO] - 
Date Range:
2025-01-22 15:03:25,772 - [INFO] -   Start: 2022-05-27 00:00:00
2025-01-22 15:03:25,772 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-22 15:03:25,772 - [INFO] - 
Total Counts:
2025-01-22 15:03:25,772 - [INFO] -   Impressions: 30
2025-01-22 15:03:25,772 - [INFO] -   Clicks: 30
2025-01-22 15:03:25,772 - [INFO] - 
Client report loading completed successfully
2025-01-22 15:03:25,772 - [INFO] - Processing and loading completed successfully
2025-01-22 15:03:26,938 - [INFO] - Spark Session stopped
2025-01-22 15:05:04,340 - [INFO] - Starting application...
2025-01-22 15:05:04,401 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 15:05:30,454 - [INFO] - Spark Session created successfully
2025-01-22 15:05:30,455 - [INFO] - Spark version: 3.5.4
2025-01-22 15:05:30,455 - [INFO] - Spark master: local[*]
2025-01-22 15:05:30,455 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 15:05:30,455 - [INFO] - Starting file processing...
2025-01-22 15:05:30,455 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 15:05:30,455 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 15:05:30,455 - [INFO] - User agent filter: some user agent
2025-01-22 15:05:30,455 - [INFO] - Found 11 total parquet files
2025-01-22 15:05:30,455 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 15:05:30,455 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 15:05:35,638 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 15:05:36,079 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 15:05:37,857 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 15:05:38,712 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 15:05:39,134 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 15:05:44,016 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 15:05:44,017 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 15:05:44,017 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 15:05:44,018 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 15:05:44,018 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 15:05:44,287 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 15:05:44,487 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 15:05:45,668 - [ERROR] - Error processing files for date '2022-05-27': An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 36) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)
	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)
	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.SocketException: Connection reset by peer: socket write error
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)
	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)
	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)

2025-01-22 15:05:45,669 - [WARNING] - Failed to process data for date: 2022-05-27 - An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 36) (host.docker.internal executor driver): java.net.SocketException: Connection reset by peer: socket write error
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)
	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)
	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.SocketException: Connection reset by peer: socket write error
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)
	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)
	at org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)

2025-01-22 15:05:45,669 - [INFO] - Successfully processed 1 dates
2025-01-22 15:05:45,669 - [INFO] - Starting client report loading process...
2025-01-22 15:05:45,669 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 15:05:45,671 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 15:05:45,677 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 15:05:45,681 - [INFO] - Archived 0 unique rows
2025-01-22 15:05:45,714 - [ERROR] - Error loading data: (sqlalchemy.exc.InvalidRequestError) A value is required for bind parameter 'min_date'
[SQL: SELECT COUNT(*) FROM adform_dw.client_report  WHERE datetime BETWEEN %(min_date)s AND %(max_date)s;]
(Background on this error at: https://sqlalche.me/e/14/cd3x)
2025-01-22 15:05:45,714 - [ERROR] - Error processing C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv: (sqlalchemy.exc.InvalidRequestError) A value is required for bind parameter 'min_date'
[SQL: SELECT COUNT(*) FROM adform_dw.client_report  WHERE datetime BETWEEN %(min_date)s AND %(max_date)s;]
(Background on this error at: https://sqlalche.me/e/14/cd3x)
2025-01-22 15:05:45,714 - [ERROR] - Processing or loading failed
2025-01-22 15:05:46,300 - [INFO] - Spark Session stopped
2025-01-22 15:11:29,430 - [INFO] - Starting application...
2025-01-22 15:11:29,490 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 15:11:45,148 - [INFO] - Spark Session created successfully
2025-01-22 15:11:45,148 - [INFO] - Spark version: 3.5.4
2025-01-22 15:11:45,148 - [INFO] - Spark master: local[*]
2025-01-22 15:11:45,148 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 15:11:45,148 - [INFO] - Starting file processing...
2025-01-22 15:11:45,149 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 15:11:45,149 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 15:11:45,149 - [INFO] - User agent filter: some user agent
2025-01-22 15:11:45,150 - [INFO] - Found 11 total parquet files
2025-01-22 15:11:45,150 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 15:11:45,150 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 15:11:50,747 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 15:11:51,251 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 15:11:53,188 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 15:11:54,075 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 15:11:54,510 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 15:11:59,669 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 15:11:59,670 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 15:11:59,670 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 15:11:59,671 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 15:11:59,671 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 15:11:59,945 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 15:12:00,121 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 15:12:01,309 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 15:12:01,727 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 15:12:01,946 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 15:12:02,229 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 15:12:02,387 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 15:12:03,851 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 15:12:04,288 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 15:12:04,535 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 15:12:08,646 - [WARNING] - Found records where clicks exceed impressions:
2025-01-22 15:12:09,427 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 15:12:09,428 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 15:12:09,429 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 15:12:09,429 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 15:12:09,430 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 15:12:09,430 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 15:12:09,431 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 15:12:09,431 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 15:12:09,432 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 15:12:09,432 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 15:12:09,432 - [INFO] - Successfully processed 2 dates
2025-01-22 15:12:09,432 - [INFO] - Starting client report loading process...
2025-01-22 15:12:09,433 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 15:12:09,435 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 15:12:09,441 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 15:12:09,445 - [INFO] - Archived 0 unique rows
2025-01-22 15:12:09,458 - [INFO] - Successfully loaded 48 rows
2025-01-22 15:12:09,464 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-22 15:12:09,464 - [INFO] - - Archived rows: 0
2025-01-22 15:12:09,464 - [INFO] - - Loaded rows: 48
2025-01-22 15:12:09,464 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-22 15:12:09,466 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 15:12:09,470 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 15:12:09,473 - [INFO] - Archived 0 unique rows
2025-01-22 15:12:09,483 - [INFO] - Successfully loaded 48 rows
2025-01-22 15:12:09,487 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-22 15:12:09,487 - [INFO] - - Archived rows: 0
2025-01-22 15:12:09,487 - [INFO] - - Loaded rows: 48
2025-01-22 15:12:09,487 - [INFO] - 
Verifying data load...
2025-01-22 15:12:09,493 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-22 15:12:09,494 - [INFO] - 
Load Summary:
2025-01-22 15:12:09,494 - [INFO] - Total files processed: 2
2025-01-22 15:12:09,494 - [INFO] - Total rows loaded: 96
2025-01-22 15:12:09,494 - [INFO] - 
Date Range:
2025-01-22 15:12:09,494 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-22 15:12:09,494 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-22 15:12:09,495 - [INFO] - 
Total Counts:
2025-01-22 15:12:09,495 - [INFO] -   Impressions: 44
2025-01-22 15:12:09,495 - [INFO] -   Clicks: 30
2025-01-22 15:12:09,495 - [INFO] - 
Client report loading completed successfully
2025-01-22 15:12:09,495 - [INFO] - Processing and loading completed successfully
2025-01-22 15:12:10,644 - [INFO] - Spark Session stopped
2025-01-22 15:26:18,525 - [INFO] - Starting application...
2025-01-22 15:26:18,586 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 15:26:45,118 - [INFO] - Spark Session created successfully
2025-01-22 15:26:45,119 - [INFO] - Spark version: 3.5.4
2025-01-22 15:26:45,119 - [INFO] - Spark master: local[*]
2025-01-22 15:26:45,119 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 15:26:45,119 - [INFO] - Starting file processing...
2025-01-22 15:26:45,119 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 15:26:45,119 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 15:26:45,119 - [INFO] - User agent filter: some user agent
2025-01-22 15:26:45,120 - [INFO] - Found 11 total parquet files
2025-01-22 15:26:45,120 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 15:26:45,120 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 15:26:50,345 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 15:26:50,848 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 15:26:52,815 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 15:26:53,683 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 15:26:54,087 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 15:26:59,174 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 15:26:59,175 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 15:26:59,176 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 15:26:59,176 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 15:26:59,176 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 15:26:59,523 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 15:26:59,767 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 15:27:01,024 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 15:27:01,489 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 15:27:01,744 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 15:27:02,072 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 15:27:02,242 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 15:27:03,647 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 15:27:04,101 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 15:27:04,356 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 15:27:08,313 - [WARNING] - Found records where clicks exceed impressions:
2025-01-22 15:27:09,058 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 15:27:09,059 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 15:27:09,060 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 15:27:09,060 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 15:27:09,061 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 15:27:09,061 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 15:27:09,062 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 15:27:09,062 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 15:27:09,063 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 15:27:09,063 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 15:27:09,063 - [INFO] - Successfully processed 2 dates
2025-01-22 15:27:09,063 - [INFO] - Starting client report loading process...
2025-01-22 15:27:09,064 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 15:27:09,066 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 15:27:09,078 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 15:27:09,082 - [INFO] - Archived 0 unique rows
2025-01-22 15:27:09,098 - [INFO] - Successfully loaded 48 rows
2025-01-22 15:27:09,104 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-22 15:27:09,104 - [INFO] - - Archived rows: 0
2025-01-22 15:27:09,104 - [INFO] - - Loaded rows: 48
2025-01-22 15:27:09,104 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-22 15:27:09,107 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 15:27:09,113 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 15:27:09,116 - [INFO] - Archived 0 unique rows
2025-01-22 15:27:09,123 - [INFO] - Successfully loaded 48 rows
2025-01-22 15:27:09,126 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-22 15:27:09,127 - [INFO] - - Archived rows: 0
2025-01-22 15:27:09,127 - [INFO] - - Loaded rows: 48
2025-01-22 15:27:09,127 - [INFO] - 
Verifying data load...
2025-01-22 15:27:09,131 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-22 15:27:09,132 - [INFO] - 
Load Summary:
2025-01-22 15:27:09,132 - [INFO] - Total files processed: 2
2025-01-22 15:27:09,132 - [INFO] - Total rows loaded: 96
2025-01-22 15:27:09,132 - [INFO] - 
Date Range:
2025-01-22 15:27:09,132 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-22 15:27:09,132 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-22 15:27:09,132 - [INFO] - 
Total Counts:
2025-01-22 15:27:09,132 - [INFO] -   Impressions: 44
2025-01-22 15:27:09,132 - [INFO] -   Clicks: 30
2025-01-22 15:27:09,132 - [INFO] - 
Client report loading completed successfully
2025-01-22 15:27:09,132 - [INFO] - Processing and loading completed successfully
2025-01-22 15:27:10,313 - [INFO] - Spark Session stopped
2025-01-22 15:31:29,801 - [INFO] - Starting application...
2025-01-22 15:31:29,866 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 15:31:34,801 - [INFO] - Spark Session created successfully
2025-01-22 15:31:34,801 - [INFO] - Spark version: 3.5.4
2025-01-22 15:31:34,801 - [INFO] - Spark master: local[*]
2025-01-22 15:31:34,801 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 15:31:34,801 - [INFO] - Starting file processing...
2025-01-22 15:31:34,801 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 15:31:34,801 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 15:31:34,802 - [INFO] - User agent filter: some user agent
2025-01-22 15:31:34,802 - [INFO] - Found 11 total parquet files
2025-01-22 15:31:34,803 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 15:31:34,803 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 15:31:41,142 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 15:31:41,604 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 15:31:43,344 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 15:31:44,259 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 15:31:44,713 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 15:31:49,927 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 15:31:49,928 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 15:31:49,928 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 15:31:49,929 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 15:31:49,929 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 15:31:50,211 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 15:31:50,439 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 15:31:51,566 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 15:31:52,320 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 15:31:52,703 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 15:31:53,120 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 15:31:53,290 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 15:31:55,386 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 15:31:56,031 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 15:31:56,355 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 15:32:00,703 - [WARNING] - Found records where clicks exceed impressions:
2025-01-22 15:32:01,472 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 15:32:01,473 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 15:32:01,473 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 15:32:01,474 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 15:32:01,474 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 15:32:01,474 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 15:32:01,475 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 15:32:01,475 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 15:32:01,476 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 15:32:01,476 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 15:32:01,476 - [INFO] - Successfully processed 2 dates
2025-01-22 15:32:01,476 - [INFO] - Starting client report loading process...
2025-01-22 15:32:01,476 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 15:32:01,478 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 15:32:01,487 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 15:32:01,490 - [INFO] - Archived 0 unique rows
2025-01-22 15:32:01,505 - [INFO] - Successfully loaded 48 rows
2025-01-22 15:32:01,512 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-22 15:32:01,512 - [INFO] - - Archived rows: 0
2025-01-22 15:32:01,512 - [INFO] - - Loaded rows: 48
2025-01-22 15:32:01,512 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-22 15:32:01,515 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 15:32:01,521 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 15:32:01,523 - [INFO] - Archived 0 unique rows
2025-01-22 15:32:01,533 - [INFO] - Successfully loaded 48 rows
2025-01-22 15:32:01,538 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-22 15:32:01,539 - [INFO] - - Archived rows: 0
2025-01-22 15:32:01,539 - [INFO] - - Loaded rows: 48
2025-01-22 15:32:01,539 - [INFO] - 
Verifying data load...
2025-01-22 15:32:01,544 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-22 15:32:01,545 - [INFO] - 
Load Summary:
2025-01-22 15:32:01,545 - [INFO] - Total files processed: 2
2025-01-22 15:32:01,545 - [INFO] - Total rows loaded: 96
2025-01-22 15:32:01,545 - [INFO] - 
Date Range:
2025-01-22 15:32:01,545 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-22 15:32:01,545 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-22 15:32:01,545 - [INFO] - 
Total Counts:
2025-01-22 15:32:01,545 - [INFO] -   Impressions: 44
2025-01-22 15:32:01,545 - [INFO] -   Clicks: 30
2025-01-22 15:32:01,545 - [INFO] - 
Client report loading completed successfully
2025-01-22 15:32:01,545 - [INFO] - Processing and loading completed successfully
2025-01-22 15:32:02,716 - [INFO] - Spark Session stopped
2025-01-22 15:38:06,778 - [INFO] - Starting application...
2025-01-22 15:38:06,842 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 15:38:32,943 - [INFO] - Spark Session created successfully
2025-01-22 15:38:32,943 - [INFO] - Spark version: 3.5.4
2025-01-22 15:38:32,943 - [INFO] - Spark master: local[*]
2025-01-22 15:38:32,943 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 15:38:32,943 - [INFO] - Starting file processing...
2025-01-22 15:38:32,944 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 15:38:32,944 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 15:38:32,944 - [INFO] - User agent filter: some user agent
2025-01-22 15:38:32,945 - [INFO] - Found 11 total parquet files
2025-01-22 15:38:32,945 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 15:38:32,945 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 15:38:38,542 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 15:38:39,019 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 15:38:40,963 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 15:38:41,927 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 15:38:42,377 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 15:38:47,369 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 15:38:47,370 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 15:38:47,371 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 15:38:47,371 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 15:38:47,371 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 15:38:47,629 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 15:38:47,795 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 15:38:48,941 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 15:38:49,395 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 15:38:49,589 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 15:38:49,835 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 15:38:50,046 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 15:38:51,479 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 15:38:51,917 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 15:38:52,150 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 15:38:56,162 - [WARNING] - Found records where clicks exceed impressions:
2025-01-22 15:38:56,947 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 15:38:56,948 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 15:38:56,948 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 15:38:56,949 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 15:38:56,949 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 15:38:56,950 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 15:38:56,950 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 15:38:56,951 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 15:38:56,951 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 15:38:56,951 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 15:38:56,951 - [INFO] - Successfully processed 2 dates
2025-01-22 15:38:56,951 - [INFO] - Starting client report loading process...
2025-01-22 15:38:56,952 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 15:38:56,954 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 15:38:56,960 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 15:38:56,965 - [INFO] - Archived 0 unique rows
2025-01-22 15:38:56,979 - [INFO] - Successfully loaded 48 rows
2025-01-22 15:38:56,985 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-22 15:38:56,985 - [INFO] - - Archived rows: 0
2025-01-22 15:38:56,985 - [INFO] - - Loaded rows: 48
2025-01-22 15:38:56,986 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-22 15:38:56,996 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 15:38:57,002 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 15:38:57,005 - [INFO] - Archived 0 unique rows
2025-01-22 15:38:57,012 - [INFO] - Successfully loaded 48 rows
2025-01-22 15:38:57,017 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-22 15:38:57,017 - [INFO] - - Archived rows: 0
2025-01-22 15:38:57,017 - [INFO] - - Loaded rows: 48
2025-01-22 15:38:57,017 - [INFO] - 
Verifying data load...
2025-01-22 15:38:57,021 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-22 15:38:57,022 - [INFO] - 
Load Summary:
2025-01-22 15:38:57,022 - [INFO] - Total files processed: 2
2025-01-22 15:38:57,022 - [INFO] - Total rows loaded: 96
2025-01-22 15:38:57,022 - [INFO] - 
Date Range:
2025-01-22 15:38:57,022 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-22 15:38:57,022 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-22 15:38:57,022 - [INFO] - 
Total Counts:
2025-01-22 15:38:57,023 - [INFO] -   Impressions: 44
2025-01-22 15:38:57,023 - [INFO] -   Clicks: 30
2025-01-22 15:38:57,023 - [INFO] - 
Client report loading completed successfully
2025-01-22 15:38:57,023 - [INFO] - Processing and loading completed successfully
2025-01-22 15:38:58,159 - [INFO] - Spark Session stopped
2025-01-22 16:03:48,119 - [INFO] - Starting application...
2025-01-22 16:03:52,247 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 16:03:52,247 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 253, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 50, in __init__
    self._ensure_schema()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 55, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 16:05:31,834 - [INFO] - Starting application...
2025-01-22 16:05:31,927 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 16:06:07,607 - [INFO] - Spark Session created successfully
2025-01-22 16:06:07,607 - [INFO] - Spark version: 3.5.4
2025-01-22 16:06:07,607 - [INFO] - Spark master: local[*]
2025-01-22 16:06:07,608 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 16:06:07,608 - [INFO] - Starting file processing...
2025-01-22 16:06:07,608 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 16:06:07,608 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 16:06:07,608 - [INFO] - User agent filter: some user agent
2025-01-22 16:06:07,608 - [WARNING] - No parquet files found in input directory
2025-01-22 16:06:07,608 - [ERROR] - Spark processing failed
2025-01-22 16:06:07,608 - [ERROR] - Processing or loading failed
2025-01-22 16:06:07,933 - [INFO] - Spark Session stopped
2025-01-22 16:07:09,015 - [INFO] - Starting application...
2025-01-22 16:07:13,124 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 16:07:13,124 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 254, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 50, in __init__
    self._ensure_schema()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 55, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-22 16:10:47,420 - [INFO] - Starting application...
2025-01-22 16:10:47,480 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 16:10:52,066 - [INFO] - Spark Session created successfully
2025-01-22 16:10:52,066 - [INFO] - Spark version: 3.5.4
2025-01-22 16:10:52,066 - [INFO] - Spark master: local[*]
2025-01-22 16:10:52,067 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 16:10:52,067 - [INFO] - Starting file processing...
2025-01-22 16:10:52,067 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 16:10:52,067 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 16:10:52,067 - [INFO] - User agent filter: some user agent
2025-01-22 16:10:52,068 - [INFO] - Found 11 total parquet files
2025-01-22 16:10:52,068 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 16:10:52,068 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 16:10:57,499 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 16:10:57,955 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 16:10:59,732 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 16:11:01,145 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 16:11:01,714 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 16:11:07,066 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 16:11:07,067 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 16:11:07,068 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 16:11:07,068 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 16:11:07,068 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 16:11:07,357 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 16:11:07,512 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 16:11:08,617 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 16:11:09,103 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 16:11:09,326 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 16:11:09,621 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 16:11:09,840 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 16:11:11,137 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 16:11:11,670 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 16:11:11,920 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 16:11:15,803 - [WARNING] - Found records where clicks exceed impressions:
2025-01-22 16:11:16,607 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 16:11:16,607 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 16:11:16,607 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 16:11:16,608 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 16:11:16,608 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 16:11:16,609 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 16:11:16,609 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 16:11:16,609 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 16:11:16,610 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 16:11:16,610 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 16:11:16,610 - [INFO] - Successfully processed 2 dates
2025-01-22 16:11:16,610 - [INFO] - Starting client report loading process...
2025-01-22 16:11:16,610 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 16:11:16,620 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 16:11:16,627 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 16:11:16,630 - [INFO] - Archived 0 unique rows
2025-01-22 16:11:16,646 - [INFO] - Successfully loaded 48 rows
2025-01-22 16:11:16,651 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-22 16:11:16,651 - [INFO] - - Archived rows: 0
2025-01-22 16:11:16,651 - [INFO] - - Loaded rows: 48
2025-01-22 16:11:16,651 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-22 16:11:16,662 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 16:11:16,669 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 16:11:16,673 - [INFO] - Archived 0 unique rows
2025-01-22 16:11:16,683 - [INFO] - Successfully loaded 48 rows
2025-01-22 16:11:16,688 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-22 16:11:16,688 - [INFO] - - Archived rows: 0
2025-01-22 16:11:16,688 - [INFO] - - Loaded rows: 48
2025-01-22 16:11:16,688 - [INFO] - 
Verifying data load...
2025-01-22 16:11:16,693 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-22 16:11:16,695 - [INFO] - 
Load Summary:
2025-01-22 16:11:16,695 - [INFO] - Total files processed: 2
2025-01-22 16:11:16,695 - [INFO] - Total rows loaded: 96
2025-01-22 16:11:16,695 - [INFO] - 
Date Range:
2025-01-22 16:11:16,695 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-22 16:11:16,695 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-22 16:11:16,695 - [INFO] - 
Total Counts:
2025-01-22 16:11:16,695 - [INFO] -   Impressions: 44
2025-01-22 16:11:16,695 - [INFO] -   Clicks: 30
2025-01-22 16:11:16,695 - [INFO] - 
Client report loading completed successfully
2025-01-22 16:11:16,695 - [INFO] - Processing and loading completed successfully
2025-01-22 16:11:17,805 - [INFO] - Spark Session stopped
2025-01-22 21:01:17,200 - [INFO] - Starting application...
2025-01-22 21:01:17,264 - [ERROR] - Critical error during execution: 'ClientReportETL' object has no attribute 'logger'
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 254, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 46, in __init__
    self._ensure_schema()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 92, in _ensure_schema
    self.logger.info(
    ^^^^^^^^^^^
AttributeError: 'ClientReportETL' object has no attribute 'logger'
2025-01-22 21:06:43,194 - [INFO] - Starting application...
2025-01-22 21:06:43,285 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 21:07:19,410 - [INFO] - Spark Session created successfully
2025-01-22 21:07:19,411 - [INFO] - Spark version: 3.5.4
2025-01-22 21:07:19,411 - [INFO] - Spark master: local[*]
2025-01-22 21:07:19,411 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 21:07:19,411 - [INFO] - Starting file processing...
2025-01-22 21:07:19,411 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 21:07:19,411 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 21:07:19,411 - [INFO] - User agent filter: some user agent
2025-01-22 21:07:19,412 - [ERROR] - Error extracting date from filename clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet: unconverted data remains: 80
2025-01-22 21:07:19,412 - [ERROR] - Error processing filename clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet: unconverted data remains: 80
2025-01-22 21:07:19,412 - [ERROR] - Error extracting date from filename clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet: unconverted data remains: 23
2025-01-22 21:07:19,412 - [ERROR] - Error processing filename clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet: unconverted data remains: 23
2025-01-22 21:07:19,412 - [ERROR] - Error extracting date from filename clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet: unconverted data remains: 24
2025-01-22 21:07:19,412 - [ERROR] - Error processing filename clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet: unconverted data remains: 24
2025-01-22 21:07:19,412 - [ERROR] - Error extracting date from filename clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet: unconverted data remains: 81
2025-01-22 21:07:19,412 - [ERROR] - Error processing filename clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet: unconverted data remains: 81
2025-01-22 21:07:19,412 - [ERROR] - Error extracting date from filename clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet: unconverted data remains: 81
2025-01-22 21:07:19,412 - [ERROR] - Error processing filename clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet: unconverted data remains: 81
2025-01-22 21:07:19,413 - [ERROR] - Error extracting date from filename clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet: unconverted data remains: 24
2025-01-22 21:07:19,413 - [ERROR] - Error processing filename clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet: unconverted data remains: 24
2025-01-22 21:07:19,413 - [ERROR] - Error extracting date from filename impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet: unconverted data remains: 63
2025-01-22 21:07:19,413 - [ERROR] - Error processing filename impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet: unconverted data remains: 63
2025-01-22 21:07:19,413 - [ERROR] - Error extracting date from filename impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet: unconverted data remains: 39
2025-01-22 21:07:19,413 - [ERROR] - Error processing filename impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet: unconverted data remains: 39
2025-01-22 21:07:19,413 - [ERROR] - Error extracting date from filename impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet: unconverted data remains: 70
2025-01-22 21:07:19,413 - [ERROR] - Error processing filename impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet: unconverted data remains: 70
2025-01-22 21:07:19,413 - [ERROR] - Error extracting date from filename impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet: unconverted data remains: 55
2025-01-22 21:07:19,413 - [ERROR] - Error processing filename impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet: unconverted data remains: 55
2025-01-22 21:07:19,413 - [ERROR] - Error extracting date from filename impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet: unconverted data remains: 88
2025-01-22 21:07:19,413 - [ERROR] - Error processing filename impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet: unconverted data remains: 88
2025-01-22 21:07:19,413 - [INFO] - Found 11 total parquet files
2025-01-22 21:07:19,413 - [INFO] - Impressions files by date: 
2025-01-22 21:07:19,413 - [INFO] - Clicks files by date: 
2025-01-22 21:07:19,413 - [ERROR] - No dates were successfully processed
2025-01-22 21:07:19,413 - [ERROR] - Spark processing failed
2025-01-22 21:07:19,413 - [ERROR] - Processing or loading failed
2025-01-22 21:07:19,761 - [INFO] - Spark Session stopped
2025-01-22 21:10:04,812 - [INFO] - Starting application...
2025-01-22 21:10:04,871 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 21:10:09,802 - [INFO] - Spark Session created successfully
2025-01-22 21:10:09,802 - [INFO] - Spark version: 3.5.4
2025-01-22 21:10:09,802 - [INFO] - Spark master: local[*]
2025-01-22 21:10:09,802 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 21:10:09,802 - [INFO] - Starting file processing...
2025-01-22 21:10:09,802 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 21:10:09,802 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 21:10:09,802 - [INFO] - User agent filter: some user agent
2025-01-22 21:10:09,803 - [ERROR] - Error extracting date from filename clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet: unconverted data remains: 80
2025-01-22 21:10:09,803 - [ERROR] - Error processing filename clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet: unconverted data remains: 80
2025-01-22 21:10:09,803 - [ERROR] - Error extracting date from filename clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet: unconverted data remains: 23
2025-01-22 21:10:09,803 - [ERROR] - Error processing filename clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet: unconverted data remains: 23
2025-01-22 21:10:09,803 - [ERROR] - Error extracting date from filename clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet: unconverted data remains: 24
2025-01-22 21:10:09,803 - [ERROR] - Error processing filename clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet: unconverted data remains: 24
2025-01-22 21:10:09,803 - [ERROR] - Error extracting date from filename clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet: unconverted data remains: 81
2025-01-22 21:10:09,803 - [ERROR] - Error processing filename clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet: unconverted data remains: 81
2025-01-22 21:10:09,804 - [ERROR] - Error extracting date from filename clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet: unconverted data remains: 81
2025-01-22 21:10:09,804 - [ERROR] - Error processing filename clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet: unconverted data remains: 81
2025-01-22 21:10:09,804 - [ERROR] - Error extracting date from filename clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet: unconverted data remains: 24
2025-01-22 21:10:09,804 - [ERROR] - Error processing filename clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet: unconverted data remains: 24
2025-01-22 21:10:09,804 - [ERROR] - Error extracting date from filename impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet: unconverted data remains: 63
2025-01-22 21:10:09,804 - [ERROR] - Error processing filename impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet: unconverted data remains: 63
2025-01-22 21:10:09,804 - [ERROR] - Error extracting date from filename impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet: unconverted data remains: 39
2025-01-22 21:10:09,804 - [ERROR] - Error processing filename impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet: unconverted data remains: 39
2025-01-22 21:10:09,804 - [ERROR] - Error extracting date from filename impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet: unconverted data remains: 70
2025-01-22 21:10:09,804 - [ERROR] - Error processing filename impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet: unconverted data remains: 70
2025-01-22 21:10:09,804 - [ERROR] - Error extracting date from filename impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet: unconverted data remains: 55
2025-01-22 21:10:09,804 - [ERROR] - Error processing filename impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet: unconverted data remains: 55
2025-01-22 21:10:09,804 - [ERROR] - Error extracting date from filename impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet: unconverted data remains: 88
2025-01-22 21:10:09,804 - [ERROR] - Error processing filename impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet: unconverted data remains: 88
2025-01-22 21:10:09,804 - [INFO] - Found 11 total parquet files
2025-01-22 21:10:09,804 - [INFO] - Impressions files by date: 
2025-01-22 21:10:09,804 - [INFO] - Clicks files by date: 
2025-01-22 21:10:09,804 - [ERROR] - No dates were successfully processed
2025-01-22 21:10:09,804 - [ERROR] - Spark processing failed
2025-01-22 21:10:09,804 - [ERROR] - Processing or loading failed
2025-01-22 21:10:10,154 - [INFO] - Spark Session stopped
2025-01-22 21:11:37,207 - [INFO] - Starting application...
2025-01-22 21:11:37,279 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 21:12:03,831 - [INFO] - Spark Session created successfully
2025-01-22 21:12:03,832 - [INFO] - Spark version: 3.5.4
2025-01-22 21:12:03,832 - [INFO] - Spark master: local[*]
2025-01-22 21:12:03,832 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 21:12:03,832 - [INFO] - Starting file processing...
2025-01-22 21:12:03,832 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 21:12:03,832 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 21:12:03,832 - [INFO] - User agent filter: some user agent
2025-01-22 21:12:03,833 - [INFO] - Found 11 total parquet files
2025-01-22 21:12:03,833 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 21:12:03,833 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 21:12:09,107 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 21:12:09,578 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 21:12:10,182 - [ERROR] - Error processing files for date '2022-05-26': 'list' object has no attribute 'isEmpty'
2025-01-22 21:12:10,182 - [WARNING] - Failed to process data for date: 2022-05-26 - 'list' object has no attribute 'isEmpty'
2025-01-22 21:12:10,490 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 21:12:10,684 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 21:12:10,945 - [ERROR] - Error processing files for date '2022-05-27': 'list' object has no attribute 'isEmpty'
2025-01-22 21:12:10,945 - [WARNING] - Failed to process data for date: 2022-05-27 - 'list' object has no attribute 'isEmpty'
2025-01-22 21:12:10,945 - [ERROR] - No dates were successfully processed
2025-01-22 21:12:10,945 - [ERROR] - Spark processing failed
2025-01-22 21:12:10,945 - [ERROR] - Processing or loading failed
2025-01-22 21:12:11,286 - [INFO] - Spark Session stopped
2025-01-22 21:16:47,691 - [INFO] - Starting application...
2025-01-22 21:16:47,773 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 21:17:14,928 - [INFO] - Spark Session created successfully
2025-01-22 21:17:14,929 - [INFO] - Spark version: 3.5.4
2025-01-22 21:17:14,929 - [INFO] - Spark master: local[*]
2025-01-22 21:17:14,929 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 21:17:14,929 - [INFO] - Starting file processing...
2025-01-22 21:17:14,929 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 21:17:14,929 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 21:17:14,929 - [INFO] - User agent filter: some user agent
2025-01-22 21:17:14,930 - [INFO] - Found 11 total parquet files
2025-01-22 21:17:14,930 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 21:17:14,930 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 21:17:21,754 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 21:17:22,359 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 21:17:24,105 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 21:17:24,572 - [INFO] - Found 0 records with invalid hours
2025-01-22 21:17:25,007 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 21:17:25,381 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-22 21:17:25,597 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 21:17:30,331 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 21:17:30,332 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 21:17:30,332 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 21:17:30,332 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 21:17:30,332 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 21:17:30,606 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 21:17:30,738 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 21:17:31,829 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 21:17:32,083 - [INFO] - Found 0 records with invalid hours
2025-01-22 21:17:32,276 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 21:17:32,464 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-22 21:17:32,654 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 21:17:33,088 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 21:17:33,235 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 21:17:34,599 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 21:17:34,813 - [INFO] - Found 0 records with invalid hours
2025-01-22 21:17:34,991 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 21:17:35,303 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-22 21:17:35,474 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 21:17:39,394 - [WARNING] - Found records where clicks exceed impressions:
2025-01-22 21:17:40,103 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 21:17:40,104 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 21:17:40,104 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 21:17:40,105 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 21:17:40,105 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 21:17:40,106 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 21:17:40,107 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 21:17:40,107 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 21:17:40,108 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 21:17:40,108 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 21:17:40,108 - [INFO] - Successfully processed 2 dates
2025-01-22 21:17:40,108 - [INFO] - Starting client report loading process...
2025-01-22 21:17:40,109 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 21:17:40,110 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 21:17:40,118 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 21:17:40,123 - [INFO] - Archived 0 unique rows
2025-01-22 21:17:40,136 - [INFO] - Successfully loaded 48 rows
2025-01-22 21:17:40,142 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-22 21:17:40,142 - [INFO] - - Archived rows: 0
2025-01-22 21:17:40,142 - [INFO] - - Loaded rows: 48
2025-01-22 21:17:40,142 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-22 21:17:40,150 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 21:17:40,155 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 21:17:40,158 - [INFO] - Archived 0 unique rows
2025-01-22 21:17:40,166 - [INFO] - Successfully loaded 48 rows
2025-01-22 21:17:40,170 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-22 21:17:40,170 - [INFO] - - Archived rows: 0
2025-01-22 21:17:40,170 - [INFO] - - Loaded rows: 48
2025-01-22 21:17:40,170 - [INFO] - 
Verifying data load...
2025-01-22 21:17:40,175 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-22 21:17:40,176 - [INFO] - 
Load Summary:
2025-01-22 21:17:40,176 - [INFO] - Total files processed: 2
2025-01-22 21:17:40,176 - [INFO] - Total rows loaded: 96
2025-01-22 21:17:40,176 - [INFO] - 
Date Range:
2025-01-22 21:17:40,176 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-22 21:17:40,176 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-22 21:17:40,176 - [INFO] - 
Total Counts:
2025-01-22 21:17:40,176 - [INFO] -   Impressions: 44
2025-01-22 21:17:40,176 - [INFO] -   Clicks: 30
2025-01-22 21:17:40,176 - [INFO] - 
Client report loading completed successfully
2025-01-22 21:17:40,176 - [INFO] - Processing and loading completed successfully
2025-01-22 21:17:40,395 - [INFO] - Spark Session stopped
2025-01-22 23:08:48,096 - AdformETLLogger - INFO - Starting application...
2025-01-22 23:08:48,163 - AdformETLLogger - INFO - Database schema and tables verified/created successfully
2025-01-22 23:10:05,816 - AdformETLLogger - ERROR - Critical error during execution: [WinError 3] The system cannot find the path specified: 'C:\\Users\\Rokas\\AppData\\Local\\Temp\\tmphrtnorhg'
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\java_gateway.py", line 104, in launch_gateway
    time.sleep(0.1)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 259, in main
    spark = create_spark_session()
            ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 115, in create_spark_session
    .getOrCreate()
     ^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\sql\session.py", line 497, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 515, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 201, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\context.py", line 436, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
                                       ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\pyspark\java_gateway.py", line 116, in launch_gateway
    shutil.rmtree(conn_info_dir)
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\shutil.py", line 787, in rmtree
    return _rmtree_unsafe(path, onerror)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\shutil.py", line 615, in _rmtree_unsafe
    onerror(os.scandir, path, sys.exc_info())
  File "C:\Users\Rokas\AppData\Local\Programs\Python\Python311\Lib\shutil.py", line 612, in _rmtree_unsafe
    with os.scandir(path) as scandir_it:
         ^^^^^^^^^^^^^^^^
FileNotFoundError: [WinError 3] The system cannot find the path specified: 'C:\\Users\\Rokas\\AppData\\Local\\Temp\\tmphrtnorhg'
2025-01-22 23:10:11,824 - [INFO] - Starting application...
2025-01-22 23:10:11,903 - [INFO] - Database schema and tables verified/created successfully
2025-01-22 23:10:38,829 - [INFO] - Spark Session created successfully
2025-01-22 23:10:38,830 - [INFO] - Spark version: 3.5.4
2025-01-22 23:10:38,830 - [INFO] - Spark master: local[*]
2025-01-22 23:10:38,830 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-22 23:10:38,830 - [INFO] - Starting file processing...
2025-01-22 23:10:38,830 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-22 23:10:38,830 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-22 23:10:38,830 - [INFO] - User agent filter: some user agent
2025-01-22 23:10:38,831 - [INFO] - Found 11 total parquet files
2025-01-22 23:10:38,831 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-22 23:10:38,831 - [INFO] - Clicks files by date: 2022-05-27
2025-01-22 23:10:44,565 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 23:10:44,991 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-22 23:10:46,715 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 23:10:47,190 - [INFO] - Found 0 records with invalid hours
2025-01-22 23:10:47,623 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 23:10:47,999 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-22 23:10:48,242 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-22 23:10:53,153 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-22 23:10:53,153 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-22 23:10:53,154 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-22 23:10:53,154 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-22 23:10:53,155 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-22 23:10:53,456 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-22 23:10:53,591 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-22 23:10:54,770 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 23:10:55,010 - [INFO] - Found 0 records with invalid hours
2025-01-22 23:10:55,204 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-22 23:10:55,405 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-22 23:10:55,592 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-22 23:10:55,860 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-22 23:10:56,002 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-22 23:10:57,317 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 23:10:57,649 - [INFO] - Found 0 records with invalid hours
2025-01-22 23:10:57,825 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 23:10:58,028 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-22 23:10:58,186 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-22 23:11:02,016 - [WARNING] - Found records where clicks exceed impressions:
2025-01-22 23:11:02,788 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-22 23:11:02,790 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-22 23:11:02,792 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-22 23:11:02,794 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-22 23:11:02,796 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-22 23:11:02,798 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-22 23:11:02,799 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-22 23:11:02,800 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-22 23:11:02,801 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-22 23:11:02,802 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-22 23:11:02,802 - [INFO] - Successfully processed 2 dates
2025-01-22 23:11:02,802 - [INFO] - Starting client report loading process...
2025-01-22 23:11:02,804 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-22 23:11:02,807 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 23:11:02,823 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 23:11:02,830 - [INFO] - Archived 0 unique rows
2025-01-22 23:11:02,855 - [INFO] - Successfully loaded 48 rows
2025-01-22 23:11:02,862 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-22 23:11:02,862 - [INFO] - - Archived rows: 0
2025-01-22 23:11:02,862 - [INFO] - - Loaded rows: 48
2025-01-22 23:11:02,862 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-22 23:11:02,865 - [INFO] - Read CSV with shape: (24, 4)
2025-01-22 23:11:02,873 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-22 23:11:02,877 - [INFO] - Archived 0 unique rows
2025-01-22 23:11:02,889 - [INFO] - Successfully loaded 48 rows
2025-01-22 23:11:02,894 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-22 23:11:02,894 - [INFO] - - Archived rows: 0
2025-01-22 23:11:02,894 - [INFO] - - Loaded rows: 48
2025-01-22 23:11:02,894 - [INFO] - 
Verifying data load...
2025-01-22 23:11:02,901 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-22 23:11:02,903 - [INFO] - 
Load Summary:
2025-01-22 23:11:02,903 - [INFO] - Total files processed: 2
2025-01-22 23:11:02,903 - [INFO] - Total rows loaded: 96
2025-01-22 23:11:02,903 - [INFO] - 
Date Range:
2025-01-22 23:11:02,903 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-22 23:11:02,903 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-22 23:11:02,903 - [INFO] - 
Total Counts:
2025-01-22 23:11:02,903 - [INFO] -   Impressions: 44
2025-01-22 23:11:02,903 - [INFO] -   Clicks: 30
2025-01-22 23:11:02,903 - [INFO] - 
Client report loading completed successfully
2025-01-22 23:11:02,903 - [INFO] - Processing and loading completed successfully
2025-01-22 23:11:04,045 - [INFO] - Spark Session stopped
2025-01-23 12:04:43,836 - [INFO] - Starting application...
2025-01-23 12:04:43,886 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-23 12:04:43,886 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 254, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 48, in __init__
    self._ensure_schema()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 53, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-23 12:40:08,992 - [INFO] - Starting application...
2025-01-23 12:40:09,037 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-23 12:40:09,038 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 254, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 48, in __init__
    self._ensure_schema()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 53, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-23 12:41:00,001 - [INFO] - Starting application...
2025-01-23 12:41:00,047 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-23 12:41:00,047 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 254, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 48, in __init__
    self._ensure_schema()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 53, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-23 12:42:51,061 - [INFO] - Starting application...
2025-01-23 12:42:51,107 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-23 12:42:51,107 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 254, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 48, in __init__
    self._ensure_schema()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 53, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-23 12:46:04,309 - [INFO] - Starting application...
2025-01-23 12:46:04,358 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-23 12:46:04,358 - [ERROR] - Critical error during execution: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 254, in main
    etl = ClientReportETL(warehouse_connection, logger)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 48, in __init__
    self._ensure_schema()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\src\etl\warehouse.py", line 53, in _ensure_schema
    with self.engine.begin() as conn:
         ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3143, in begin
    conn = self.connect(close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3361, in _wrap_pool_connect
    return fn()
           ^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 388, in __init__
    self.__connect()
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 690, in __connect
    with util.safe_reraise():
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\util\compat.py", line 211, in raise_
    raise exception
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\pool\base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\.venv\Lib\site-packages\psycopg2\__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_db"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-23 12:48:16,346 - [INFO] - Starting application...
2025-01-23 12:48:16,430 - [INFO] - Database schema and tables verified/created successfully
2025-01-23 12:48:42,558 - [INFO] - Spark Session created successfully
2025-01-23 12:48:42,559 - [INFO] - Spark version: 3.5.4
2025-01-23 12:48:42,559 - [INFO] - Spark master: local[*]
2025-01-23 12:48:42,559 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-23 12:48:42,559 - [INFO] - Starting file processing...
2025-01-23 12:48:42,559 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-23 12:48:42,559 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-23 12:48:42,559 - [INFO] - User agent filter: some user agent
2025-01-23 12:48:42,560 - [INFO] - Found 11 total parquet files
2025-01-23 12:48:42,560 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-23 12:48:42,560 - [INFO] - Clicks files by date: 2022-05-27
2025-01-23 12:48:47,718 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 12:48:48,156 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-23 12:48:49,845 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 12:48:50,284 - [INFO] - Found 0 records with invalid hours
2025-01-23 12:48:50,688 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 12:48:51,062 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-23 12:48:51,313 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 12:48:56,214 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-23 12:48:56,215 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-23 12:48:56,216 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 12:48:56,216 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-23 12:48:56,217 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-23 12:48:56,490 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 12:48:56,647 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-23 12:48:57,837 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 12:48:58,079 - [INFO] - Found 0 records with invalid hours
2025-01-23 12:48:58,290 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-23 12:48:58,538 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-23 12:48:58,721 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-23 12:48:58,962 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-23 12:48:59,226 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-23 12:49:00,560 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 12:49:00,850 - [INFO] - Found 0 records with invalid hours
2025-01-23 12:49:01,041 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 12:49:01,267 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-23 12:49:01,431 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 12:49:05,277 - [WARNING] - Found records where clicks exceed impressions:
2025-01-23 12:49:06,065 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-23 12:49:06,066 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-23 12:49:06,066 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 12:49:06,067 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-23 12:49:06,067 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-23 12:49:06,068 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 12:49:06,068 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-23 12:49:06,068 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-23 12:49:06,069 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-23 12:49:06,069 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-23 12:49:06,069 - [INFO] - Successfully processed 2 dates
2025-01-23 12:49:06,069 - [INFO] - Starting client report loading process...
2025-01-23 12:49:06,070 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-23 12:49:06,071 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 12:49:06,078 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 12:49:06,083 - [INFO] - Archived 0 unique rows
2025-01-23 12:49:06,099 - [INFO] - Successfully loaded 24 rows
2025-01-23 12:49:06,105 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-23 12:49:06,105 - [INFO] - - Archived rows: 0
2025-01-23 12:49:06,105 - [INFO] - - Loaded rows: 24
2025-01-23 12:49:06,106 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-23 12:49:06,115 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 12:49:06,120 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 12:49:06,123 - [INFO] - Archived 0 unique rows
2025-01-23 12:49:06,131 - [INFO] - Successfully loaded 48 rows
2025-01-23 12:49:06,135 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-23 12:49:06,135 - [INFO] - - Archived rows: 0
2025-01-23 12:49:06,135 - [INFO] - - Loaded rows: 48
2025-01-23 12:49:06,135 - [INFO] - 
Verifying data load...
2025-01-23 12:49:06,140 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-23 12:49:06,141 - [INFO] - 
Load Summary:
2025-01-23 12:49:06,141 - [INFO] - Total files processed: 2
2025-01-23 12:49:06,141 - [INFO] - Total rows loaded: 72
2025-01-23 12:49:06,141 - [INFO] - 
Date Range:
2025-01-23 12:49:06,141 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-23 12:49:06,141 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-23 12:49:06,141 - [INFO] - 
Total Counts:
2025-01-23 12:49:06,141 - [INFO] -   Impressions: 44
2025-01-23 12:49:06,141 - [INFO] -   Clicks: 30
2025-01-23 12:49:06,141 - [INFO] - 
Client report loading completed successfully
2025-01-23 12:49:06,141 - [INFO] - Processing and loading completed successfully
2025-01-23 12:49:07,316 - [INFO] - Spark Session stopped
2025-01-23 13:55:53,448 - [INFO] - Starting application...
2025-01-23 13:55:53,532 - [INFO] - Database schema and tables verified/created successfully
2025-01-23 13:56:19,331 - [INFO] - Spark Session created successfully
2025-01-23 13:56:19,331 - [INFO] - Spark version: 3.5.4
2025-01-23 13:56:19,332 - [INFO] - Spark master: local[*]
2025-01-23 13:56:19,332 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-23 13:56:19,332 - [INFO] - Starting file processing...
2025-01-23 13:56:19,332 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-23 13:56:19,332 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-23 13:56:19,332 - [INFO] - User agent filter: some user agent
2025-01-23 13:56:19,333 - [INFO] - Found 11 total parquet files
2025-01-23 13:56:19,333 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-23 13:56:19,333 - [INFO] - Clicks files by date: 2022-05-27
2025-01-23 13:56:24,884 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 13:56:25,327 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-23 13:56:27,069 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 13:56:27,525 - [INFO] - Found 0 records with invalid hours
2025-01-23 13:56:27,942 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 13:56:28,304 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-23 13:56:28,536 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 13:56:33,841 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-23 13:56:33,841 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-23 13:56:33,841 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 13:56:33,842 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-23 13:56:33,842 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-23 13:56:34,111 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 13:56:34,277 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-23 13:56:35,378 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 13:56:35,629 - [INFO] - Found 0 records with invalid hours
2025-01-23 13:56:35,820 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-23 13:56:36,050 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-23 13:56:36,236 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-23 13:56:36,501 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-23 13:56:36,661 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-23 13:56:38,138 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 13:56:38,426 - [INFO] - Found 0 records with invalid hours
2025-01-23 13:56:38,621 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 13:56:38,827 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-23 13:56:39,026 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 13:56:42,941 - [WARNING] - Found records where clicks exceed impressions:
2025-01-23 13:56:43,710 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-23 13:56:43,711 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-23 13:56:43,711 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 13:56:43,712 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-23 13:56:43,712 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-23 13:56:43,713 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 13:56:43,713 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-23 13:56:43,713 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-23 13:56:43,714 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-23 13:56:43,714 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-23 13:56:43,714 - [INFO] - Successfully processed 2 dates
2025-01-23 13:56:43,714 - [INFO] - Starting client report loading process...
2025-01-23 13:56:43,715 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-23 13:56:43,717 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 13:56:43,731 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 13:56:43,736 - [INFO] - Archived 24 unique rows
2025-01-23 13:56:43,760 - [INFO] - Successfully loaded 48 rows
2025-01-23 13:56:43,767 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-23 13:56:43,767 - [INFO] - - Archived rows: 24
2025-01-23 13:56:43,767 - [INFO] - - Loaded rows: 48
2025-01-23 13:56:43,767 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-23 13:56:43,776 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 13:56:43,782 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 13:56:43,785 - [INFO] - Archived 24 unique rows
2025-01-23 13:56:43,793 - [INFO] - Successfully loaded 48 rows
2025-01-23 13:56:43,798 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-23 13:56:43,798 - [INFO] - - Archived rows: 24
2025-01-23 13:56:43,798 - [INFO] - - Loaded rows: 48
2025-01-23 13:56:43,798 - [INFO] - 
Verifying data load...
2025-01-23 13:56:43,803 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-23 13:56:43,805 - [INFO] - 
Load Summary:
2025-01-23 13:56:43,805 - [INFO] - Total files processed: 2
2025-01-23 13:56:43,805 - [INFO] - Total rows loaded: 96
2025-01-23 13:56:43,805 - [INFO] - 
Date Range:
2025-01-23 13:56:43,805 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-23 13:56:43,805 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-23 13:56:43,805 - [INFO] - 
Total Counts:
2025-01-23 13:56:43,805 - [INFO] -   Impressions: 44
2025-01-23 13:56:43,805 - [INFO] -   Clicks: 30
2025-01-23 13:56:43,805 - [INFO] - 
Client report loading completed successfully
2025-01-23 13:56:43,805 - [INFO] - Processing and loading completed successfully
2025-01-23 13:56:44,935 - [INFO] - Spark Session stopped
2025-01-23 14:00:05,104 - [INFO] - Starting application...
2025-01-23 14:00:05,166 - [INFO] - Database schema and tables verified/created successfully
2025-01-23 14:00:09,935 - [INFO] - Spark Session created successfully
2025-01-23 14:00:09,936 - [INFO] - Spark version: 3.5.4
2025-01-23 14:00:09,936 - [INFO] - Spark master: local[*]
2025-01-23 14:00:09,936 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-23 14:00:09,936 - [INFO] - Starting file processing...
2025-01-23 14:00:09,936 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-23 14:00:09,936 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-23 14:00:09,936 - [INFO] - User agent filter: some user agent
2025-01-23 14:00:09,936 - [WARNING] - No parquet files found in input directory
2025-01-23 14:00:09,936 - [ERROR] - Spark processing failed
2025-01-23 14:00:09,936 - [ERROR] - Processing or loading failed
2025-01-23 14:00:10,270 - [INFO] - Spark Session stopped
2025-01-23 14:49:31,845 - [INFO] - Starting application...
2025-01-23 14:49:31,908 - [INFO] - Database schema and tables verified/created successfully
2025-01-23 14:49:57,790 - [INFO] - Spark Session created successfully
2025-01-23 14:49:57,790 - [INFO] - Spark version: 3.5.4
2025-01-23 14:49:57,790 - [INFO] - Spark master: local[*]
2025-01-23 14:49:57,790 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-23 14:49:57,790 - [INFO] - Starting file processing...
2025-01-23 14:49:57,790 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-23 14:49:57,790 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-23 14:49:57,790 - [INFO] - User agent filter: some user agent
2025-01-23 14:49:57,791 - [INFO] - Found 11 total parquet files
2025-01-23 14:49:57,791 - [INFO] - Impressions files by date: 20220526, 20220527
2025-01-23 14:49:57,791 - [INFO] - Clicks files by date: 20220527
2025-01-23 14:50:03,086 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 14:50:03,555 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-23 14:50:05,301 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 14:50:05,751 - [INFO] - Found 0 records with invalid hours
2025-01-23 14:50:06,148 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 14:50:06,567 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-23 14:50:06,803 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 14:50:11,655 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_20220526.csv
2025-01-23 14:50:11,655 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-23 14:50:11,656 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 14:50:11,656 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-23 14:50:11,656 - [INFO] - Successfully processed data for date: 20220526
2025-01-23 14:50:11,948 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 14:50:12,086 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-23 14:50:13,209 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 14:50:13,451 - [INFO] - Found 0 records with invalid hours
2025-01-23 14:50:13,635 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-23 14:50:13,839 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-23 14:50:14,028 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-23 14:50:14,279 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-23 14:50:14,424 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-23 14:50:15,805 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 14:50:16,061 - [INFO] - Found 0 records with invalid hours
2025-01-23 14:50:16,243 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 14:50:16,446 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-23 14:50:16,603 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 14:50:20,560 - [WARNING] - Found records where clicks exceed impressions:
2025-01-23 14:50:21,320 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_20220527.csv
2025-01-23 14:50:21,320 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-23 14:50:21,321 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 14:50:21,321 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-23 14:50:21,321 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-23 14:50:21,322 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 14:50:21,322 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-23 14:50:21,323 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-23 14:50:21,323 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-23 14:50:21,323 - [INFO] - Successfully processed data for date: 20220527
2025-01-23 14:50:21,323 - [INFO] - Successfully processed 2 dates
2025-01-23 14:50:21,323 - [INFO] - Starting client report loading process...
2025-01-23 14:50:21,324 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220526.csv
2025-01-23 14:50:21,325 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 14:50:21,334 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 14:50:21,338 - [INFO] - Archived 0 unique rows
2025-01-23 14:50:21,355 - [INFO] - Successfully loaded 72 rows
2025-01-23 14:50:21,359 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220526.csv:
2025-01-23 14:50:21,360 - [INFO] - - Archived rows: 0
2025-01-23 14:50:21,360 - [INFO] - - Loaded rows: 72
2025-01-23 14:50:21,360 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220527.csv
2025-01-23 14:50:21,371 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 14:50:21,379 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 14:50:21,382 - [INFO] - Archived 24 unique rows
2025-01-23 14:50:21,392 - [INFO] - Successfully loaded 72 rows
2025-01-23 14:50:21,396 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220527.csv:
2025-01-23 14:50:21,396 - [INFO] - - Archived rows: 24
2025-01-23 14:50:21,396 - [INFO] - - Loaded rows: 72
2025-01-23 14:50:21,396 - [INFO] - 
Verifying data load...
2025-01-23 14:50:21,400 - [INFO] - Verification results: {'total_rows': 72, 'date_range': {'start': datetime.datetime(1970, 1, 1, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('74'), 'clicks': Decimal('60')}}
2025-01-23 14:50:21,401 - [INFO] - 
Load Summary:
2025-01-23 14:50:21,402 - [INFO] - Total files processed: 2
2025-01-23 14:50:21,402 - [INFO] - Total rows loaded: 144
2025-01-23 14:50:21,402 - [INFO] - 
Date Range:
2025-01-23 14:50:21,402 - [INFO] -   Start: 1970-01-01 00:00:00
2025-01-23 14:50:21,402 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-23 14:50:21,402 - [INFO] - 
Total Counts:
2025-01-23 14:50:21,402 - [INFO] -   Impressions: 74
2025-01-23 14:50:21,402 - [INFO] -   Clicks: 60
2025-01-23 14:50:21,402 - [INFO] - 
Client report loading completed successfully
2025-01-23 14:50:21,402 - [INFO] - Processing and loading completed successfully
2025-01-23 14:50:22,561 - [INFO] - Spark Session stopped
2025-01-23 16:25:38,796 - [INFO] - Starting application...
2025-01-23 16:25:38,867 - [INFO] - Database schema and tables verified/created successfully
2025-01-23 16:25:45,256 - [INFO] - Spark Session created successfully
2025-01-23 16:25:45,257 - [INFO] - Spark version: 3.5.4
2025-01-23 16:25:45,257 - [INFO] - Spark master: local[*]
2025-01-23 16:25:45,257 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-23 16:25:45,257 - [INFO] - Starting file processing...
2025-01-23 16:25:45,257 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-23 16:25:45,257 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-23 16:25:45,257 - [INFO] - User agent filter: some user agent
2025-01-23 16:25:45,258 - [INFO] - Found 11 total parquet files
2025-01-23 16:25:45,258 - [INFO] - Impressions files by date: 20220526, 20220527
2025-01-23 16:25:45,258 - [INFO] - Clicks files by date: 20220527
2025-01-23 16:25:50,540 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 16:25:51,039 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-23 16:25:52,939 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 16:25:53,395 - [INFO] - Found 0 records with invalid hours
2025-01-23 16:25:53,798 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 16:25:54,165 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-23 16:25:54,402 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 16:25:59,489 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_20220526.csv
2025-01-23 16:25:59,490 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-23 16:25:59,490 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 16:25:59,491 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-23 16:25:59,491 - [INFO] - Successfully processed data for date: 20220526
2025-01-23 16:25:59,785 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 16:25:59,935 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-23 16:26:01,099 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 16:26:01,357 - [INFO] - Found 0 records with invalid hours
2025-01-23 16:26:01,542 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-23 16:26:01,730 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-23 16:26:01,952 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-23 16:26:02,186 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-23 16:26:02,354 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-23 16:26:03,849 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 16:26:04,107 - [INFO] - Found 0 records with invalid hours
2025-01-23 16:26:04,297 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 16:26:04,535 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-23 16:26:04,707 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 16:26:08,859 - [WARNING] - Found records where clicks exceed impressions:
2025-01-23 16:26:09,648 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_20220527.csv
2025-01-23 16:26:09,649 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-23 16:26:09,649 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 16:26:09,650 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-23 16:26:09,650 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-23 16:26:09,651 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 16:26:09,651 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-23 16:26:09,652 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-23 16:26:09,652 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-23 16:26:09,653 - [INFO] - Successfully processed data for date: 20220527
2025-01-23 16:26:09,653 - [INFO] - Successfully processed 2 dates
2025-01-23 16:26:09,653 - [INFO] - Starting client report loading process...
2025-01-23 16:26:09,653 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220526.csv
2025-01-23 16:26:09,655 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 16:26:09,662 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 16:26:09,666 - [INFO] - Archived 0 unique rows
2025-01-23 16:26:09,680 - [INFO] - Successfully loaded 72 rows
2025-01-23 16:26:09,685 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220526.csv:
2025-01-23 16:26:09,685 - [INFO] - - Archived rows: 0
2025-01-23 16:26:09,685 - [INFO] - - Loaded rows: 72
2025-01-23 16:26:09,685 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220527.csv
2025-01-23 16:26:09,688 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 16:26:09,692 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 16:26:09,695 - [INFO] - Archived 0 unique rows
2025-01-23 16:26:09,702 - [INFO] - Successfully loaded 72 rows
2025-01-23 16:26:09,706 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220527.csv:
2025-01-23 16:26:09,706 - [INFO] - - Archived rows: 0
2025-01-23 16:26:09,706 - [INFO] - - Loaded rows: 72
2025-01-23 16:26:09,706 - [INFO] - 
Verifying data load...
2025-01-23 16:26:09,710 - [INFO] - Verification results: {'total_rows': 72, 'date_range': {'start': datetime.datetime(1970, 1, 1, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('74'), 'clicks': Decimal('60')}}
2025-01-23 16:26:09,711 - [INFO] - 
Load Summary:
2025-01-23 16:26:09,711 - [INFO] - Total files processed: 2
2025-01-23 16:26:09,711 - [INFO] - Total rows loaded: 144
2025-01-23 16:26:09,711 - [INFO] - 
Date Range:
2025-01-23 16:26:09,711 - [INFO] -   Start: 1970-01-01 00:00:00
2025-01-23 16:26:09,711 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-23 16:26:09,711 - [INFO] - 
Total Counts:
2025-01-23 16:26:09,711 - [INFO] -   Impressions: 74
2025-01-23 16:26:09,711 - [INFO] -   Clicks: 60
2025-01-23 16:26:09,711 - [INFO] - 
Client report loading completed successfully
2025-01-23 16:26:09,711 - [INFO] - Processing and loading completed successfully
2025-01-23 16:26:10,857 - [INFO] - Spark Session stopped
2025-01-23 16:41:17,734 - [INFO] - Starting application...
2025-01-23 16:41:17,808 - [INFO] - Database schema and tables verified/created successfully
2025-01-23 16:41:34,090 - [INFO] - Spark Session created successfully
2025-01-23 16:41:34,091 - [INFO] - Spark version: 3.5.4
2025-01-23 16:41:34,091 - [INFO] - Spark master: local[*]
2025-01-23 16:41:34,091 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-23 16:41:34,091 - [INFO] - Starting file processing...
2025-01-23 16:41:34,091 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-23 16:41:34,091 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-23 16:41:34,091 - [INFO] - User agent filter: some user agent
2025-01-23 16:41:34,092 - [INFO] - Found 11 total parquet files
2025-01-23 16:41:34,092 - [INFO] - Impressions files by date: 20220526, 20220527
2025-01-23 16:41:34,092 - [INFO] - Clicks files by date: 20220527
2025-01-23 16:41:39,164 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 16:41:39,595 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-23 16:41:41,322 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 16:41:41,816 - [INFO] - Found 0 records with invalid hours
2025-01-23 16:41:42,252 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 16:41:42,665 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-23 16:41:42,948 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 16:41:47,675 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_20220526.csv
2025-01-23 16:41:47,675 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-23 16:41:47,676 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 16:41:47,676 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-23 16:41:47,676 - [INFO] - Successfully processed data for date: 20220526
2025-01-23 16:41:47,935 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 16:41:48,083 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-23 16:41:49,235 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 16:41:49,464 - [INFO] - Found 0 records with invalid hours
2025-01-23 16:41:49,627 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-23 16:41:49,812 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-23 16:41:49,962 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-23 16:41:50,292 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-23 16:41:50,440 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-23 16:41:51,717 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 16:41:51,957 - [INFO] - Found 0 records with invalid hours
2025-01-23 16:41:52,154 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 16:41:52,356 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-23 16:41:52,523 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 16:41:56,371 - [WARNING] - Found records where clicks exceed impressions:
2025-01-23 16:41:57,079 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_20220527.csv
2025-01-23 16:41:57,080 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-23 16:41:57,080 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 16:41:57,080 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-23 16:41:57,081 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-23 16:41:57,081 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 16:41:57,082 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-23 16:41:57,082 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-23 16:41:57,083 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-23 16:41:57,083 - [INFO] - Successfully processed data for date: 20220527
2025-01-23 16:41:57,083 - [INFO] - Successfully processed 2 dates
2025-01-23 16:41:57,083 - [INFO] - Starting client report loading process...
2025-01-23 16:41:57,083 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220526.csv
2025-01-23 16:41:57,085 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 16:41:57,091 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 16:41:57,094 - [INFO] - Archived 0 unique rows
2025-01-23 16:41:57,106 - [INFO] - Successfully loaded 72 rows
2025-01-23 16:41:57,111 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220526.csv:
2025-01-23 16:41:57,111 - [INFO] - - Archived rows: 0
2025-01-23 16:41:57,111 - [INFO] - - Loaded rows: 72
2025-01-23 16:41:57,111 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220527.csv
2025-01-23 16:41:57,113 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 16:41:57,117 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 16:41:57,120 - [INFO] - Archived 0 unique rows
2025-01-23 16:41:57,126 - [INFO] - Successfully loaded 72 rows
2025-01-23 16:41:57,130 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220527.csv:
2025-01-23 16:41:57,130 - [INFO] - - Archived rows: 0
2025-01-23 16:41:57,130 - [INFO] - - Loaded rows: 72
2025-01-23 16:41:57,130 - [INFO] - 
Verifying data load...
2025-01-23 16:41:57,133 - [INFO] - Verification results: {'total_rows': 72, 'date_range': {'start': datetime.datetime(1970, 1, 1, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}}
2025-01-23 16:41:57,134 - [INFO] - 
Load Summary:
2025-01-23 16:41:57,134 - [INFO] - Total files processed: 2
2025-01-23 16:41:57,134 - [INFO] - Total rows loaded: 144
2025-01-23 16:41:57,134 - [INFO] - 
Date Range:
2025-01-23 16:41:57,134 - [INFO] -   Start: 1970-01-01 00:00:00
2025-01-23 16:41:57,134 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-23 16:41:57,135 - [INFO] - 
Total Counts:
2025-01-23 16:41:57,135 - [ERROR] - Error in process_and_load_data: 'totals'
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 206, in process_and_load_data
    f"  Impressions: {verification['totals']['impressions']:,}")
                      ~~~~~~~~~~~~^^^^^^^^^^
KeyError: 'totals'
2025-01-23 16:41:57,135 - [ERROR] - Processing or loading failed
2025-01-23 16:41:57,357 - [INFO] - Spark Session stopped
2025-01-23 16:48:42,700 - [INFO] - Starting application...
2025-01-23 16:48:42,793 - [INFO] - Database schema and tables verified/created successfully
2025-01-23 16:48:49,062 - [INFO] - Spark Session created successfully
2025-01-23 16:48:49,062 - [INFO] - Spark version: 3.5.4
2025-01-23 16:48:49,062 - [INFO] - Spark master: local[*]
2025-01-23 16:48:49,062 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-23 16:48:49,062 - [INFO] - Starting file processing...
2025-01-23 16:48:49,062 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-23 16:48:49,062 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-23 16:48:49,062 - [INFO] - User agent filter: some user agent
2025-01-23 16:48:49,063 - [INFO] - Found 11 total parquet files
2025-01-23 16:48:49,064 - [INFO] - Impressions files by date: 20220526, 20220527
2025-01-23 16:48:49,064 - [INFO] - Clicks files by date: 20220527
2025-01-23 16:48:56,466 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 16:48:57,284 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-23 16:48:59,762 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 16:49:00,395 - [INFO] - Found 0 records with invalid hours
2025-01-23 16:49:00,902 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 16:49:01,406 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-23 16:49:01,696 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 16:49:07,368 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_20220526.csv
2025-01-23 16:49:07,369 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-23 16:49:07,369 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 16:49:07,370 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-23 16:49:07,370 - [INFO] - Successfully processed data for date: 20220526
2025-01-23 16:49:07,703 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 16:49:07,873 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-23 16:49:09,282 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 16:49:09,563 - [INFO] - Found 0 records with invalid hours
2025-01-23 16:49:09,759 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-23 16:49:09,954 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-23 16:49:10,171 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-23 16:49:10,530 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-23 16:49:10,720 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-23 16:49:12,246 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 16:49:12,516 - [INFO] - Found 0 records with invalid hours
2025-01-23 16:49:12,717 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 16:49:12,948 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-23 16:49:13,140 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 16:49:17,481 - [WARNING] - Found records where clicks exceed impressions:
2025-01-23 16:49:18,391 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_20220527.csv
2025-01-23 16:49:18,391 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-23 16:49:18,392 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 16:49:18,392 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-23 16:49:18,393 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-23 16:49:18,394 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 16:49:18,394 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-23 16:49:18,394 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-23 16:49:18,395 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-23 16:49:18,396 - [INFO] - Successfully processed data for date: 20220527
2025-01-23 16:49:18,396 - [INFO] - Successfully processed 2 dates
2025-01-23 16:49:18,396 - [INFO] - Starting client report loading process...
2025-01-23 16:49:18,397 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220526.csv
2025-01-23 16:49:18,399 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 16:49:18,408 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 16:49:18,413 - [INFO] - Archived 0 unique rows
2025-01-23 16:49:18,430 - [INFO] - Successfully loaded 72 rows
2025-01-23 16:49:18,436 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220526.csv:
2025-01-23 16:49:18,436 - [INFO] - - Archived rows: 0
2025-01-23 16:49:18,436 - [INFO] - - Loaded rows: 72
2025-01-23 16:49:18,436 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220527.csv
2025-01-23 16:49:18,445 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 16:49:18,450 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 16:49:18,454 - [INFO] - Archived 0 unique rows
2025-01-23 16:49:18,463 - [INFO] - Successfully loaded 72 rows
2025-01-23 16:49:18,468 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220527.csv:
2025-01-23 16:49:18,468 - [INFO] - - Archived rows: 0
2025-01-23 16:49:18,468 - [INFO] - - Loaded rows: 72
2025-01-23 16:49:18,468 - [INFO] - 
Verifying data load...
2025-01-23 16:49:18,474 - [INFO] - Verification results: {'total_rows': 72, 'date_range': {'start': datetime.datetime(1970, 1, 1, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}}
2025-01-23 16:49:18,475 - [INFO] - 
Load Summary:
2025-01-23 16:49:18,475 - [INFO] - Total files processed: 2
2025-01-23 16:49:18,475 - [INFO] - Total rows loaded: 144
2025-01-23 16:49:18,475 - [INFO] - 
Date Range:
2025-01-23 16:49:18,475 - [INFO] -   Start: 1970-01-01 00:00:00
2025-01-23 16:49:18,475 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-23 16:49:18,475 - [ERROR] - Error in process_and_load_data: 'totals'
Traceback (most recent call last):
  File "C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\main.py", line 207, in process_and_load_data
    f"  Impressions: {verification['totals']['impressions']:,}")
                      ~~~~~~~~~~~~^^^^^^^^^^
KeyError: 'totals'
2025-01-23 16:49:18,476 - [ERROR] - Processing or loading failed
2025-01-23 16:49:19,481 - [INFO] - Spark Session stopped
2025-01-23 19:09:14,545 - [INFO] - Starting application...
2025-01-23 19:09:14,608 - [INFO] - Database schema and tables verified/created successfully
2025-01-23 19:09:40,570 - [INFO] - Spark Session created successfully
2025-01-23 19:09:40,570 - [INFO] - Spark version: 3.5.4
2025-01-23 19:09:40,570 - [INFO] - Spark master: local[*]
2025-01-23 19:09:40,570 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-23 19:09:40,570 - [INFO] - Starting file processing...
2025-01-23 19:09:40,570 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-23 19:09:40,571 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-23 19:09:40,571 - [INFO] - User agent filter: some user agent
2025-01-23 19:09:40,571 - [INFO] - Found 11 total parquet files
2025-01-23 19:09:40,572 - [INFO] - Impressions files by date: 20220526, 20220527
2025-01-23 19:09:40,572 - [INFO] - Clicks files by date: 20220527
2025-01-23 19:09:45,596 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 19:09:46,010 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-23 19:09:47,685 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 19:09:48,125 - [INFO] - Found 0 records with invalid hours
2025-01-23 19:09:48,517 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 19:09:48,888 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-23 19:09:49,127 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 19:09:53,778 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_20220526.csv
2025-01-23 19:09:53,778 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-23 19:09:53,779 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 19:09:53,779 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-23 19:09:53,779 - [INFO] - Successfully processed data for date: 20220526
2025-01-23 19:09:54,023 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 19:09:54,171 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-23 19:09:55,278 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 19:09:55,533 - [INFO] - Found 0 records with invalid hours
2025-01-23 19:09:55,710 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-23 19:09:55,898 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-23 19:09:56,071 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-23 19:09:56,389 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-23 19:09:56,612 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-23 19:09:57,948 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 19:09:58,298 - [INFO] - Found 0 records with invalid hours
2025-01-23 19:09:58,491 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 19:09:58,680 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-23 19:09:58,955 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 19:10:02,720 - [WARNING] - Found records where clicks exceed impressions:
2025-01-23 19:10:03,455 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_20220527.csv
2025-01-23 19:10:03,455 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-23 19:10:03,455 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 19:10:03,456 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-23 19:10:03,456 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-23 19:10:03,457 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 19:10:03,457 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-23 19:10:03,457 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-23 19:10:03,458 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-23 19:10:03,458 - [INFO] - Successfully processed data for date: 20220527
2025-01-23 19:10:03,458 - [INFO] - Successfully processed 2 dates
2025-01-23 19:10:03,458 - [INFO] - Starting client report loading process...
2025-01-23 19:10:03,459 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220526.csv
2025-01-23 19:10:03,460 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 19:10:03,465 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 19:10:03,469 - [INFO] - Archived 0 unique rows
2025-01-23 19:10:03,483 - [INFO] - Successfully loaded 24 rows
2025-01-23 19:10:03,486 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220526.csv:
2025-01-23 19:10:03,486 - [INFO] - - Archived rows: 0
2025-01-23 19:10:03,486 - [INFO] - - Loaded rows: 24
2025-01-23 19:10:03,486 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220527.csv
2025-01-23 19:10:03,495 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 19:10:03,500 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 19:10:03,502 - [INFO] - Archived 24 unique rows
2025-01-23 19:10:03,510 - [INFO] - Successfully loaded 24 rows
2025-01-23 19:10:03,512 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_20220527.csv:
2025-01-23 19:10:03,512 - [INFO] - - Archived rows: 24
2025-01-23 19:10:03,513 - [INFO] - - Loaded rows: 24
2025-01-23 19:10:03,513 - [INFO] - 
Verifying data load...
2025-01-23 19:10:03,516 - [INFO] - Verification results: {'total_rows': 24, 'date_range': {'start': datetime.datetime(1970, 1, 1, 0, 0), 'end': datetime.datetime(1970, 1, 1, 23, 0)}, 'totals': {'impressions': Decimal('30'), 'clicks': Decimal('30')}}
2025-01-23 19:10:03,517 - [INFO] - 
Load Summary:
2025-01-23 19:10:03,517 - [INFO] - Total files processed: 2
2025-01-23 19:10:03,517 - [INFO] - Total rows loaded: 48
2025-01-23 19:10:03,517 - [INFO] - 
Date Range:
2025-01-23 19:10:03,517 - [INFO] -   Start: 1970-01-01 00:00:00
2025-01-23 19:10:03,517 - [INFO] -   End: 1970-01-01 23:00:00
2025-01-23 19:10:03,517 - [INFO] - 
Total Counts:
2025-01-23 19:10:03,518 - [INFO] -   Impressions: 30
2025-01-23 19:10:03,518 - [INFO] -   Clicks: 30
2025-01-23 19:10:03,518 - [INFO] - 
Client report loading completed successfully
2025-01-23 19:10:03,518 - [INFO] - Processing and loading completed successfully
2025-01-23 19:10:04,716 - [INFO] - Spark Session stopped
2025-01-23 19:16:38,101 - [INFO] - Starting application...
2025-01-23 19:16:38,160 - [INFO] - Database schema and tables verified/created successfully
2025-01-23 19:16:43,059 - [INFO] - Spark Session created successfully
2025-01-23 19:16:43,060 - [INFO] - Spark version: 3.5.4
2025-01-23 19:16:43,060 - [INFO] - Spark master: local[*]
2025-01-23 19:16:43,060 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-23 19:16:43,060 - [INFO] - Starting file processing...
2025-01-23 19:16:43,060 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-23 19:16:43,060 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-23 19:16:43,060 - [INFO] - User agent filter: some user agent
2025-01-23 19:16:43,061 - [INFO] - Found 11 total parquet files
2025-01-23 19:16:43,061 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-23 19:16:43,061 - [INFO] - Clicks files by date: 2022-05-27
2025-01-23 19:16:48,649 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 19:16:49,120 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-23 19:16:51,566 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 19:16:52,117 - [INFO] - Found 0 records with invalid hours
2025-01-23 19:16:52,483 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 19:16:52,851 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-23 19:16:53,183 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 19:16:57,875 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-23 19:16:57,876 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-23 19:16:57,876 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 19:16:57,877 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-23 19:16:57,877 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-23 19:16:58,229 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 19:16:58,396 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-23 19:16:59,540 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 19:16:59,766 - [INFO] - Found 0 records with invalid hours
2025-01-23 19:16:59,947 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-23 19:17:00,177 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-23 19:17:00,379 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-23 19:17:00,618 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-23 19:17:00,825 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-23 19:17:02,082 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 19:17:02,322 - [INFO] - Found 0 records with invalid hours
2025-01-23 19:17:02,509 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 19:17:02,730 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-23 19:17:03,058 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 19:17:06,884 - [WARNING] - Found records where clicks exceed impressions:
2025-01-23 19:17:07,622 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-23 19:17:07,623 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-23 19:17:07,623 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 19:17:07,624 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-23 19:17:07,624 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-23 19:17:07,625 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 19:17:07,625 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-23 19:17:07,626 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-23 19:17:07,626 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-23 19:17:07,626 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-23 19:17:07,627 - [INFO] - Successfully processed 2 dates
2025-01-23 19:17:07,627 - [INFO] - Starting client report loading process...
2025-01-23 19:17:07,627 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-23 19:17:07,629 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 19:17:07,636 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 19:17:07,641 - [INFO] - Archived 0 unique rows
2025-01-23 19:17:07,657 - [INFO] - Successfully loaded 24 rows
2025-01-23 19:17:07,660 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-23 19:17:07,660 - [INFO] - - Archived rows: 0
2025-01-23 19:17:07,660 - [INFO] - - Loaded rows: 24
2025-01-23 19:17:07,660 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-23 19:17:07,670 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 19:17:07,675 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 19:17:07,678 - [INFO] - Archived 0 unique rows
2025-01-23 19:17:07,686 - [INFO] - Successfully loaded 48 rows
2025-01-23 19:17:07,688 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-23 19:17:07,688 - [INFO] - - Archived rows: 0
2025-01-23 19:17:07,689 - [INFO] - - Loaded rows: 48
2025-01-23 19:17:07,689 - [INFO] - 
Verifying data load...
2025-01-23 19:17:07,693 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-23 19:17:07,693 - [INFO] - 
Load Summary:
2025-01-23 19:17:07,694 - [INFO] - Total files processed: 2
2025-01-23 19:17:07,694 - [INFO] - Total rows loaded: 72
2025-01-23 19:17:07,694 - [INFO] - 
Date Range:
2025-01-23 19:17:07,694 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-23 19:17:07,694 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-23 19:17:07,694 - [INFO] - 
Total Counts:
2025-01-23 19:17:07,694 - [INFO] -   Impressions: 44
2025-01-23 19:17:07,694 - [INFO] -   Clicks: 30
2025-01-23 19:17:07,695 - [INFO] - 
Client report loading completed successfully
2025-01-23 19:17:07,695 - [INFO] - Processing and loading completed successfully
2025-01-23 19:17:07,871 - [INFO] - Spark Session stopped
2025-01-23 19:23:10,462 - [INFO] - Starting application...
2025-01-23 19:23:10,528 - [INFO] - Database schema and tables verified/created successfully
2025-01-23 19:23:28,741 - [INFO] - Spark Session created successfully
2025-01-23 19:23:28,741 - [INFO] - Spark version: 3.5.4
2025-01-23 19:23:28,741 - [INFO] - Spark master: local[*]
2025-01-23 19:23:28,742 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-23 19:23:28,742 - [INFO] - Starting file processing...
2025-01-23 19:23:28,742 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-23 19:23:28,742 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-23 19:23:28,742 - [INFO] - User agent filter: some user agent
2025-01-23 19:23:28,743 - [INFO] - Found 11 total parquet files
2025-01-23 19:23:28,743 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-23 19:23:28,743 - [INFO] - Clicks files by date: 2022-05-27
2025-01-23 19:23:34,209 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 19:23:34,650 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-23 19:23:36,339 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 19:23:36,840 - [INFO] - Found 0 records with invalid hours
2025-01-23 19:23:37,288 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 19:23:37,648 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-23 19:23:37,859 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 19:23:42,748 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-23 19:23:42,748 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-23 19:23:42,749 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 19:23:42,749 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-23 19:23:42,749 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-23 19:23:43,039 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 19:23:43,183 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-23 19:23:44,321 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 19:23:44,580 - [INFO] - Found 0 records with invalid hours
2025-01-23 19:23:44,759 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-23 19:23:44,951 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-23 19:23:45,104 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-23 19:23:45,446 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-23 19:23:45,617 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-23 19:23:46,962 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 19:23:47,190 - [INFO] - Found 0 records with invalid hours
2025-01-23 19:23:47,380 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 19:23:47,572 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-23 19:23:47,777 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 19:23:51,646 - [WARNING] - Found records where clicks exceed impressions:
2025-01-23 19:23:52,382 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-23 19:23:52,383 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-23 19:23:52,385 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 19:23:52,386 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-23 19:23:52,388 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-23 19:23:52,389 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 19:23:52,391 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-23 19:23:52,393 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-23 19:23:52,395 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-23 19:23:52,395 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-23 19:23:52,395 - [INFO] - Successfully processed 2 dates
2025-01-23 19:23:52,395 - [INFO] - Starting client report loading process...
2025-01-23 19:23:52,398 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-23 19:23:52,404 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 19:23:52,428 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 19:23:52,436 - [INFO] - Archived 24 unique rows
2025-01-23 19:23:52,466 - [INFO] - Successfully loaded 48 rows
2025-01-23 19:23:52,472 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-23 19:23:52,472 - [INFO] - - Archived rows: 24
2025-01-23 19:23:52,472 - [INFO] - - Loaded rows: 48
2025-01-23 19:23:52,473 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-23 19:23:52,475 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 19:23:52,485 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 19:23:52,493 - [INFO] - Archived 24 unique rows
2025-01-23 19:23:52,511 - [INFO] - Successfully loaded 48 rows
2025-01-23 19:23:52,515 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-23 19:23:52,516 - [INFO] - - Archived rows: 24
2025-01-23 19:23:52,516 - [INFO] - - Loaded rows: 48
2025-01-23 19:23:52,516 - [INFO] - 
Verifying data load...
2025-01-23 19:23:52,527 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-23 19:23:52,530 - [INFO] - 
Load Summary:
2025-01-23 19:23:52,530 - [INFO] - Total files processed: 2
2025-01-23 19:23:52,530 - [INFO] - Total rows loaded: 96
2025-01-23 19:23:52,530 - [INFO] - 
Date Range:
2025-01-23 19:23:52,530 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-23 19:23:52,530 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-23 19:23:52,530 - [INFO] - 
Total Counts:
2025-01-23 19:23:52,530 - [INFO] -   Impressions: 44
2025-01-23 19:23:52,531 - [INFO] -   Clicks: 30
2025-01-23 19:23:52,531 - [INFO] - 
Client report loading completed successfully
2025-01-23 19:23:52,531 - [INFO] - Processing and loading completed successfully
2025-01-23 19:23:53,642 - [INFO] - Spark Session stopped
2025-01-23 20:06:21,449 - [INFO] - Starting application...
2025-01-23 20:06:21,519 - [INFO] - Database schema and tables verified/created successfully
2025-01-23 20:06:37,954 - [INFO] - Spark Session created successfully
2025-01-23 20:06:37,954 - [INFO] - Spark version: 3.5.4
2025-01-23 20:06:37,954 - [INFO] - Spark master: local[*]
2025-01-23 20:06:37,954 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-23 20:06:37,955 - [INFO] - Starting file processing...
2025-01-23 20:06:37,955 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-23 20:06:37,955 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-23 20:06:37,955 - [INFO] - User agent filter: some user agent
2025-01-23 20:06:37,956 - [INFO] - Found 11 total parquet files
2025-01-23 20:06:37,956 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-23 20:06:37,956 - [INFO] - Clicks files by date: 2022-05-27
2025-01-23 20:06:43,128 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 20:06:43,545 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-23 20:06:45,264 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 20:06:45,730 - [INFO] - Found 0 records with invalid hours
2025-01-23 20:06:46,120 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 20:06:46,490 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-23 20:06:46,714 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 20:06:51,423 - [ERROR] - Error processing files for date '2022-05-26': 'DataProcessor' object has no attribute '_handle_click_impression_mismatch'
2025-01-23 20:06:51,424 - [WARNING] - Failed to process data for date: 2022-05-26 - 'DataProcessor' object has no attribute '_handle_click_impression_mismatch'
2025-01-23 20:06:51,701 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 20:06:51,847 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-23 20:06:52,968 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 20:06:53,237 - [INFO] - Found 0 records with invalid hours
2025-01-23 20:06:53,485 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-23 20:06:53,710 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-23 20:06:53,932 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-23 20:06:54,282 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-23 20:06:54,451 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-23 20:06:55,782 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 20:06:56,023 - [INFO] - Found 0 records with invalid hours
2025-01-23 20:06:56,243 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 20:06:56,440 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-23 20:06:56,676 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 20:07:00,468 - [WARNING] - Found records where clicks exceed impressions:
2025-01-23 20:07:01,142 - [ERROR] - Error processing files for date '2022-05-27': 'DataProcessor' object has no attribute '_handle_click_impression_mismatch'
2025-01-23 20:07:01,142 - [WARNING] - Failed to process data for date: 2022-05-27 - 'DataProcessor' object has no attribute '_handle_click_impression_mismatch'
2025-01-23 20:07:01,142 - [ERROR] - No dates were successfully processed
2025-01-23 20:07:01,142 - [ERROR] - Spark processing failed
2025-01-23 20:07:01,142 - [ERROR] - Processing or loading failed
2025-01-23 20:07:01,452 - [INFO] - Spark Session stopped
2025-01-23 20:13:44,226 - [INFO] - Starting application...
2025-01-23 20:13:44,288 - [INFO] - Database schema and tables verified/created successfully
2025-01-23 20:14:10,221 - [INFO] - Spark Session created successfully
2025-01-23 20:14:10,221 - [INFO] - Spark version: 3.5.4
2025-01-23 20:14:10,221 - [INFO] - Spark master: local[*]
2025-01-23 20:14:10,221 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-23 20:14:10,221 - [INFO] - Starting file processing...
2025-01-23 20:14:10,221 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-23 20:14:10,221 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-23 20:14:10,221 - [INFO] - User agent filter: some user agent
2025-01-23 20:14:10,222 - [INFO] - Found 11 total parquet files
2025-01-23 20:14:10,223 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-23 20:14:10,223 - [INFO] - Clicks files by date: 2022-05-27
2025-01-23 20:14:15,236 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 20:14:15,667 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-23 20:14:17,319 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 20:14:17,755 - [INFO] - Found 0 records with invalid hours
2025-01-23 20:14:18,132 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 20:14:18,479 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-23 20:14:18,707 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 20:14:23,464 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-23 20:14:23,465 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-23 20:14:23,466 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 20:14:23,466 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-23 20:14:23,466 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-23 20:14:23,764 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 20:14:23,898 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-23 20:14:24,957 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 20:14:25,187 - [INFO] - Found 0 records with invalid hours
2025-01-23 20:14:25,378 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-23 20:14:25,580 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-23 20:14:25,752 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-23 20:14:26,036 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-23 20:14:26,187 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-23 20:14:27,445 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 20:14:27,686 - [INFO] - Found 0 records with invalid hours
2025-01-23 20:14:27,933 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 20:14:28,285 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-23 20:14:28,461 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 20:14:32,324 - [WARNING] - Found records where clicks exceed impressions: 2022-05-27
2025-01-23 20:14:32,987 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-23 20:14:32,988 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-23 20:14:32,988 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 20:14:32,989 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-23 20:14:32,989 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-23 20:14:32,990 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 20:14:32,990 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-23 20:14:32,991 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-23 20:14:32,991 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-23 20:14:32,991 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-23 20:14:32,991 - [INFO] - Successfully processed 2 dates
2025-01-23 20:14:32,992 - [INFO] - Starting client report loading process...
2025-01-23 20:14:32,992 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-23 20:14:32,994 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 20:14:33,001 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 20:14:33,005 - [INFO] - Archived 0 unique rows
2025-01-23 20:14:33,021 - [INFO] - Successfully loaded 48 rows
2025-01-23 20:14:33,030 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-23 20:14:33,030 - [INFO] - - Archived rows: 0
2025-01-23 20:14:33,030 - [INFO] - - Loaded rows: 48
2025-01-23 20:14:33,030 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-23 20:14:33,032 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 20:14:33,037 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 20:14:33,041 - [INFO] - Archived 0 unique rows
2025-01-23 20:14:33,050 - [INFO] - Successfully loaded 48 rows
2025-01-23 20:14:33,055 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-23 20:14:33,055 - [INFO] - - Archived rows: 0
2025-01-23 20:14:33,055 - [INFO] - - Loaded rows: 48
2025-01-23 20:14:33,055 - [INFO] - 
Verifying data load...
2025-01-23 20:14:33,060 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-23 20:14:33,061 - [INFO] - 
Load Summary:
2025-01-23 20:14:33,061 - [INFO] - Total files processed: 2
2025-01-23 20:14:33,061 - [INFO] - Total rows loaded: 96
2025-01-23 20:14:33,061 - [INFO] - 
Date Range:
2025-01-23 20:14:33,061 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-23 20:14:33,061 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-23 20:14:33,062 - [INFO] - 
Total Counts:
2025-01-23 20:14:33,062 - [INFO] -   Impressions: 44
2025-01-23 20:14:33,062 - [INFO] -   Clicks: 30
2025-01-23 20:14:33,062 - [INFO] - 
Client report loading completed successfully
2025-01-23 20:14:33,062 - [INFO] - Processing and loading completed successfully
2025-01-23 20:14:33,322 - [INFO] - Spark Session stopped
2025-01-23 20:38:06,989 - [INFO] - Starting application...
2025-01-23 20:38:07,055 - [INFO] - Database schema and tables verified/created successfully
2025-01-23 20:38:32,906 - [INFO] - Spark Session created successfully
2025-01-23 20:38:32,906 - [INFO] - Spark version: 3.5.4
2025-01-23 20:38:32,906 - [INFO] - Spark master: local[*]
2025-01-23 20:38:32,906 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-23 20:38:32,906 - [INFO] - Starting file processing...
2025-01-23 20:38:32,906 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-23 20:38:32,907 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-23 20:38:32,907 - [INFO] - User agent filter: some user agent
2025-01-23 20:38:32,908 - [INFO] - Found 11 total parquet files
2025-01-23 20:38:32,908 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-23 20:38:32,908 - [INFO] - Clicks files by date: 2022-05-27
2025-01-23 20:38:37,935 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 20:38:38,380 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-23 20:38:40,050 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 20:38:40,504 - [INFO] - Found 0 records with invalid hours
2025-01-23 20:38:40,861 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 20:38:41,209 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-23 20:38:41,455 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 20:38:46,172 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-23 20:38:46,173 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-23 20:38:46,173 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 20:38:46,173 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-23 20:38:46,174 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-23 20:38:46,423 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 20:38:46,590 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-23 20:38:47,724 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 20:38:47,989 - [INFO] - Found 0 records with invalid hours
2025-01-23 20:38:48,180 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-23 20:38:48,348 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-23 20:38:48,525 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-23 20:38:48,832 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-23 20:38:49,075 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-23 20:38:50,446 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 20:38:50,692 - [INFO] - Found 0 records with invalid hours
2025-01-23 20:38:50,859 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 20:38:51,090 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-23 20:38:51,380 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 20:38:55,232 - [WARNING] - Found records where clicks exceed impressions: 2022-05-27
2025-01-23 20:38:55,955 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-23 20:38:55,956 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-23 20:38:55,956 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 20:38:55,957 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-23 20:38:55,958 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-23 20:38:55,958 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 20:38:55,958 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-23 20:38:55,959 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-23 20:38:55,959 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-23 20:38:55,960 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-23 20:38:55,960 - [INFO] - Successfully processed 2 dates
2025-01-23 20:38:55,960 - [INFO] - Starting client report loading process...
2025-01-23 20:38:55,960 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-23 20:38:55,962 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 20:38:55,973 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 20:38:55,977 - [INFO] - Archived 0 unique rows
2025-01-23 20:38:55,995 - [INFO] - Successfully loaded 48 rows
2025-01-23 20:38:56,000 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-23 20:38:56,000 - [INFO] - - Archived rows: 0
2025-01-23 20:38:56,000 - [INFO] - - Loaded rows: 48
2025-01-23 20:38:56,000 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-23 20:38:56,003 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 20:38:56,009 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 20:38:56,013 - [INFO] - Archived 0 unique rows
2025-01-23 20:38:56,023 - [INFO] - Successfully loaded 48 rows
2025-01-23 20:38:56,027 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-23 20:38:56,027 - [INFO] - - Archived rows: 0
2025-01-23 20:38:56,027 - [INFO] - - Loaded rows: 48
2025-01-23 20:38:56,027 - [INFO] - 
Verifying data load...
2025-01-23 20:38:56,032 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-23 20:38:56,032 - [INFO] - 
Load Summary:
2025-01-23 20:38:56,033 - [INFO] - Total files processed: 2
2025-01-23 20:38:56,033 - [INFO] - Total rows loaded: 96
2025-01-23 20:38:56,033 - [INFO] - 
Date Range:
2025-01-23 20:38:56,033 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-23 20:38:56,033 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-23 20:38:56,033 - [INFO] - 
Total Counts:
2025-01-23 20:38:56,033 - [INFO] -   Impressions: 44
2025-01-23 20:38:56,033 - [INFO] -   Clicks: 30
2025-01-23 20:38:56,033 - [INFO] - 
Client report loading completed successfully
2025-01-23 20:38:56,033 - [INFO] - Processing and loading completed successfully
2025-01-23 20:38:56,231 - [INFO] - Spark Session stopped
2025-01-23 20:50:35,491 - [INFO] - Starting application...
2025-01-23 20:50:35,551 - [INFO] - Database schema and tables verified/created successfully
2025-01-23 20:50:44,897 - [INFO] - Spark Session created successfully
2025-01-23 20:50:44,897 - [INFO] - Spark version: 3.5.4
2025-01-23 20:50:44,897 - [INFO] - Spark master: local[*]
2025-01-23 20:50:44,897 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-23 20:50:44,897 - [INFO] - Starting file processing...
2025-01-23 20:50:44,897 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-23 20:50:44,897 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-23 20:50:44,897 - [INFO] - User agent filter: some user agent
2025-01-23 20:50:44,898 - [INFO] - Found 11 total parquet files
2025-01-23 20:50:44,898 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-23 20:50:44,898 - [INFO] - Clicks files by date: 2022-05-27
2025-01-23 20:50:51,011 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 20:50:51,733 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-23 20:50:53,525 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 20:50:53,989 - [INFO] - Found 0 records with invalid hours
2025-01-23 20:50:54,510 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 20:50:54,907 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-23 20:50:55,117 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 20:50:59,831 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-23 20:50:59,831 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-23 20:50:59,832 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 20:50:59,832 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-23 20:50:59,832 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-23 20:51:00,086 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 20:51:00,273 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-23 20:51:01,380 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 20:51:01,651 - [INFO] - Found 0 records with invalid hours
2025-01-23 20:51:01,811 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-23 20:51:02,004 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-23 20:51:02,190 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-23 20:51:02,470 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-23 20:51:02,625 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-23 20:51:03,998 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 20:51:04,242 - [INFO] - Found 0 records with invalid hours
2025-01-23 20:51:04,420 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 20:51:04,627 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-23 20:51:04,972 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 20:51:08,916 - [WARNING] - Found records where clicks exceed impressions: 2022-05-27
2025-01-23 20:51:09,615 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-23 20:51:09,616 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-23 20:51:09,616 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 20:51:09,617 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-23 20:51:09,617 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-23 20:51:09,618 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 20:51:09,618 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-23 20:51:09,619 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-23 20:51:09,620 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-23 20:51:09,620 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-23 20:51:09,620 - [INFO] - Successfully processed 2 dates
2025-01-23 20:51:09,620 - [INFO] - Starting client report loading process...
2025-01-23 20:51:09,621 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-23 20:51:09,622 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 20:51:09,630 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 20:51:09,633 - [INFO] - Archived 0 unique rows
2025-01-23 20:51:09,646 - [INFO] - Successfully loaded 48 rows
2025-01-23 20:51:09,653 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-23 20:51:09,654 - [INFO] - - Archived rows: 0
2025-01-23 20:51:09,654 - [INFO] - - Loaded rows: 48
2025-01-23 20:51:09,654 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-23 20:51:09,664 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 20:51:09,670 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 20:51:09,672 - [INFO] - Archived 0 unique rows
2025-01-23 20:51:09,680 - [INFO] - Successfully loaded 48 rows
2025-01-23 20:51:09,685 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-23 20:51:09,685 - [INFO] - - Archived rows: 0
2025-01-23 20:51:09,685 - [INFO] - - Loaded rows: 48
2025-01-23 20:51:09,685 - [INFO] - 
Verifying data load...
2025-01-23 20:51:09,689 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-23 20:51:09,690 - [INFO] - 
Load Summary:
2025-01-23 20:51:09,690 - [INFO] - Total files processed: 2
2025-01-23 20:51:09,690 - [INFO] - Total rows loaded: 96
2025-01-23 20:51:09,690 - [INFO] - 
Date Range:
2025-01-23 20:51:09,690 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-23 20:51:09,690 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-23 20:51:09,690 - [INFO] - 
Total Counts:
2025-01-23 20:51:09,690 - [INFO] -   Impressions: 44
2025-01-23 20:51:09,690 - [INFO] -   Clicks: 30
2025-01-23 20:51:09,690 - [INFO] - 
Client report loading completed successfully
2025-01-23 20:51:09,690 - [INFO] - Processing and loading completed successfully
2025-01-23 20:51:09,901 - [INFO] - Spark Session stopped
2025-01-23 21:03:57,363 - [INFO] - Starting application...
2025-01-23 21:03:57,427 - [INFO] - Database schema and tables verified/created successfully
2025-01-23 21:04:23,380 - [INFO] - Spark Session created successfully
2025-01-23 21:04:23,380 - [INFO] - Spark version: 3.5.4
2025-01-23 21:04:23,380 - [INFO] - Spark master: local[*]
2025-01-23 21:04:23,381 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-23 21:04:23,381 - [INFO] - Starting file processing...
2025-01-23 21:04:23,381 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-23 21:04:23,381 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-23 21:04:23,381 - [INFO] - User agent filter: some user agent
2025-01-23 21:04:23,382 - [INFO] - Found 11 total parquet files
2025-01-23 21:04:23,382 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-23 21:04:23,382 - [INFO] - Clicks files by date: 2022-05-27
2025-01-23 21:04:28,458 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 21:04:28,881 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-23 21:04:30,613 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 21:04:31,049 - [INFO] - Found 0 records with invalid hours
2025-01-23 21:04:31,416 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 21:04:31,790 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-23 21:04:32,072 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-23 21:04:37,171 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-23 21:04:37,171 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-23 21:04:37,172 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-23 21:04:37,172 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-23 21:04:37,172 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-23 21:04:37,459 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-23 21:04:37,603 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-23 21:04:38,715 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 21:04:38,957 - [INFO] - Found 0 records with invalid hours
2025-01-23 21:04:39,153 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-23 21:04:39,427 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-23 21:04:39,645 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-23 21:04:39,911 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-23 21:04:40,096 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-23 21:04:41,492 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 21:04:41,784 - [INFO] - Found 0 records with invalid hours
2025-01-23 21:04:42,041 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 21:04:42,415 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-23 21:04:42,912 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-23 21:04:47,530 - [WARNING] - Found records where clicks exceed impressions: 2022-05-27
2025-01-23 21:04:48,226 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-23 21:04:48,226 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-23 21:04:48,227 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-23 21:04:48,228 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-23 21:04:48,228 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-23 21:04:48,229 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-23 21:04:48,229 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-23 21:04:48,230 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-23 21:04:48,230 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-23 21:04:48,230 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-23 21:04:48,231 - [INFO] - Successfully processed 2 dates
2025-01-23 21:04:48,231 - [INFO] - Starting client report loading process...
2025-01-23 21:04:48,231 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-23 21:04:48,233 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 21:04:48,242 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 21:04:48,246 - [INFO] - Archived 0 unique rows
2025-01-23 21:04:48,262 - [INFO] - Successfully loaded 48 rows
2025-01-23 21:04:48,267 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-23 21:04:48,267 - [INFO] - - Archived rows: 0
2025-01-23 21:04:48,267 - [INFO] - - Loaded rows: 48
2025-01-23 21:04:48,267 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-23 21:04:48,277 - [INFO] - Read CSV with shape: (24, 4)
2025-01-23 21:04:48,283 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-23 21:04:48,287 - [INFO] - Archived 0 unique rows
2025-01-23 21:04:48,295 - [INFO] - Successfully loaded 48 rows
2025-01-23 21:04:48,300 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-23 21:04:48,300 - [INFO] - - Archived rows: 0
2025-01-23 21:04:48,300 - [INFO] - - Loaded rows: 48
2025-01-23 21:04:48,300 - [INFO] - 
Verifying data load...
2025-01-23 21:04:48,305 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-23 21:04:48,307 - [INFO] - 
Load Summary:
2025-01-23 21:04:48,307 - [INFO] - Total files processed: 2
2025-01-23 21:04:48,307 - [INFO] - Total rows loaded: 96
2025-01-23 21:04:48,307 - [INFO] - 
Date Range:
2025-01-23 21:04:48,307 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-23 21:04:48,307 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-23 21:04:48,307 - [INFO] - 
Total Counts:
2025-01-23 21:04:48,307 - [INFO] -   Impressions: 44
2025-01-23 21:04:48,307 - [INFO] -   Clicks: 30
2025-01-23 21:04:48,307 - [INFO] - 
Client report loading completed successfully
2025-01-23 21:04:48,307 - [INFO] - Processing and loading completed successfully
2025-01-23 21:04:48,530 - [INFO] - Spark Session stopped
2025-01-24 07:50:10,183 - [INFO] - Starting application...
2025-01-24 07:50:10,298 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-24 07:51:09,104 - [INFO] - Starting application...
2025-01-24 07:51:09,290 - [ERROR] - Error ensuring schema: (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "adform_user"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
2025-01-24 07:53:11,766 - [INFO] - Starting application...
2025-01-24 07:53:11,847 - [INFO] - Database schema and tables verified/created successfully
2025-01-24 07:53:44,970 - [INFO] - Spark Session created successfully
2025-01-24 07:53:44,971 - [INFO] - Spark version: 3.5.4
2025-01-24 07:53:44,971 - [INFO] - Spark master: local[*]
2025-01-24 07:53:44,971 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-24 07:53:44,971 - [INFO] - Starting file processing...
2025-01-24 07:53:44,971 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-24 07:53:44,971 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-24 07:53:44,971 - [INFO] - User agent filter: some user agent
2025-01-24 07:53:44,972 - [INFO] - Found 11 total parquet files
2025-01-24 07:53:44,972 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-24 07:53:44,972 - [INFO] - Clicks files by date: 2022-05-27
2025-01-24 07:53:50,420 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-24 07:53:50,860 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-24 07:53:52,565 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-24 07:53:53,000 - [INFO] - Found 0 records with invalid hours
2025-01-24 07:53:53,350 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-24 07:53:53,690 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-24 07:53:53,913 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-24 07:53:58,738 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-24 07:53:58,739 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-24 07:53:58,739 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-24 07:53:58,740 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-24 07:53:58,740 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-24 07:53:59,035 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-24 07:53:59,194 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-24 07:54:00,291 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-24 07:54:00,550 - [INFO] - Found 0 records with invalid hours
2025-01-24 07:54:00,743 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-24 07:54:00,932 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-24 07:54:01,071 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-24 07:54:01,342 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-24 07:54:01,480 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-24 07:54:02,807 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-24 07:54:03,068 - [INFO] - Found 0 records with invalid hours
2025-01-24 07:54:03,338 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-24 07:54:03,544 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-24 07:54:03,818 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-24 07:54:07,899 - [WARNING] - Found records where clicks exceed impressions: 2022-05-27
2025-01-24 07:54:08,587 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-24 07:54:08,588 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-24 07:54:08,588 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-24 07:54:08,589 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-24 07:54:08,589 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-24 07:54:08,590 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-24 07:54:08,590 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-24 07:54:08,591 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-24 07:54:08,591 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-24 07:54:08,591 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-24 07:54:08,591 - [INFO] - Successfully processed 2 dates
2025-01-24 07:54:08,591 - [INFO] - Starting client report loading process...
2025-01-24 07:54:08,592 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-24 07:54:08,594 - [INFO] - Read CSV with shape: (24, 4)
2025-01-24 07:54:08,609 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-24 07:54:08,625 - [INFO] - Archived 0 unique rows
2025-01-24 07:54:08,641 - [INFO] - Successfully loaded 48 rows
2025-01-24 07:54:08,647 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-24 07:54:08,647 - [INFO] - - Archived rows: 0
2025-01-24 07:54:08,647 - [INFO] - - Loaded rows: 48
2025-01-24 07:54:08,648 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-24 07:54:08,657 - [INFO] - Read CSV with shape: (24, 4)
2025-01-24 07:54:08,662 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-24 07:54:08,666 - [INFO] - Archived 0 unique rows
2025-01-24 07:54:08,674 - [INFO] - Successfully loaded 48 rows
2025-01-24 07:54:08,681 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-24 07:54:08,681 - [INFO] - - Archived rows: 0
2025-01-24 07:54:08,682 - [INFO] - - Loaded rows: 48
2025-01-24 07:54:08,682 - [INFO] - 
Verifying data load...
2025-01-24 07:54:08,690 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-24 07:54:08,691 - [INFO] - 
Load Summary:
2025-01-24 07:54:08,691 - [INFO] - Total files processed: 2
2025-01-24 07:54:08,691 - [INFO] - Total rows loaded: 96
2025-01-24 07:54:08,691 - [INFO] - 
Date Range:
2025-01-24 07:54:08,691 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-24 07:54:08,691 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-24 07:54:08,691 - [INFO] - 
Total Counts:
2025-01-24 07:54:08,692 - [INFO] -   Impressions: 44
2025-01-24 07:54:08,692 - [INFO] -   Clicks: 30
2025-01-24 07:54:08,692 - [INFO] - 
Client report loading completed successfully
2025-01-24 07:54:08,692 - [INFO] - Processing and loading completed successfully
2025-01-24 07:54:08,896 - [INFO] - Spark Session stopped
2025-01-24 07:59:14,472 - [INFO] - Starting application...
2025-01-24 07:59:14,539 - [INFO] - Database schema and tables verified/created successfully
2025-01-24 07:59:40,449 - [INFO] - Spark Session created successfully
2025-01-24 07:59:40,450 - [INFO] - Spark version: 3.5.4
2025-01-24 07:59:40,450 - [INFO] - Spark master: local[*]
2025-01-24 07:59:40,450 - [INFO] - Spark app name: ImpressionClickCounterApp
2025-01-24 07:59:40,450 - [INFO] - Starting file processing...
2025-01-24 07:59:40,450 - [INFO] - Input path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data
2025-01-24 07:59:40,450 - [INFO] - Output path: C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output
2025-01-24 07:59:40,450 - [INFO] - User agent filter: some user agent
2025-01-24 07:59:40,451 - [INFO] - Found 11 total parquet files
2025-01-24 07:59:40,451 - [INFO] - Impressions files by date: 2022-05-26, 2022-05-27
2025-01-24 07:59:40,451 - [INFO] - Clicks files by date: 2022-05-27
2025-01-24 07:59:45,535 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-24 07:59:46,003 - [INFO] - Found 14 impressions for user agent: some user agent
2025-01-24 07:59:47,711 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-24 07:59:48,172 - [INFO] - Found 0 records with invalid hours
2025-01-24 07:59:48,566 - [INFO] - Hour distribution before aggregation: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-24 07:59:48,941 - [INFO] - Aggregated hour distribution: [Row(hour=19, count=10, valid_count=10), Row(hour=11, count=4, valid_count=4)]
2025-01-24 07:59:49,220 - [INFO] - Final hour distribution: [Row(hour=19, count=10), Row(hour=11, count=4)]
2025-01-24 07:59:53,896 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-26.csv
2025-01-24 07:59:53,897 - [INFO] - Removed processed file: impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet
2025-01-24 07:59:53,897 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204695_172756391-172756397_0.parquet
2025-01-24 07:59:53,898 - [INFO] - Removed processed file: impressions_processed_dk_20220526193204903_172845709-172845711_1.parquet
2025-01-24 07:59:53,898 - [INFO] - Successfully processed data for date: 2022-05-26
2025-01-24 07:59:54,176 - [INFO] - Distinct user agents in impressions: [Row(user_agent='some user agent')]
2025-01-24 07:59:54,310 - [INFO] - Found 10 impressions for user agent: some user agent
2025-01-24 07:59:55,383 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-24 07:59:55,641 - [INFO] - Found 0 records with invalid hours
2025-01-24 07:59:55,823 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=10)]
2025-01-24 07:59:56,088 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=10, valid_count=10)]
2025-01-24 07:59:56,293 - [INFO] - Final hour distribution: [Row(hour=12, count=10)]
2025-01-24 07:59:56,584 - [INFO] - Distinct user agents in clicks: [Row(user_agent='some user agent')]
2025-01-24 07:59:56,733 - [INFO] - Found 30 clicks for user agent: some user agent
2025-01-24 07:59:58,098 - [INFO] - Processing file: file:///C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/raw_data/clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-24 07:59:58,395 - [INFO] - Found 0 records with invalid hours
2025-01-24 07:59:58,625 - [INFO] - Hour distribution before aggregation: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-24 07:59:58,794 - [INFO] - Aggregated hour distribution: [Row(hour=12, count=20, valid_count=20), Row(hour=11, count=10, valid_count=10)]
2025-01-24 07:59:59,002 - [INFO] - Final hour distribution: [Row(hour=12, count=20), Row(hour=11, count=10)]
2025-01-24 08:00:03,347 - [WARNING] - Found records where clicks exceed impressions: 2022-05-27
2025-01-24 08:00:04,127 - [INFO] - Successfully wrote output to C:/Users/Rokas/Documents/Airidas/adform/adform_spark_app/output\task1_output_2022-05-27.csv
2025-01-24 08:00:04,127 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154212_172756551-172756554_0.parquet
2025-01-24 08:00:04,128 - [INFO] - Removed processed file: impressions_processed_dk_20220527123154402_172845889-172845894_1.parquet
2025-01-24 08:00:04,128 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet
2025-01-24 08:00:04,129 - [INFO] - Removed processed file: clicks_processed_dk_20220527113145201_163587236-163587240_0.parquet
2025-01-24 08:00:04,129 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143730_163587241-163587247_0.parquet
2025-01-24 08:00:04,130 - [INFO] - Removed processed file: clicks_processed_dk_20220527120143900_163644810-163644812_1.parquet
2025-01-24 08:00:04,130 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154754_163644813-163644819_1.parquet
2025-01-24 08:00:04,131 - [INFO] - Removed processed file: clicks_processed_dk_20220527123154813_163587248-163587250_0.parquet
2025-01-24 08:00:04,131 - [INFO] - Successfully processed data for date: 2022-05-27
2025-01-24 08:00:04,131 - [INFO] - Successfully processed 2 dates
2025-01-24 08:00:04,131 - [INFO] - Starting client report loading process...
2025-01-24 08:00:04,132 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv
2025-01-24 08:00:04,134 - [INFO] - Read CSV with shape: (24, 4)
2025-01-24 08:00:04,145 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-24 08:00:04,151 - [INFO] - Archived 0 unique rows
2025-01-24 08:00:04,166 - [INFO] - Successfully loaded 48 rows
2025-01-24 08:00:04,172 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-26.csv:
2025-01-24 08:00:04,172 - [INFO] - - Archived rows: 0
2025-01-24 08:00:04,173 - [INFO] - - Loaded rows: 48
2025-01-24 08:00:04,173 - [INFO] - Reading file: C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv
2025-01-24 08:00:04,176 - [INFO] - Read CSV with shape: (24, 4)
2025-01-24 08:00:04,182 - [INFO] - Data prepared successfully. Shape: (24, 4)
2025-01-24 08:00:04,184 - [INFO] - Archived 0 unique rows
2025-01-24 08:00:04,193 - [INFO] - Successfully loaded 48 rows
2025-01-24 08:00:04,199 - [INFO] - Processed C:\Users\Rokas\Documents\Airidas\adform\adform_spark_app\output\task1_output_2022-05-27.csv:
2025-01-24 08:00:04,199 - [INFO] - - Archived rows: 0
2025-01-24 08:00:04,199 - [INFO] - - Loaded rows: 48
2025-01-24 08:00:04,199 - [INFO] - 
Verifying data load...
2025-01-24 08:00:04,204 - [INFO] - Verification results: {'total_rows': 48, 'date_range': {'start': datetime.datetime(2022, 5, 26, 0, 0), 'end': datetime.datetime(2022, 5, 27, 23, 0)}, 'totals': {'impressions': Decimal('44'), 'clicks': Decimal('30')}}
2025-01-24 08:00:04,205 - [INFO] - 
Load Summary:
2025-01-24 08:00:04,205 - [INFO] - Total files processed: 2
2025-01-24 08:00:04,205 - [INFO] - Total rows loaded: 96
2025-01-24 08:00:04,205 - [INFO] - 
Date Range:
2025-01-24 08:00:04,205 - [INFO] -   Start: 2022-05-26 00:00:00
2025-01-24 08:00:04,205 - [INFO] -   End: 2022-05-27 23:00:00
2025-01-24 08:00:04,205 - [INFO] - 
Total Counts:
2025-01-24 08:00:04,206 - [INFO] -   Impressions: 44
2025-01-24 08:00:04,206 - [INFO] -   Clicks: 30
2025-01-24 08:00:04,206 - [INFO] - 
Client report loading completed successfully
2025-01-24 08:00:04,206 - [INFO] - Processing and loading completed successfully
2025-01-24 08:00:05,348 - [INFO] - Spark Session stopped
