{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ETL application for counting impressions and clicks by date/hour for a specific user agent.\n",
    "Uses PySpark, environment variables (DB credentials, etc.), and writes CSV outputs.\n",
    "Follows best practices: DRY, PEP-8, logging, error handling, missing data handling.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Environment variables (example usage, no hardcoded credentials)\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"user\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"password\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "def parse_filename_datetime(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    From a filename like 'impressions_processed_dk_20220526113212045_172845633-172845636_1.parquet',\n",
    "    extract the date/time in the format '2022-05-26 11:32'.\n",
    "    \"\"\"\n",
    "    match = re.search(r'_dk_(\\d{8})(\\d{4})', filename)\n",
    "    if not match:\n",
    "        return None\n",
    "    date_str, time_str = match.group(1), match.group(2)\n",
    "    parsed_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\")\n",
    "    return parsed_dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "def create_date_hour_df(spark, date_str):\n",
    "    \"\"\"\n",
    "    Create a Spark DataFrame with all hours of the given date (00-23) to ensure no missing hours.\n",
    "    \"\"\"\n",
    "    base_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    rows = [(base_date.strftime(\"%Y-%m-%d\"), h) for h in range(24)]\n",
    "    return spark.createDataFrame(rows, [\"date\", \"hour\"])\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"ImpressionsClicksETL\").getOrCreate()\n",
    "\n",
    "    # Example: input directory, output directory\n",
    "    input_dir = \"./input_parquet\"\n",
    "    output_dir = \"./output_csv\"\n",
    "    target_user_agent = \"some user agent\"\n",
    "\n",
    "    # Force creation of the directory if it doesn't exist\n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "    # Process all parquet files in the input directory\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith(\".parquet\"):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "\n",
    "            # Parse date/time from filename\n",
    "            date_time_str = parse_filename_datetime(file_name)\n",
    "            if not date_time_str:\n",
    "                logging.warning(\"Skipping file %s, unable to parse date/time.\", file_name)\n",
    "                continue\n",
    "\n",
    "            # Read parquet\n",
    "            df = spark.read.parquet(file_path)\n",
    "\n",
    "            # Extract date/hour from the filenameâ€™s date/time\n",
    "            extracted_date = date_time_str.split(\" \")[0]\n",
    "\n",
    "            # Filter records by our target user agent\n",
    "            filtered_df = df.filter(F.col(\"device_settings.user_agent\") == target_user_agent)\n",
    "\n",
    "            # Suppose impressions/clicks are columns: \"impressions\", \"clicks\"\n",
    "            # Derive hour from the processed date_time_str (for merging with the hour dimension)\n",
    "            hour_val = int(date_time_str.split(\" \")[1].split(\":\")[0])\n",
    "            aggregated_df = filtered_df.agg(\n",
    "                F.sum(\"impressions\").alias(\"impressions_sum\"),\n",
    "                F.sum(\"clicks\").alias(\"clicks_sum\")\n",
    "            ).withColumn(\"date\", F.lit(extracted_date)) \\\n",
    "             .withColumn(\"hour\", F.lit(hour_val))\n",
    "\n",
    "            # Create a full date-hour DataFrame (00-23) for the same date\n",
    "            date_hour_df = create_date_hour_df(spark, extracted_date)\n",
    "\n",
    "            # Join aggregated_df to date_hour_df to fill missing hours with zeros\n",
    "            joined_df = date_hour_df.join(\n",
    "                aggregated_df,\n",
    "                on=[\"date\", \"hour\"],\n",
    "                how=\"left\"\n",
    "            ).na.fill({\"impressions_sum\": 0, \"clicks_sum\": 0})\n",
    "\n",
    "            # Rename columns to match the required output\n",
    "            result_df = joined_df.select(\n",
    "                \"date\",\n",
    "                \"hour\",\n",
    "                F.col(\"impressions_sum\").alias(\"impression_count\"),\n",
    "                F.col(\"clicks_sum\").alias(\"click_count\")\n",
    "            )\n",
    "\n",
    "            # Write to CSV\n",
    "            output_file = os.path.join(output_dir, f\"impressions_clicks_{extracted_date}.csv\")\n",
    "            result_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_file)\n",
    "            logging.info(\"Processed file %s -> %s\", file_path, output_file)\n",
    "\n",
    "            # Remove input file\n",
    "            os.remove(file_path)\n",
    "            logging.info(\"Removed input file: %s\", file_path)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a8976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a62e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the parquet file\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ParquetReader\").getOrCreate()\n",
    "\n",
    "file_path = r\"C:\\Users\\Rokas\\Documents\\Airidas\\adform\\adform_spark_app\\raw_data\\clicks_processed_dk_20220527113145108_163644805-163644809_1.parquet\"\n",
    "df = spark.read.parquet(file_path)\n",
    "\n",
    "# Display the first few rows and schema\n",
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b3eaa9",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error connecting to database: connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"adform_user\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from psycopg2 import OperationalError\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"postgres\",  # default database name\n",
    "        user=\"adform_user\",    # using the correct username\n",
    "        password=\"adform_pass\", # replace with your actual password\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    print(\"Connected OK!\")\n",
    "    conn.close()\n",
    "except OperationalError as e:\n",
    "    print(f\"Error connecting to database: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8db5609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~yspark (c:\\Users\\Rokas\\Documents\\Airidas\\adform\\adform_spark_app\\.venv\\Lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement main (from versions: none)\n",
      "ERROR: No matching distribution found for main\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'main'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall main\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_warehouse_connection\n\u001b[0;32m      6\u001b[0m get_warehouse_connection()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'main'"
     ]
    }
   ],
   "source": [
    "%pip install main\n",
    "\n",
    "from main import get_warehouse_connection\n",
    "\n",
    "\n",
    "get_warehouse_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe051447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected OK!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from psycopg2 import OperationalError\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"adform_db\",  # default database name\n",
    "        user=\"adform_user\",    # using the correct username\n",
    "        password=\"adform_pass\", # replace with your actual password\n",
    "        host=\"localhost\",\n",
    "        port=\"5433\"\n",
    "    )\n",
    "    print(\"Connected OK!\")\n",
    "    conn.close()\n",
    "except OperationalError as e:\n",
    "    print(f\"Error connecting to database: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
